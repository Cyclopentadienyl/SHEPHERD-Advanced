# é†«ç™‚çŸ¥è­˜åœ–è­œè¨ºæ–·å¼•æ“ - é€²éšåŠŸèƒ½ TODO

**ç‰ˆæœ¬**: v1.0  
**å»ºè­°å„ªå…ˆç´š**: Phase 2-3  
**é è¨ˆæ™‚é–“**: 4-6 é€±  
**ç‹€æ…‹**: ğŸ”´ æœªé–‹å§‹

---

## ğŸ¯ åŠŸèƒ½æ¦‚è¿°

### åŠŸèƒ½ 1: ç—‡ç‹€/è®Šç•°é—œè¯åˆ†æ
- **ç›®æ¨™**: åˆ†æå…©å€‹ç—‡ç‹€æˆ–è®Šç•°ä¹‹é–“çš„é—œè¯æ€§
- **æ‡‰ç”¨**: è¼”åŠ©é†«ç”Ÿç†è§£ç—‡ç‹€çµ„åˆçš„ç”Ÿç‰©å­¸æ©Ÿåˆ¶
- **æŠ€è¡“**: åœ–è·¯å¾‘åˆ†æ + çµ±è¨ˆå…±ç¾ + æœ¬é«”æ¨ç†

### åŠŸèƒ½ 2: åŸºå› å‹-è¡¨å‹é—œè¯æ’åºå¢å¼·
- **ç›®æ¨™**: æä¾›è©³ç´°çš„åŸºå› -ç—‡ç‹€é—œè¯å¼·åº¦ã€å¤–é¡¯ç‡ã€æ–‡ç»æ”¯æŒ
- **æ‡‰ç”¨**: ç²¾æº–é†«å­¸ï¼ŒåŸºå› æª¢æ¸¬çµæœè§£è®€
- **æŠ€è¡“**: æ•´åˆ ClinVar + Pubtator + çŸ¥è­˜åœ–è­œ

### åŠŸèƒ½ 3: è—¥ç‰©/æ²»ç™‚å»ºè­°ï¼ˆç ”ç©¶åƒè€ƒï¼‰
- **ç›®æ¨™**: åŸºæ–¼è¨ºæ–·çµæœæä¾›è—¥ç‰©æ²»ç™‚æ€è·¯ï¼ˆåƒ…ä¾›åƒè€ƒï¼‰
- **æ‡‰ç”¨**: çµ¦é†«ç”Ÿæä¾›æ²»ç™‚æ–¹å‘çš„åˆæ­¥ç·šç´¢
- **æŠ€è¡“**: æ“´å±•çŸ¥è­˜åœ–è­œæ•´åˆ DrugBank
- **âš ï¸ é‡è¦**: é™„å¸¶æ³•å¾‹å…è²¬è²æ˜ï¼Œåƒ…ä½œç ”ç©¶åƒè€ƒ

### åŠŸèƒ½ 4: é›¢ç·šæ¨ç† + æ–‡ç»æª¢ç´¢èˆ‡æ’åº
- **ç›®æ¨™**: è¨ºæ–·å¾Œè‡ªå‹•æª¢ç´¢ç›¸é—œæ–‡ç»ä¸¦æŒ‰å¯ä¿¡åº¦æ’åº
- **æ‡‰ç”¨**: æä¾›è¨ºæ–·çš„æ–‡ç»æ”¯æŒï¼Œè¼”åŠ©è‡¨åºŠæ±ºç­–
- **æŠ€è¡“**: Pubtator é ä¸‹è¼‰ + å¯é¸ PubMed API

---

## ğŸ“‹ Phase 2.3: ç—‡ç‹€/è®Šç•°é—œè¯åˆ†æ (Week 1-1.5)

### ğŸŸ  P1 - åŸºç¤æ¶æ§‹ (Day 1-2)

#### 1.1 å‰µå»ºé—œè¯åˆ†ææ¨¡çµ„

- [ ] å‰µå»º `src/analysis/__init__.py` ğŸ• 5min
  ```python
  """
  åˆ†ææ¨¡çµ„ï¼šç—‡ç‹€é—œè¯ã€åŸºå› -è¡¨å‹é—œè¯ç­‰
  """
  from .phenotype_correlation import PhenotypeCorrelationAnalyzer
  from .genotype_phenotype_ranker import GenotypePhenotypeRanker
  
  __all__ = [
      'PhenotypeCorrelationAnalyzer',
      'GenotypePhenotypeRanker'
  ]
  ```

- [ ] å‰µå»º `src/analysis/phenotype_correlation.py` ğŸ“… 1 å¤©
  ```python
  """
  ç—‡ç‹€é—œè¯åˆ†æå™¨
  
  ä¾è³´:
      - src/kg/builder.py (çŸ¥è­˜åœ–è­œ)
      - src/retrieval/path_retriever.py (è·¯å¾‘æª¢ç´¢)
      - src/ontology/similarity.py (æœ¬é«”ç›¸ä¼¼åº¦)
  """
  from typing import Dict, List, Tuple
  from src.kg.builder import KnowledgeGraphBuilder
  from src.retrieval.path_retriever import PathRetriever
  from src.ontology.similarity import OntologySimilarity
  
  class PhenotypeCorrelationAnalyzer:
      """åˆ†æå…©å€‹ç—‡ç‹€ä¹‹é–“çš„å¤šç¶­åº¦é—œè¯"""
      
      def __init__(self, kg: KnowledgeGraphBuilder):
          self.kg = kg
          self.path_retriever = PathRetriever(kg)
          self.ontology_sim = OntologySimilarity()
          self.cooccurrence = self._load_cooccurrence_graph()
      
      def analyze_correlation(
          self,
          phenotype_a: str,  # HPO ID
          phenotype_b: str   # HPO ID
      ) -> Dict:
          """
          åˆ†æå…©å€‹ç—‡ç‹€çš„é—œè¯æ€§
          
          Returns:
              {
                  'correlation_type': str,       # 'synergistic', 'antagonistic', 'independent'
                  'strength': float,             # [0, 1]
                  'confidence': float,           # çµ±è¨ˆé¡¯è‘—æ€§
                  'mechanisms': List[Dict],      # ç”Ÿç‰©å­¸æ©Ÿåˆ¶
                  'shared_diseases': List[str],  # å…±åŒé—œè¯ç–¾ç—…
                  'shared_genes': List[str],     # å…±åŒé—œè¯åŸºå› 
                  'connecting_paths': List[Dict],# é€£æ¥è·¯å¾‘
                  'evidence': List[Dict]         # æ”¯æŒè­‰æ“š
              }
          """
          # 1. è·¯å¾‘åˆ†æ
          paths = self._find_connecting_paths(phenotype_a, phenotype_b)
          
          # 2. å…±ç¾åˆ†æ
          cooccurrence_stats = self._compute_cooccurrence(phenotype_a, phenotype_b)
          
          # 3. æœ¬é«”é—œä¿‚
          ontology_relation = self._check_ontology_relation(phenotype_a, phenotype_b)
          
          # 4. å…±äº«å¯¦é«”
          shared_entities = self._find_shared_entities(phenotype_a, phenotype_b)
          
          # 5. ç”Ÿç‰©å­¸æ©Ÿåˆ¶æ¨æ–·
          mechanisms = self._infer_mechanisms(paths, shared_entities)
          
          # 6. ç¶œåˆè©•åˆ†
          correlation_type, strength = self._classify_and_score(
              paths, cooccurrence_stats, ontology_relation
          )
          
          return {
              'correlation_type': correlation_type,
              'strength': strength,
              'confidence': cooccurrence_stats.get('p_value', 0.0),
              'mechanisms': mechanisms,
              'shared_diseases': shared_entities['diseases'],
              'shared_genes': shared_entities['genes'],
              'connecting_paths': paths[:5],  # å‰5æ¢è·¯å¾‘
              'evidence': self._collect_evidence(paths)
          }
      
      def _find_connecting_paths(self, phenotype_a, phenotype_b):
          """æ‰¾åˆ°é€£æ¥å…©ç—‡ç‹€çš„æ‰€æœ‰è·¯å¾‘"""
          return self.path_retriever.find_paths_between(
              source=phenotype_a,
              target=phenotype_b,
              max_hops=4,
              top_k=20
          )
      
      def _compute_cooccurrence(self, phenotype_a, phenotype_b):
          """
          è¨ˆç®—çµ±è¨ˆå…±ç¾æŒ‡æ¨™
          
          Returns:
              {
                  'frequency': float,      # å…±ç¾é »ç‡
                  'pmi': float,            # é»äº’ä¿¡æ¯
                  'chi_square': float,     # å¡æ–¹çµ±è¨ˆé‡
                  'p_value': float,        # é¡¯è‘—æ€§
                  'odds_ratio': float      # å„ªå‹¢æ¯”
              }
          """
          # å¾ç–¾ç—…å…±ç¾åœ–æŸ¥è©¢
          return self.cooccurrence.compute_metrics(phenotype_a, phenotype_b)
      
      def _check_ontology_relation(self, phenotype_a, phenotype_b):
          """æª¢æŸ¥æœ¬é«”å±¤æ¬¡é—œä¿‚"""
          return {
              'is_parent_child': self.ontology_sim.is_ancestor(phenotype_a, phenotype_b),
              'is_sibling': self.ontology_sim.are_siblings(phenotype_a, phenotype_b),
              'semantic_similarity': self.ontology_sim.compute_similarity(
                  phenotype_a, phenotype_b
              ),
              'common_ancestor': self.ontology_sim.lowest_common_ancestor(
                  phenotype_a, phenotype_b
              )
          }
      
      def _find_shared_entities(self, phenotype_a, phenotype_b):
          """æ‰¾åˆ°å…©ç—‡ç‹€å…±äº«çš„ç–¾ç—…å’ŒåŸºå› """
          # æŸ¥è©¢çŸ¥è­˜åœ–è­œ
          diseases_a = self.kg.query_connected_entities(phenotype_a, 'disease')
          diseases_b = self.kg.query_connected_entities(phenotype_b, 'disease')
          
          genes_a = self.kg.query_connected_entities(phenotype_a, 'gene')
          genes_b = self.kg.query_connected_entities(phenotype_b, 'gene')
          
          return {
              'diseases': list(set(diseases_a) & set(diseases_b)),
              'genes': list(set(genes_a) & set(genes_b))
          }
      
      def _infer_mechanisms(self, paths, shared_entities):
          """
          æ¨æ–·ç”Ÿç‰©å­¸æ©Ÿåˆ¶
          
          æ©Ÿåˆ¶é¡å‹:
              1. å…±äº«åŸºå› çªè®Š
              2. å…±äº«ç”Ÿç‰©é€šè·¯
              3. ç´šè¯æ•ˆæ‡‰ï¼ˆä¸€å€‹ç—‡ç‹€å°è‡´å¦ä¸€å€‹ï¼‰
              4. å¹³è¡Œæ•ˆæ‡‰ï¼ˆå…±åŒåŸå› ï¼‰
          """
          mechanisms = []
          
          # æ©Ÿåˆ¶1ï¼šå…±äº«åŸºå› 
          if shared_entities['genes']:
              mechanisms.append({
                  'type': 'shared_genetic_basis',
                  'genes': shared_entities['genes'][:5],
                  'description': f"å…©ç—‡ç‹€ç”± {len(shared_entities['genes'])} å€‹å…±åŒåŸºå› é—œè¯"
              })
          
          # æ©Ÿåˆ¶2ï¼šè·¯å¾‘æ¨æ–·
          for path in paths[:3]:
              if len(path['nodes']) == 3:  # A â†’ X â†’ B
                  intermediate = path['nodes'][1]
                  mechanisms.append({
                      'type': 'cascade_effect',
                      'mediator': intermediate,
                      'description': f"ç—‡ç‹€Aé€šé {intermediate} å°è‡´ç—‡ç‹€B"
                  })
          
          return mechanisms
      
      def _classify_and_score(self, paths, cooccurrence, ontology_relation):
          """
          åˆ†é¡é—œè¯é¡å‹ä¸¦è©•åˆ†
          
          é¡å‹:
              - synergistic: å”åŒï¼ˆå…±ç¾é »ç‡é«˜ï¼‰
              - antagonistic: æ‹®æŠ—ï¼ˆå¾ˆå°‘å…±ç¾ï¼‰
              - independent: ç¨ç«‹ï¼ˆç„¡é¡¯è‘—é—œè¯ï¼‰
          """
          # åŸºæ–¼å…±ç¾é »ç‡å’Œpå€¼
          if cooccurrence['p_value'] < 0.05:
              if cooccurrence['odds_ratio'] > 1.5:
                  correlation_type = 'synergistic'
                  strength = min(cooccurrence['odds_ratio'] / 10.0, 1.0)
              elif cooccurrence['odds_ratio'] < 0.5:
                  correlation_type = 'antagonistic'
                  strength = max(1.0 - cooccurrence['odds_ratio'], 0.5)
              else:
                  correlation_type = 'independent'
                  strength = 0.3
          else:
              correlation_type = 'independent'
              strength = 0.1
          
          # è·¯å¾‘å­˜åœ¨æ€§å¢å¼·è©•åˆ†
          if paths:
              strength = min(strength + 0.2, 1.0)
          
          # æœ¬é«”ç›¸ä¼¼åº¦å¢å¼·è©•åˆ†
          if ontology_relation['semantic_similarity'] > 0.7:
              strength = min(strength + 0.15, 1.0)
          
          return correlation_type, strength
  ```

#### 1.2 API ç«¯é»æ•´åˆ

- [ ] æ›´æ–° `src/api/routes/analysis.py` ğŸ“… 0.5 å¤©
  ```python
  """
  åˆ†æç›¸é—œ API ç«¯é»
  """
  from fastapi import APIRouter, HTTPException
  from pydantic import BaseModel
  from typing import List
  from src.analysis.phenotype_correlation import PhenotypeCorrelationAnalyzer
  
  router = APIRouter(prefix="/api/v2/analysis", tags=["analysis"])
  
  class CorrelationRequest(BaseModel):
      phenotype_a: str  # HPO ID
      phenotype_b: str  # HPO ID
  
  @router.post("/phenotype-correlation")
  async def analyze_phenotype_correlation(request: CorrelationRequest):
      """
      åˆ†æå…©å€‹ç—‡ç‹€çš„é—œè¯æ€§
      
      Example:
          POST /api/v2/analysis/phenotype-correlation
          {
              "phenotype_a": "HP:0001324",  // è‚Œè‚‰ç„¡åŠ›
              "phenotype_b": "HP:0001649"   // å¿ƒå¾‹ä¸æ•´
          }
      """
      try:
          analyzer = PhenotypeCorrelationAnalyzer(kg=global_kg)
          result = analyzer.analyze_correlation(
              request.phenotype_a,
              request.phenotype_b
          )
          return result
      except Exception as e:
          raise HTTPException(status_code=500, detail=str(e))
  ```

#### 1.3 WebUI æ•´åˆ

- [ ] æ›´æ–° `webui/components/analysis_tab.py` ğŸ“… 0.5 å¤©
  ```python
  # æ–°å¢ Analysis Tab
  
  with gr.Tab("ğŸ”¬ é—œè¯åˆ†æ"):
      gr.Markdown("## ç—‡ç‹€é—œè¯åˆ†æ")
      
      with gr.Row():
          phenotype_a_input = gr.Textbox(
              label="ç—‡ç‹€ A (HPO ID)",
              placeholder="ä¾‹å¦‚: HP:0001324"
          )
          phenotype_b_input = gr.Textbox(
              label="ç—‡ç‹€ B (HPO ID)",
              placeholder="ä¾‹å¦‚: HP:0001649"
          )
      
      analyze_btn = gr.Button("åˆ†æé—œè¯", variant="primary")
      
      # çµæœé¡¯ç¤º
      with gr.Column():
          correlation_type = gr.Textbox(label="é—œè¯é¡å‹")
          strength_slider = gr.Slider(
              minimum=0, maximum=1,
              label="é—œè¯å¼·åº¦",
              interactive=False
          )
          mechanisms_display = gr.JSON(label="ç”Ÿç‰©å­¸æ©Ÿåˆ¶")
          shared_diseases = gr.DataFrame(label="å…±åŒé—œè¯ç–¾ç—…")
          paths_viz = gr.HTML(label="é€£æ¥è·¯å¾‘å¯è¦–åŒ–")
  ```

**å°è¨ˆ**: ğŸ“† 3-4 å¤©

---

## ğŸ“‹ Phase 2.4: åŸºå› å‹-è¡¨å‹é—œè¯æ’åºå¢å¼· (Week 1.5-2)

### ğŸŸ  P1 - å¢å¼·æ’åºæ¨¡çµ„

- [ ] æ“´å±• `src/models/tasks/gene_scoring.py` ğŸ“… 2 å¤©
  ```python
  """
  å¢å¼·ç‰ˆåŸºå› -è¡¨å‹é—œè¯è©•åˆ†
  
  æ–°å¢åŠŸèƒ½:
      - å¤–é¡¯ç‡è¨ˆç®—
      - ClinVar è®Šç•°è³‡è¨Šæ•´åˆ
      - éºå‚³æ¨¡å¼æ¨æ–·
      - æ–‡ç»æ”¯æŒè©•åˆ†
  """
  from typing import Dict, List
  import requests
  
  class EnhancedGeneScoring:
      """å¢å¼·ç‰ˆåŸºå› è©•åˆ†ç³»çµ±"""
      
      def __init__(self, kg, clinvar_api, pubtator_db):
          self.kg = kg
          self.clinvar_api = clinvar_api
          self.pubtator = pubtator_db
          self.base_scorer = GeneScoring(kg)  # ç¾æœ‰æ¨¡çµ„
      
      def rank_genotype_phenotype_correlations(
          self,
          genotypes: List[str],  # åŸºå› æ¸…å–®æˆ–è®Šç•°ID
          phenotypes: List[str]  # HPO IDs
      ) -> List[Dict]:
          """
          è©³ç´°çš„åŸºå› -è¡¨å‹é—œè¯æ’åº
          
          Returns:
              List[{
                  'gene': str,
                  'phenotype': str,
                  'correlation_score': float,       # æ•´é«”åˆ†æ•¸
                  'penetrance': float,              # å¤–é¡¯ç‡
                  'pathogenicity': str,             # è‡´ç—…æ€§
                  'mode_of_inheritance': str,       # éºå‚³æ¨¡å¼
                  'allele_frequency': float,        # ç­‰ä½åŸºå› é »ç‡
                  'clinical_significance': str,
                  'evidence_strength': str,         # 'strong', 'moderate', 'weak'
                  'literature_count': int,
                  'top_papers': List[Dict]
              }]
          """
          results = []
          
          for gene in genotypes:
              for phenotype in phenotypes:
                  # 1. åŸºç¤ GNN è©•åˆ†
                  gnn_score = self.base_scorer.score_gene_phenotype(gene, phenotype)
                  
                  # 2. ClinVar è®Šç•°è³‡è¨Š
                  clinvar_data = self._fetch_clinvar_data(gene, phenotype)
                  
                  # 3. å¤–é¡¯ç‡ä¼°ç®—
                  penetrance = self._estimate_penetrance(gene, phenotype, clinvar_data)
                  
                  # 4. éºå‚³æ¨¡å¼
                  inheritance_mode = self._infer_inheritance_mode(gene, clinvar_data)
                  
                  # 5. æ–‡ç»æ”¯æŒ
                  literature = self._search_literature(gene, phenotype)
                  
                  # 6. ç¶œåˆè©•åˆ†
                  final_score = self._compute_final_score(
                      gnn_score,
                      clinvar_data,
                      penetrance,
                      len(literature)
                  )
                  
                  results.append({
                      'gene': gene,
                      'phenotype': phenotype,
                      'correlation_score': final_score,
                      'penetrance': penetrance,
                      'pathogenicity': clinvar_data.get('significance', 'VUS'),
                      'mode_of_inheritance': inheritance_mode,
                      'allele_frequency': clinvar_data.get('frequency', 0.0),
                      'clinical_significance': clinvar_data.get('description', ''),
                      'evidence_strength': self._classify_evidence(clinvar_data, literature),
                      'literature_count': len(literature),
                      'top_papers': literature[:5]
                  })
          
          # æ’åº
          return sorted(results, key=lambda x: x['correlation_score'], reverse=True)
      
      def _fetch_clinvar_data(self, gene, phenotype):
          """å¾ ClinVar ç²å–è®Šç•°è³‡è¨Š"""
          # æŸ¥è©¢ ClinVar API
          response = self.clinvar_api.search(
              gene=gene,
              phenotype=phenotype
          )
          return self._parse_clinvar_response(response)
      
      def _estimate_penetrance(self, gene, phenotype, clinvar_data):
          """
          ä¼°ç®—å¤–é¡¯ç‡
          
          æ–¹æ³•:
              1. å¾ ClinVar è®Šç•°è³‡æ–™æ¨æ–·
              2. å¾æ–‡ç»ä¸­æå–
              3. åŸºæ–¼çŸ¥è­˜åœ–è­œçµ±è¨ˆ
          """
          # ç°¡åŒ–ç‰ˆï¼šåŸºæ–¼ ClinVar è‡´ç—…æ€§
          pathogenicity_penetrance = {
              'Pathogenic': 0.8,
              'Likely pathogenic': 0.6,
              'VUS': 0.3,
              'Likely benign': 0.1,
              'Benign': 0.05
          }
          return pathogenicity_penetrance.get(
              clinvar_data.get('significance', 'VUS'),
              0.5
          )
      
      def _infer_inheritance_mode(self, gene, clinvar_data):
          """æ¨æ–·éºå‚³æ¨¡å¼"""
          # å¾ ClinVar æˆ–çŸ¥è­˜åœ–è­œæŸ¥è©¢
          kg_mode = self.kg.query_gene_attribute(gene, 'inheritance_mode')
          clinvar_mode = clinvar_data.get('inheritance', '')
          
          # å„ªå…ˆä½¿ç”¨ ClinVar
          if clinvar_mode:
              return clinvar_mode
          elif kg_mode:
              return kg_mode
          else:
              return 'unknown'
      
      def _search_literature(self, gene, phenotype):
          """æœå°‹ç›¸é—œæ–‡ç»"""
          # å¾ Pubtator æœ¬åœ°è³‡æ–™åº«æŸ¥è©¢
          papers = self.pubtator.search(
              entities=[gene, phenotype],
              relation_type='gene_phenotype',
              limit=20
          )
          return papers
      
      def _compute_final_score(self, gnn_score, clinvar_data, penetrance, lit_count):
          """ç¶œåˆè©•åˆ†"""
          # åŠ æ¬Šå¹³å‡
          weights = {
              'gnn': 0.4,
              'clinvar': 0.3,
              'penetrance': 0.2,
              'literature': 0.1
          }
          
          clinvar_score = self._clinvar_to_score(clinvar_data)
          lit_score = min(lit_count / 20.0, 1.0)
          
          final = (
              weights['gnn'] * gnn_score +
              weights['clinvar'] * clinvar_score +
              weights['penetrance'] * penetrance +
              weights['literature'] * lit_score
          )
          
          return final
      
      def _clinvar_to_score(self, clinvar_data):
          """ClinVar è‡´ç—…æ€§è½‰è©•åˆ†"""
          pathogenicity_scores = {
              'Pathogenic': 1.0,
              'Likely pathogenic': 0.8,
              'VUS': 0.5,
              'Likely benign': 0.2,
              'Benign': 0.0
          }
          return pathogenicity_scores.get(
              clinvar_data.get('significance', 'VUS'),
              0.5
          )
      
      def _classify_evidence(self, clinvar_data, literature):
          """åˆ†é¡è­‰æ“šå¼·åº¦"""
          if clinvar_data.get('significance') == 'Pathogenic' and len(literature) >= 10:
              return 'strong'
          elif len(literature) >= 5:
              return 'moderate'
          else:
              return 'weak'
  ```

- [ ] å‰µå»º ClinVar API åŒ…è£å™¨ ğŸ“… 0.5 å¤©
  ```python
  # src/data/integrations/clinvar_api.py
  
  """
  ClinVar API åŒ…è£å™¨
  """
  import requests
  from typing import Dict, List
  
  class ClinVarAPI:
      """ClinVar è®Šç•°è³‡æ–™åº« API"""
      
      BASE_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
      
      def search(self, gene: str, phenotype: str = None) -> List[Dict]:
          """
          æœå°‹ ClinVar è®Šç•°
          
          API: E-utilities
          """
          # æ§‹å»ºæŸ¥è©¢
          query = f"{gene}[gene]"
          if phenotype:
              query += f" AND {phenotype}[phenotype]"
          
          # å‘¼å« API
          response = requests.get(
              f"{self.BASE_URL}/esearch.fcgi",
              params={
                  'db': 'clinvar',
                  'term': query,
                  'retmode': 'json',
                  'retmax': 100
              }
          )
          
          # è§£æçµæœ
          return self._parse_response(response.json())
  ```

**å°è¨ˆ**: ğŸ“† 2-3 å¤©

---

## ğŸ“‹ Phase 2.5: è—¥ç‰©/æ²»ç™‚å»ºè­°ï¼ˆç ”ç©¶åƒè€ƒï¼‰(Week 2.5-3.5) âš ï¸

### ğŸ”´ é‡è¦è²æ˜

```
âš ï¸âš ï¸âš ï¸ æ³•å¾‹èˆ‡å€«ç†è²æ˜ âš ï¸âš ï¸âš ï¸

æ­¤åŠŸèƒ½åƒ…ä¾›ç ”ç©¶èˆ‡æ•™å­¸ç”¨é€”ï¼Œæä¾›çš„è—¥ç‰©å»ºè­°ï¼š
1. ä¸æ§‹æˆé†«ç™‚å»ºè­°æˆ–è™•æ–¹
2. å¿…é ˆç”±å°ˆæ¥­é†«å¸«å¯©æ ¸ç¢ºèª
3. ä¸å¾—ç›´æ¥ç”¨æ–¼è‡¨åºŠæ±ºç­–
4. ç³»çµ±é–‹ç™¼è€…ä¸æ‰¿æ“”ä»»ä½•æ³•å¾‹è²¬ä»»

ä½¿ç”¨å‰å¿…é ˆï¼š
- ç²å¾—é†«é™¢å€«ç†å§”å“¡æœƒæ‰¹å‡†
- åœ¨ UI é¡¯è‘—ä½ç½®æ¨™ç¤ºå…è²¬è²æ˜
- è¨˜éŒ„æ‰€æœ‰æŸ¥è©¢æ—¥èªŒä¾›å¯©è¨ˆ
- å®šæœŸç”±é†«å­¸å°ˆå®¶å¯©æ ¸å»ºè­°å“è³ª
```

### ğŸŸ¡ P2 - è—¥ç‰©çŸ¥è­˜åœ–è­œæ“´å±•

#### 3.1 è³‡æ–™æ•´åˆ

- [ ] ä¸‹è¼‰ DrugBank è³‡æ–™ ğŸ“… 0.5 å¤©
  ```bash
  # éœ€è¦è¨»å†Š DrugBank å¸³è™Ÿ
  # https://www.drugbank.ca/
  
  # ä¸‹è¼‰ä½ç½®
  data/raw/drugbank/
  â”œâ”€â”€ drugbank_all_full_database.xml
  â”œâ”€â”€ drug_links.csv
  â””â”€â”€ README.txt
  ```

- [ ] å‰µå»º DrugBank è§£æå™¨ ğŸ“… 1 å¤©
  ```python
  # src/data/parsers/drugbank_parser.py
  
  """
  DrugBank XML è§£æå™¨
  """
  import xml.etree.ElementTree as ET
  from typing import Dict, List
  
  class DrugBankParser:
      """è§£æ DrugBank è³‡æ–™åº«"""
      
      def parse(self, xml_file: str) -> List[Dict]:
          """
          è§£æ DrugBank XML
          
          æå–è³‡è¨Š:
              - è—¥ç‰©ID, åç¨±
              - é©æ‡‰ç—‡ (indications)
              - è—¥ç‰©-ç–¾ç—…é—œè¯
              - è—¥ç‰©-åŸºå› äº¤äº’ä½œç”¨
              - å‰¯ä½œç”¨
          """
          tree = ET.parse(xml_file)
          root = tree.getroot()
          
          drugs = []
          for drug_element in root.findall('.//drug'):
              drug_info = self._extract_drug_info(drug_element)
              drugs.append(drug_info)
          
          return drugs
      
      def _extract_drug_info(self, drug_element):
          """æå–å–®å€‹è—¥ç‰©è³‡è¨Š"""
          return {
              'drugbank_id': drug_element.find('drugbank-id').text,
              'name': drug_element.find('name').text,
              'indications': self._extract_indications(drug_element),
              'targets': self._extract_targets(drug_element),
              'side_effects': self._extract_side_effects(drug_element)
          }
  ```

- [ ] æ§‹å»ºè—¥ç‰©çŸ¥è­˜åœ–è­œ ğŸ“… 1 å¤©
  ```python
  # src/kg/drug_kg_builder.py
  
  """
  è—¥ç‰©çŸ¥è­˜åœ–è­œæ§‹å»ºå™¨
  """
  from typing import Dict, List
  import torch
  from torch_geometric.data import HeteroData
  
  class DrugKnowledgeGraphBuilder:
      """æ§‹å»ºåŒ…å«è—¥ç‰©çš„çŸ¥è­˜åœ–è­œ"""
      
      def build(self, drugbank_data, base_kg):
          """
          æ“´å±•ç¾æœ‰çŸ¥è­˜åœ–è­œåŠ å…¥è—¥ç‰©ç¯€é»
          
          æ–°å¢ç¯€é»é¡å‹:
              - drug
          
          æ–°å¢é‚Šé¡å‹:
              - (drug, treats, disease)
              - (drug, targets, gene)
              - (drug, causes, phenotype)  # å‰¯ä½œç”¨
          """
          # æ·»åŠ è—¥ç‰©ç¯€é»
          drug_nodes = [d['drugbank_id'] for d in drugbank_data]
          
          # æ·»åŠ è—¥ç‰©-ç–¾ç—…é‚Š
          drug_disease_edges = self._create_drug_disease_edges(drugbank_data)
          
          # æ·»åŠ è—¥ç‰©-åŸºå› é‚Š
          drug_gene_edges = self._create_drug_gene_edges(drugbank_data)
          
          # åˆä½µåˆ°ç¾æœ‰åœ–
          extended_kg = self._merge_with_base_kg(
              base_kg,
              drug_nodes,
              drug_disease_edges,
              drug_gene_edges
          )
          
          return extended_kg
  ```

#### 3.2 è—¥ç‰©å»ºè­°å¼•æ“

- [ ] å‰µå»º `src/treatment/drug_recommender.py` ğŸ“… 2 å¤©
  ```python
  """
  è—¥ç‰©å»ºè­°å¼•æ“ï¼ˆç ”ç©¶åƒè€ƒï¼‰
  
  âš ï¸ è­¦å‘Šï¼šæ­¤æ¨¡çµ„åƒ…ä¾›ç ”ç©¶åƒè€ƒï¼Œä¸å¾—ç”¨æ–¼è‡¨åºŠæ±ºç­–
  """
  from typing import Dict, List
  import logging
  
  logger = logging.getLogger(__name__)
  
  class DrugRecommendationEngine:
      """
      è—¥ç‰©å»ºè­°å¼•æ“
      
      âš ï¸ é‡è¦ï¼šæ‰€æœ‰è¼¸å‡ºå¿…é ˆåŒ…å«å…è²¬è²æ˜
      """
      
      DISCLAIMER = """
      âš ï¸âš ï¸âš ï¸ å…è²¬è²æ˜ âš ï¸âš ï¸âš ï¸
      
      ä»¥ä¸‹è—¥ç‰©å»ºè­°åƒ…ä¾›ç ”ç©¶åƒè€ƒï¼Œä¸æ§‹æˆé†«ç™‚å»ºè­°ã€‚
      æ‰€æœ‰æ²»ç™‚æ±ºç­–å¿…é ˆç”±å°ˆæ¥­é†«å¸«åŸºæ–¼æ‚£è€…å…·é«”æƒ…æ³åšå‡ºã€‚
      è«‹å‹¿åœ¨æœªç¶“é†«å¸«ç¢ºèªçš„æƒ…æ³ä¸‹ä½¿ç”¨ä»»ä½•è—¥ç‰©ã€‚
      
      æœ¬ç³»çµ±é–‹ç™¼è€…ä¸å°è—¥ç‰©ä½¿ç”¨å¾Œæœæ‰¿æ“”ä»»ä½•è²¬ä»»ã€‚
      """
      
      def __init__(self, drug_kg, confidence_threshold=0.6):
          self.drug_kg = drug_kg
          self.confidence_threshold = confidence_threshold
          
          # è¨˜éŒ„æ‰€æœ‰æŸ¥è©¢ï¼ˆå¯©è¨ˆç”¨ï¼‰
          self.audit_log = []
      
      def suggest_treatments(
          self,
          diagnosis_results: Dict,
          patient_genotype: List[str] = None,
          patient_allergies: List[str] = None
      ) -> Dict:
          """
          åŸºæ–¼è¨ºæ–·çµæœå»ºè­°æ²»ç™‚æ–¹æ¡ˆ
          
          Args:
              diagnosis_results: è¨ºæ–·çµæœï¼ˆå«ç–¾ç—…ã€åŸºå› ï¼‰
              patient_genotype: æ‚£è€…åŸºå› å‹ï¼ˆå¯é¸ï¼‰
              patient_allergies: è—¥ç‰©éæ•å²ï¼ˆå¯é¸ï¼‰
          
          Returns:
              {
                  'disclaimer': str,              # âš ï¸ å¿…é ˆåŒ…å«
                  'suggestions': List[Dict],      # è—¥ç‰©å»ºè­°
                  'confidence': str,              # 'low', 'medium', 'high'
                  'warnings': List[str],          # è­¦å‘Šè³‡è¨Š
                  'references': List[str]         # æ–‡ç»åƒè€ƒ
              }
          """
          # è¨˜éŒ„æŸ¥è©¢ï¼ˆå¯©è¨ˆï¼‰
          self._log_query(diagnosis_results, patient_genotype)
          
          # æå–ç–¾ç—…
          diseases = diagnosis_results.get('top_diseases', [])
          genes = diagnosis_results.get('top_genes', [])
          
          # æŸ¥è©¢è—¥ç‰©
          drug_candidates = self._query_drugs_for_diseases(diseases)
          
          # åŸºå› å‹éæ¿¾ï¼ˆè—¥ç‰©ä»£è¬ï¼‰
          if patient_genotype:
              drug_candidates = self._filter_by_genotype(
                  drug_candidates,
                  patient_genotype
              )
          
          # éæ•å²éæ¿¾
          if patient_allergies:
              drug_candidates = self._filter_by_allergies(
                  drug_candidates,
                  patient_allergies
              )
          
          # è©•åˆ†èˆ‡æ’åº
          ranked_drugs = self._rank_drugs(drug_candidates, diseases, genes)
          
          # ç½®ä¿¡åº¦è©•ä¼°
          confidence = self._assess_confidence(ranked_drugs)
          
          # ç”Ÿæˆè­¦å‘Š
          warnings = self._generate_warnings(ranked_drugs, diseases)
          
          return {
              'disclaimer': self.DISCLAIMER,  # âš ï¸ å¼·åˆ¶åŒ…å«
              'suggestions': ranked_drugs[:10],  # å‰10å€‹
              'confidence': confidence,
              'warnings': warnings,
              'references': self._collect_references(ranked_drugs)
          }
      
      def _query_drugs_for_diseases(self, diseases):
          """æŸ¥è©¢æ²»ç™‚é€™äº›ç–¾ç—…çš„è—¥ç‰©"""
          drugs = []
          for disease in diseases:
              disease_drugs = self.drug_kg.query_edges(
                  source_type='drug',
                  relation='treats',
                  target=disease['id']
              )
              drugs.extend(disease_drugs)
          return drugs
      
      def _filter_by_genotype(self, drugs, genotype):
          """
          åŸºæ–¼åŸºå› å‹éæ¿¾è—¥ç‰©
          
          è€ƒæ…®å› ç´ :
              - è—¥ç‰©ä»£è¬é…¶åŸºå› å‹ï¼ˆCYP450å®¶æ—ï¼‰
              - è—¥ç‰©è½‰é‹é«”åŸºå› å‹
              - è—¥ç‰©é¶é»åŸºå› å‹
          """
          # æª¢æŸ¥ CYP450 ä»£è¬
          metabolizer_status = self._predict_metabolizer_status(genotype)
          
          filtered_drugs = []
          for drug in drugs:
              # æª¢æŸ¥æ˜¯å¦éœ€è¦ç‰¹å®šä»£è¬èƒ½åŠ›
              if self._is_compatible_with_metabolism(drug, metabolizer_status):
                  filtered_drugs.append(drug)
          
          return filtered_drugs
      
      def _predict_metabolizer_status(self, genotype):
          """
          é æ¸¬è—¥ç‰©ä»£è¬èƒ½åŠ›
          
          åˆ†é¡:
              - ultra_rapid_metabolizer
              - extensive_metabolizer (æ­£å¸¸)
              - intermediate_metabolizer
              - poor_metabolizer
          """
          # ç°¡åŒ–ç‰ˆï¼šæª¢æŸ¥ CYP2D6, CYP2C19 ç­‰
          # å¯¦éš›æ‡‰ä½¿ç”¨ PharmGKB è³‡æ–™åº«
          return 'extensive_metabolizer'  # é è¨­
      
      def _rank_drugs(self, drugs, diseases, genes):
          """
          è—¥ç‰©æ’åº
          
          è©•åˆ†å› ç´ :
              1. ç–¾ç—…é©æ‡‰ç—‡åŒ¹é…åº¦ (40%)
              2. è—¥ç‰©-åŸºå› äº¤äº’ä½œç”¨ (20%)
              3. æ–‡ç»æ”¯æŒå¼·åº¦ (20%)
              4. å‰¯ä½œç”¨åš´é‡ç¨‹åº¦ (10%)
              5. è‡¨åºŠä½¿ç”¨é »ç‡ (10%)
          """
          scored_drugs = []
          
          for drug in drugs:
              indication_score = self._score_indication_match(drug, diseases)
              gene_interaction_score = self._score_gene_interaction(drug, genes)
              literature_score = self._score_literature_support(drug, diseases)
              safety_score = self._score_safety_profile(drug)
              usage_score = self._score_clinical_usage(drug)
              
              total_score = (
                  0.4 * indication_score +
                  0.2 * gene_interaction_score +
                  0.2 * literature_score +
                  0.1 * safety_score +
                  0.1 * usage_score
              )
              
              scored_drugs.append({
                  'drug_id': drug['drugbank_id'],
                  'drug_name': drug['name'],
                  'score': total_score,
                  'indication': drug.get('indication', ''),
                  'mechanism': drug.get('mechanism_of_action', ''),
                  'side_effects': drug.get('side_effects', [])[:5],
                  'contraindications': drug.get('contraindications', []),
                  'evidence_level': self._classify_evidence(literature_score)
              })
          
          return sorted(scored_drugs, key=lambda x: x['score'], reverse=True)
      
      def _assess_confidence(self, ranked_drugs):
          """
          è©•ä¼°å»ºè­°çš„ç½®ä¿¡åº¦
          
          è¦å‰‡:
              - å‰3å€‹è—¥ç‰©åˆ†æ•¸ > 0.8 â†’ high
              - å‰3å€‹è—¥ç‰©åˆ†æ•¸ > 0.6 â†’ medium
              - å…¶ä»– â†’ low
          """
          if not ranked_drugs:
              return 'low'
          
          top_3_scores = [d['score'] for d in ranked_drugs[:3]]
          avg_score = sum(top_3_scores) / len(top_3_scores)
          
          if avg_score > 0.8:
              return 'high'
          elif avg_score > 0.6:
              return 'medium'
          else:
              return 'low'
      
      def _generate_warnings(self, drugs, diseases):
          """
          ç”Ÿæˆè­¦å‘Šè³‡è¨Š
          
          è­¦å‘Šé¡å‹:
              - è—¥ç‰©äº¤äº’ä½œç”¨
              - åš´é‡å‰¯ä½œç”¨
              - ç¦å¿Œç—‡
              - ç‰¹æ®Šäººç¾¤æ³¨æ„äº‹é …
          """
          warnings = []
          
          # æª¢æŸ¥ç¦å¿Œç—‡
          for drug in drugs[:5]:
              if drug.get('contraindications'):
                  warnings.append(
                      f"âš ï¸ {drug['drug_name']}: ç¦å¿Œç—‡åŒ…æ‹¬ {drug['contraindications']}"
                  )
          
          # æª¢æŸ¥åš´é‡å‰¯ä½œç”¨
          for drug in drugs[:5]:
              serious_effects = [
                  se for se in drug.get('side_effects', [])
                  if 'serious' in se.lower() or 'severe' in se.lower()
              ]
              if serious_effects:
                  warnings.append(
                      f"âš ï¸ {drug['drug_name']}: å¯èƒ½æœ‰åš´é‡å‰¯ä½œç”¨"
                  )
          
          return warnings
      
      def _log_query(self, diagnosis, genotype):
          """è¨˜éŒ„æŸ¥è©¢ï¼ˆå¯©è¨ˆç”¨ï¼‰"""
          import datetime
          log_entry = {
              'timestamp': datetime.datetime.now().isoformat(),
              'diagnosis': diagnosis,
              'genotype': genotype
          }
          self.audit_log.append(log_entry)
          logger.info(f"Drug recommendation query: {log_entry}")
  ```

#### 3.3 API èˆ‡ WebUI æ•´åˆ

- [ ] å‰µå»ºè—¥ç‰©å»ºè­° API ç«¯é» ğŸ“… 0.5 å¤©
  ```python
  # src/api/routes/treatment.py
  
  from fastapi import APIRouter, HTTPException, Depends
  from pydantic import BaseModel
  
  router = APIRouter(prefix="/api/v2/treatment", tags=["treatment"])
  
  class TreatmentRequest(BaseModel):
      diagnosis_results: dict
      patient_genotype: list = None
      patient_allergies: list = None
      acknowledge_disclaimer: bool  # âš ï¸ å¿…é ˆç¢ºèª
  
  @router.post("/suggest-drugs")
  async def suggest_drug_treatments(request: TreatmentRequest):
      """
      è—¥ç‰©æ²»ç™‚å»ºè­°ï¼ˆâš ï¸ ç ”ç©¶åƒè€ƒï¼‰
      
      âš ï¸ è­¦å‘Šï¼šå¿…é ˆåœ¨è«‹æ±‚ä¸­ç¢ºèªå…è²¬è²æ˜
      """
      # å¼·åˆ¶ç¢ºèªå…è²¬è²æ˜
      if not request.acknowledge_disclaimer:
          raise HTTPException(
              status_code=400,
              detail="å¿…é ˆç¢ºèªå…è²¬è²æ˜æ‰èƒ½ä½¿ç”¨æ­¤åŠŸèƒ½"
          )
      
      # ç”Ÿæˆå»ºè­°
      recommender = DrugRecommendationEngine(drug_kg=global_drug_kg)
      result = recommender.suggest_treatments(
          request.diagnosis_results,
          request.patient_genotype,
          request.patient_allergies
      )
      
      return result
  ```

- [ ] WebUI è—¥ç‰©å»ºè­°ä»‹é¢ ğŸ“… 1 å¤©
  ```python
  # webui/components/treatment_tab.py
  
  with gr.Tab("ğŸ’Š æ²»ç™‚å»ºè­°ï¼ˆç ”ç©¶åƒè€ƒï¼‰"):
      # âš ï¸ é¡¯è‘—çš„å…è²¬è²æ˜
      gr.Markdown("""
      ## âš ï¸âš ï¸âš ï¸ é‡è¦è­¦å‘Š âš ï¸âš ï¸âš ï¸
      
      æ­¤åŠŸèƒ½æä¾›çš„è—¥ç‰©å»ºè­°åƒ…ä¾›ç ”ç©¶èˆ‡æ•™å­¸åƒè€ƒï¼š
      - âŒ ä¸æ§‹æˆé†«ç™‚å»ºè­°æˆ–è™•æ–¹
      - âŒ ä¸å¾—ç›´æ¥ç”¨æ–¼è‡¨åºŠæ±ºç­–
      - âœ… å¿…é ˆç”±å°ˆæ¥­é†«å¸«å¯©æ ¸ç¢ºèª
      - âœ… åƒ…ä½œç‚ºæ²»ç™‚æ€è·¯çš„åˆæ­¥åƒè€ƒ
      
      **ä½¿ç”¨æ­¤åŠŸèƒ½å³è¡¨ç¤ºæ‚¨å·²ç†è§£ä¸¦åŒæ„ä¸Šè¿°è²æ˜**
      """, elem_classes=["warning-box"])
      
      acknowledge_checkbox = gr.Checkbox(
          label="âœ… æˆ‘å·²é–±è®€ä¸¦ç†è§£ä¸Šè¿°å…è²¬è²æ˜",
          value=False
      )
      
      diagnosis_input = gr.JSON(label="è¨ºæ–·çµæœï¼ˆå¾æ¨ç†é é¢è¤‡è£½ï¼‰")
      
      genotype_input = gr.Textbox(
          label="æ‚£è€…åŸºå› å‹ï¼ˆå¯é¸ï¼‰",
          placeholder="ä¾‹å¦‚: CYP2D6*1/*4"
      )
      
      allergies_input = gr.Textbox(
          label="è—¥ç‰©éæ•å²ï¼ˆå¯é¸ï¼‰",
          placeholder="ä¾‹å¦‚: Penicillin, Aspirin"
      )
      
      suggest_btn = gr.Button(
          "ç”Ÿæˆè—¥ç‰©å»ºè­°",
          variant="primary",
          interactive=False  # é è¨­ä¸å¯é»æ“Š
      )
      
      # çµæœé¡¯ç¤º
      disclaimer_display = gr.Markdown()
      confidence_display = gr.Textbox(label="ç½®ä¿¡åº¦")
      warnings_display = gr.Markdown(label="âš ï¸ è­¦å‘Š")
      suggestions_table = gr.DataFrame(
          headers=["è—¥ç‰©", "è©•åˆ†", "é©æ‡‰ç—‡", "è­‰æ“šç­‰ç´š"],
          label="å»ºè­°è—¥ç‰©"
      )
      references_display = gr.Markdown(label="åƒè€ƒæ–‡ç»")
      
      # ç¢ºèªå…è²¬è²æ˜å¾Œæ‰èƒ½é»æ“Š
      def enable_button(acknowledged):
          return gr.Button.update(interactive=acknowledged)
      
      acknowledge_checkbox.change(
          fn=enable_button,
          inputs=[acknowledge_checkbox],
          outputs=[suggest_btn]
      )
  ```

**å°è¨ˆ**: ğŸ“† 5-6 å¤©

---

## ğŸ“‹ Phase 2.6: æ–‡ç»æª¢ç´¢èˆ‡å¯ä¿¡åº¦æ’åº (Week 3.5-4.5)

### ğŸŸ  P1 - æ–‡ç»æª¢ç´¢å¼•æ“

#### 4.1 æ··åˆå¼æª¢ç´¢ç³»çµ±

- [ ] å‰µå»º `src/literature/__init__.py` ğŸ• 5min

- [ ] å‰µå»º `src/literature/hybrid_retrieval.py` ğŸ“… 2 å¤©
  ```python
  """
  æ··åˆå¼æ–‡ç»æª¢ç´¢ç³»çµ±
  
  æ¨¡å¼:
      - offline: åƒ…ä½¿ç”¨é ä¸‹è¼‰çš„ Pubtator è³‡æ–™
      - online: å…è¨±å³æ™‚æŸ¥è©¢ PubMed APIï¼ˆéœ€é†«é™¢æ‰¹å‡†ï¼‰
  """
  from typing import Dict, List
  import requests
  from src.data.parsers.pubtator_parser import PubtatorParser
  
  class HybridLiteratureRetrieval:
      """æ··åˆå¼æ–‡ç»æª¢ç´¢èˆ‡æ’åº"""
      
      def __init__(self, mode='offline', pubmed_api_key=None):
          """
          Args:
              mode: 'offline' æˆ– 'online'
              pubmed_api_key: PubMed API é‡‘é‘°ï¼ˆç·šä¸Šæ¨¡å¼å¿…éœ€ï¼‰
          """
          self.mode = mode
          
          # é›¢ç·šè³‡æ–™åº«ï¼ˆPubtator é ä¸‹è¼‰ï¼‰
          self.offline_db = self._load_pubtator_database()
          
          # ç·šä¸Š APIï¼ˆå¯é¸ï¼‰
          if mode == 'online':
              if not pubmed_api_key:
                  raise ValueError("ç·šä¸Šæ¨¡å¼éœ€è¦ PubMed API é‡‘é‘°")
              self.pubmed_api = PubMedAPI(api_key=pubmed_api_key)
          else:
              self.pubmed_api = None
      
      def retrieve_and_rank(
          self,
          diagnosis_results: Dict,
          max_results: int = 10
      ) -> List[Dict]:
          """
          æª¢ç´¢ä¸¦æ’åºç›¸é—œæ–‡ç»
          
          Returns:
              List[{
                  'pmid': str,
                  'title': str,
                  'abstract': str,
                  'authors': List[str],
                  'journal': str,
                  'publication_date': str,
                  'relevance_score': float,       # ç›¸é—œæ€§
                  'credibility_score': float,     # å¯ä¿¡åº¦
                  'combined_score': float,        # ç¶œåˆè©•åˆ†
                  'citation_count': int,
                  'journal_impact_factor': float,
                  'evidence_level': str,          # è­‰æ“šç­‰ç´š
                  'study_type': str               # ç ”ç©¶é¡å‹
              }]
          """
          # æå–é—œéµå¯¦é«”
          diseases = [d['id'] for d in diagnosis_results.get('top_diseases', [])]
          genes = [g['id'] for g in diagnosis_results.get('top_genes', [])]
          
          # 1. é›¢ç·šæª¢ç´¢
          offline_papers = self._search_offline(diseases, genes, limit=50)
          
          # 2. ç·šä¸Šè£œå……ï¼ˆå¦‚æœå…è¨±ï¼‰
          online_papers = []
          if self.mode == 'online' and self.pubmed_api:
              online_papers = self._search_online(diseases, genes, limit=20)
          
          # 3. åˆä½µå»é‡
          all_papers = self._merge_papers(offline_papers, online_papers)
          
          # 4. å¤šç¶­åº¦è©•åˆ†
          scored_papers = [
              self._score_paper(paper, diseases, genes)
              for paper in all_papers
          ]
          
          # 5. æ’åº
          ranked_papers = sorted(
              scored_papers,
              key=lambda x: x['combined_score'],
              reverse=True
          )
          
          return ranked_papers[:max_results]
      
      def _search_offline(self, diseases, genes, limit):
          """å¾ Pubtator æœ¬åœ°è³‡æ–™åº«æœå°‹"""
          papers = []
          
          # æŸ¥è©¢ç–¾ç—…ç›¸é—œæ–‡ç»
          for disease in diseases:
              disease_papers = self.offline_db.search_by_entity(
                  entity_id=disease,
                  entity_type='disease',
                  limit=limit
              )
              papers.extend(disease_papers)
          
          # æŸ¥è©¢åŸºå› ç›¸é—œæ–‡ç»
          for gene in genes:
              gene_papers = self.offline_db.search_by_entity(
                  entity_id=gene,
                  entity_type='gene',
                  limit=limit
              )
              papers.extend(gene_papers)
          
          return papers
      
      def _search_online(self, diseases, genes, limit):
          """å¾ PubMed API æœå°‹"""
          if not self.pubmed_api:
              return []
          
          # æ§‹å»ºæŸ¥è©¢å­—ä¸²
          query_terms = diseases + genes + ['rare disease']
          query = ' AND '.join(query_terms)
          
          # å‘¼å« API
          papers = self.pubmed_api.search(
              query=query,
              max_results=limit,
              sort='relevance'
          )
          
          return papers
      
      def _merge_papers(self, offline_papers, online_papers):
          """åˆä½µä¸¦å»é‡"""
          # ä½¿ç”¨ PMID å»é‡
          seen_pmids = set()
          merged = []
          
          for paper in offline_papers + online_papers:
              pmid = paper.get('pmid')
              if pmid and pmid not in seen_pmids:
                  seen_pmids.add(pmid)
                  merged.append(paper)
          
          return merged
      
      def _score_paper(self, paper, diseases, genes):
          """
          å¤šç¶­åº¦è©•åˆ†
          
          ç¶­åº¦:
              1. ç›¸é—œæ€§ (40%): èˆ‡è¨ºæ–·å¯¦é«”çš„ç›¸é—œç¨‹åº¦
              2. å¯ä¿¡åº¦ (30%): æœŸåˆŠã€è­‰æ“šç­‰ç´š
              3. æ™‚æ•ˆæ€§ (20%): ç™¼è¡¨æ™‚é–“
              4. å½±éŸ¿åŠ› (10%): å¼•ç”¨æ¬¡æ•¸
          """
          # 1. ç›¸é—œæ€§è©•åˆ†
          relevance_score = self._compute_relevance(paper, diseases, genes)
          
          # 2. å¯ä¿¡åº¦è©•åˆ†
          credibility_score = self._compute_credibility(paper)
          
          # 3. æ™‚æ•ˆæ€§è©•åˆ†
          recency_score = self._compute_recency(paper)
          
          # 4. å½±éŸ¿åŠ›è©•åˆ†
          impact_score = self._compute_impact(paper)
          
          # ç¶œåˆè©•åˆ†
          combined_score = (
              0.4 * relevance_score +
              0.3 * credibility_score +
              0.2 * recency_score +
              0.1 * impact_score
          )
          
          # æ·»åŠ è©•åˆ†åˆ°è«–æ–‡è³‡è¨Š
          paper['relevance_score'] = relevance_score
          paper['credibility_score'] = credibility_score
          paper['combined_score'] = combined_score
          
          return paper
      
      def _compute_relevance(self, paper, diseases, genes):
          """
          è¨ˆç®—ç›¸é—œæ€§
          
          æ–¹æ³•:
              - å¯¦é«”å…±ç¾ï¼šè«–æ–‡ä¸­æåŠçš„ç–¾ç—…/åŸºå› æ•¸é‡
              - æ¨™é¡ŒåŒ¹é…ï¼šé—œéµè©åœ¨æ¨™é¡Œä¸­å‡ºç¾
              - æ‘˜è¦åŒ¹é…ï¼šé—œéµè©åœ¨æ‘˜è¦ä¸­å‡ºç¾
          """
          score = 0.0
          
          # æª¢æŸ¥æ¨™é¡Œ
          title = paper.get('title', '').lower()
          for entity in diseases + genes:
              if entity.lower() in title:
                  score += 0.3
          
          # æª¢æŸ¥æ‘˜è¦
          abstract = paper.get('abstract', '').lower()
          for entity in diseases + genes:
              if entity.lower() in abstract:
                  score += 0.1
          
          # æ­¸ä¸€åŒ–
          return min(score, 1.0)
      
      def _compute_credibility(self, paper):
          """
          è¨ˆç®—å¯ä¿¡åº¦
          
          å› ç´ :
              - æœŸåˆŠå½±éŸ¿å› å­ (40%)
              - è­‰æ“šç­‰ç´š (30%): Meta-analysis > RCT > Cohort > Case
              - ä½œè€…æ©Ÿæ§‹ (20%): é ‚ç´šé†«å­¸ä¸­å¿ƒåŠ åˆ†
              - åŒè¡Œè©•å¯© (10%)
          """
          score = 0.0
          
          # æœŸåˆŠå½±éŸ¿å› å­ï¼ˆå‡è¨­å·²æœ‰è³‡æ–™ï¼‰
          if 'journal_impact_factor' in paper:
              if_score = min(paper['journal_impact_factor'] / 50.0, 1.0)
              score += if_score * 0.4
          
          # è­‰æ“šç­‰ç´š
          evidence_weights = {
              'meta-analysis': 1.0,
              'systematic_review': 0.9,
              'randomized_controlled_trial': 0.8,
              'cohort_study': 0.6,
              'case_control_study': 0.5,
              'case_report': 0.3,
              'review': 0.4
          }
          evidence_level = paper.get('evidence_level', 'case_report')
          score += evidence_weights.get(evidence_level, 0.3) * 0.3
          
          # ä½œè€…æ©Ÿæ§‹ï¼ˆç°¡åŒ–ç‰ˆï¼‰
          affiliations = paper.get('affiliations', [])
          top_institutions = [
              'Harvard', 'Stanford', 'Mayo Clinic', 'Johns Hopkins',
              'NIH', 'Cambridge', 'Oxford'
          ]
          if any(inst in str(affiliations) for inst in top_institutions):
              score += 0.2
          
          # åŒè¡Œè©•å¯©ï¼ˆé è¨­ç‚ºæ˜¯ï¼‰
          score += 0.1
          
          return min(score, 1.0)
      
      def _compute_recency(self, paper):
          """
          è¨ˆç®—æ™‚æ•ˆæ€§
          
          è¦å‰‡:
              - 5å¹´å…§: 1.0
              - 10å¹´å…§: 0.7
              - 15å¹´å…§: 0.5
              - æ›´æ—©: 0.3
          """
          import datetime
          
          pub_date = paper.get('publication_date')
          if not pub_date:
              return 0.5
          
          try:
              pub_year = int(pub_date.split('-')[0])
              current_year = datetime.datetime.now().year
              years_ago = current_year - pub_year
              
              if years_ago <= 5:
                  return 1.0
              elif years_ago <= 10:
                  return 0.7
              elif years_ago <= 15:
                  return 0.5
              else:
                  return 0.3
          except:
              return 0.5
      
      def _compute_impact(self, paper):
          """
          è¨ˆç®—å½±éŸ¿åŠ›ï¼ˆå¼•ç”¨æ¬¡æ•¸ï¼‰
          
          æ­¸ä¸€åŒ–ï¼šå¼•ç”¨æ¬¡æ•¸ / 1000
          """
          citations = paper.get('citation_count', 0)
          return min(citations / 1000.0, 1.0)
  ```

#### 4.2 PubMed API åŒ…è£å™¨

- [ ] å‰µå»º `src/literature/pubmed_api.py` ğŸ“… 0.5 å¤©
  ```python
  """
  PubMed E-utilities API åŒ…è£å™¨
  """
  import requests
  import time
  from typing import List, Dict
  
  class PubMedAPI:
      """PubMed API å®¢æˆ¶ç«¯"""
      
      BASE_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
      
      def __init__(self, api_key=None, rate_limit=3):
          """
          Args:
              api_key: NCBI API é‡‘é‘°
              rate_limit: æ¯ç§’è«‹æ±‚æ•¸é™åˆ¶
          """
          self.api_key = api_key
          self.rate_limit = rate_limit
          self.last_request_time = 0
      
      def search(self, query: str, max_results: int = 20, sort='relevance'):
          """
          æœå°‹ PubMed
          
          API: esearch + efetch
          """
          # 1. æœå°‹ç²å– PMID åˆ—è¡¨
          pmids = self._esearch(query, max_results, sort)
          
          # 2. ç²å–è©³ç´°è³‡è¨Š
          papers = self._efetch(pmids)
          
          return papers
      
      def _esearch(self, query, max_results, sort):
          """æœå°‹ PMID"""
          self._wait_for_rate_limit()
          
          params = {
              'db': 'pubmed',
              'term': query,
              'retmax': max_results,
              'retmode': 'json',
              'sort': sort
          }
          
          if self.api_key:
              params['api_key'] = self.api_key
          
          response = requests.get(f"{self.BASE_URL}esearch.fcgi", params=params)
          data = response.json()
          
          pmids = data.get('esearchresult', {}).get('idlist', [])
          return pmids
      
      def _efetch(self, pmids):
          """ç²å–è«–æ–‡è©³ç´°è³‡è¨Š"""
          if not pmids:
              return []
          
          self._wait_for_rate_limit()
          
          params = {
              'db': 'pubmed',
              'id': ','.join(pmids),
              'retmode': 'xml'
          }
          
          if self.api_key:
              params['api_key'] = self.api_key
          
          response = requests.get(f"{self.BASE_URL}efetch.fcgi", params=params)
          
          # è§£æ XMLï¼ˆç°¡åŒ–ç‰ˆï¼‰
          papers = self._parse_pubmed_xml(response.text)
          return papers
      
      def _wait_for_rate_limit(self):
          """éµå®ˆé€Ÿç‡é™åˆ¶"""
          elapsed = time.time() - self.last_request_time
          wait_time = 1.0 / self.rate_limit
          
          if elapsed < wait_time:
              time.sleep(wait_time - elapsed)
          
          self.last_request_time = time.time()
      
      def _parse_pubmed_xml(self, xml_text):
          """è§£æ PubMed XMLï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
          # TODO: å®Œæ•´çš„ XML è§£æ
          # é€™è£¡åƒ…ç¤ºæ„
          return []
  ```

#### 4.3 API èˆ‡ WebUI æ•´åˆ

- [ ] å‰µå»ºæ–‡ç»æª¢ç´¢ API ç«¯é» ğŸ“… 0.5 å¤©
  ```python
  # src/api/routes/literature.py
  
  from fastapi import APIRouter
  from pydantic import BaseModel
  
  router = APIRouter(prefix="/api/v2/literature", tags=["literature"])
  
  class LiteratureRequest(BaseModel):
      diagnosis_results: dict
      mode: str = 'offline'  # 'offline' æˆ– 'online'
      max_results: int = 10
  
  @router.post("/search")
  async def search_literature(request: LiteratureRequest):
      """
      æª¢ç´¢ç›¸é—œæ–‡ç»
      
      æ¨¡å¼:
          - offline: åƒ…æœ¬åœ° Pubtator è³‡æ–™
          - online: å…è¨± PubMed APIï¼ˆéœ€è¨­å®šï¼‰
      """
      retriever = HybridLiteratureRetrieval(mode=request.mode)
      papers = retriever.retrieve_and_rank(
          request.diagnosis_results,
          max_results=request.max_results
      )
      return {'papers': papers}
  ```

- [ ] WebUI æ–‡ç»æª¢ç´¢ä»‹é¢ ğŸ“… 1 å¤©
  ```python
  # webui/components/literature_tab.py
  
  with gr.Tab("ğŸ“š æ–‡ç»æª¢ç´¢"):
      gr.Markdown("## ç›¸é—œæ–‡ç»æª¢ç´¢èˆ‡æ’åº")
      
      diagnosis_input = gr.JSON(label="è¨ºæ–·çµæœï¼ˆå¾æ¨ç†é é¢è¤‡è£½ï¼‰")
      
      mode_radio = gr.Radio(
          choices=["offline", "online"],
          value="offline",
          label="æª¢ç´¢æ¨¡å¼",
          info="offline: åƒ…æœ¬åœ°è³‡æ–™ | online: åŒ…å«å³æ™‚ PubMed æŸ¥è©¢"
      )
      
      max_results_slider = gr.Slider(
          minimum=5,
          maximum=50,
          value=10,
          step=5,
          label="æœ€å¤§çµæœæ•¸"
      )
      
      search_btn = gr.Button("æª¢ç´¢æ–‡ç»", variant="primary")
      
      # çµæœé¡¯ç¤º
      papers_table = gr.DataFrame(
          headers=[
              "PMID", "æ¨™é¡Œ", "æœŸåˆŠ", "å¹´ä»½",
              "ç›¸é—œæ€§", "å¯ä¿¡åº¦", "ç¶œåˆè©•åˆ†"
          ],
          label="æª¢ç´¢çµæœ"
      )
      
      # é¸ä¸­è«–æ–‡çš„è©³ç´°è³‡è¨Š
      paper_detail = gr.HTML(label="è«–æ–‡è©³æƒ…")
  ```

**å°è¨ˆ**: ğŸ“† 4-5 å¤©

---

## ğŸ“Š é©—æ”¶æ¨™æº–

### åŠŸèƒ½ 1: ç—‡ç‹€é—œè¯åˆ†æ âœ…
- [ ] èƒ½åˆ†æå…©ç—‡ç‹€çš„å¤šç¶­åº¦é—œè¯
- [ ] æä¾›ç”Ÿç‰©å­¸æ©Ÿåˆ¶æ¨æ–·
- [ ] API å¯æ­£å¸¸é‹ä½œ
- [ ] WebUI å¯è¦–åŒ–å±•ç¤º

### åŠŸèƒ½ 2: åŸºå› å‹-è¡¨å‹æ’åº âœ…
- [ ] æ•´åˆ ClinVar è®Šç•°è³‡è¨Š
- [ ] æä¾›å¤–é¡¯ç‡ä¼°ç®—
- [ ] åŒ…å«æ–‡ç»æ”¯æŒè©•åˆ†
- [ ] è©³ç´°çš„è­‰æ“šå¼·åº¦åˆ†é¡

### åŠŸèƒ½ 3: è—¥ç‰©å»ºè­° âœ… ï¼ˆâš ï¸ é™„å¸¶è­¦å‘Šï¼‰
- [ ] **å…è²¬è²æ˜åœ¨æ‰€æœ‰è¼¸å‡ºä¸­é¡¯è‘—æ¨™ç¤º**
- [ ] è—¥ç‰©çŸ¥è­˜åœ–è­œæ§‹å»ºå®Œæˆ
- [ ] åŸºå› å‹è—¥ç‰©ä»£è¬éæ¿¾
- [ ] ç½®ä¿¡åº¦è©•ä¼°èˆ‡è­¦å‘Šç”Ÿæˆ
- [ ] æ‰€æœ‰æŸ¥è©¢è¨˜éŒ„å¯©è¨ˆæ—¥èªŒ

### åŠŸèƒ½ 4: æ–‡ç»æª¢ç´¢ âœ…
- [ ] é›¢ç·šæ¨¡å¼å®Œå…¨å¯ç”¨
- [ ] ç·šä¸Šæ¨¡å¼ï¼ˆå¯é¸ï¼‰å¯¦ç¾
- [ ] å¤šç¶­åº¦å¯ä¿¡åº¦è©•åˆ†
- [ ] æ’åºçµæœæº–ç¢º

---

## ğŸ”§ ä¾è³´èˆ‡æ³¨æ„äº‹é …

### å¤–éƒ¨å¥—ä»¶
```bash
# åŠŸèƒ½ 1-2: åˆ†ææ¨¡çµ„
# ï¼ˆä½¿ç”¨ç¾æœ‰å¥—ä»¶ï¼Œç„¡æ–°å¢ï¼‰

# åŠŸèƒ½ 3: è—¥ç‰©å»ºè­°
pip install drugbank-parser==0.1.0  # æˆ–æ‰‹å‹•è§£æ

# åŠŸèƒ½ 4: æ–‡ç»æª¢ç´¢
pip install biopython==1.81  # PubMed API
pip install xmltodict==0.13.0  # XML è§£æ
```

### è³‡æ–™éœ€æ±‚
- [ ] ä¸‹è¼‰ DrugBank å®Œæ•´è³‡æ–™åº«ï¼ˆéœ€è¨»å†Šï¼‰
- [ ] ä¸‹è¼‰ Pubtator 3.0 é è™•ç†è³‡æ–™
- [ ] æº–å‚™ ClinVar è®Šç•°è³‡æ–™ï¼ˆå¯é€é APIï¼‰

### å‘¼å«ç¾æœ‰æ¨¡çµ„
```python
# åŠŸèƒ½ 1: ç—‡ç‹€é—œè¯
from src.kg.builder import KnowledgeGraphBuilder  # âœ…
from src.retrieval.path_retriever import PathRetriever  # âœ…
from src.ontology.similarity import OntologySimilarity  # âœ…

# åŠŸèƒ½ 2: åŸºå› æ’åº
from src.models.tasks.gene_scoring import GeneScoring  # âœ…

# åŠŸèƒ½ 4: æ–‡ç»æª¢ç´¢
# ï¼ˆæ–°æ¨¡çµ„ï¼Œç„¡ä¾è³´è¡çªï¼‰
```

---

## ğŸ¯ å„ªå…ˆç´šå»ºè­°

| åŠŸèƒ½ | Phase 2 | Phase 3 | å‚™è¨» |
|------|---------|---------|------|
| **ç—‡ç‹€é—œè¯åˆ†æ** | âœ… å¯¦ç¾ | - | æ ¸å¿ƒåŠŸèƒ½æ“´å±• |
| **åŸºå› æ’åºå¢å¼·** | âœ… å¯¦ç¾ | - | æå‡è¨ºæ–·åƒ¹å€¼ |
| **è—¥ç‰©å»ºè­°** | ğŸŸ¡ å¯é¸ | âœ… å®Œæ•´ | âš ï¸ éœ€å¯©æ…è©•ä¼° |
| **æ–‡ç»æª¢ç´¢** | âœ… é›¢ç·š | âœ… ç·šä¸Š | å¯¦ç”¨æ€§é«˜ |

---

## ğŸ“… æ™‚é–“è¦åŠƒ

```
Week 1 (Day 1-5): ç—‡ç‹€é—œè¯åˆ†æ
â”œâ”€â”€ Day 1-2: æ ¸å¿ƒåˆ†æå™¨å¯¦ç¾
â”œâ”€â”€ Day 3: API èˆ‡ WebUI æ•´åˆ
â””â”€â”€ Day 4-5: æ¸¬è©¦èˆ‡èª¿å„ª

Week 2 (Day 1-5): åŸºå› æ’åºå¢å¼· + è—¥ç‰©å»ºè­°ï¼ˆèµ·æ­¥ï¼‰
â”œâ”€â”€ Day 1-2: å¢å¼·æ’åºæ¨¡çµ„
â”œâ”€â”€ Day 3: DrugBank æ•´åˆ
â”œâ”€â”€ Day 4-5: è—¥ç‰©çŸ¥è­˜åœ–è­œæ§‹å»º

Week 3 (Day 1-5): è—¥ç‰©å»ºè­°ï¼ˆå®Œæˆï¼‰
â”œâ”€â”€ Day 1-2: è—¥ç‰©å»ºè­°å¼•æ“
â”œâ”€â”€ Day 3: API æ•´åˆ + å…è²¬è²æ˜
â”œâ”€â”€ Day 4: WebUI æ•´åˆ
â””â”€â”€ Day 5: æ¸¬è©¦èˆ‡å¯©æ ¸

Week 4 (Day 1-5): æ–‡ç»æª¢ç´¢
â”œâ”€â”€ Day 1-2: æ··åˆæª¢ç´¢å¼•æ“
â”œâ”€â”€ Day 3: å¯ä¿¡åº¦è©•åˆ†ç³»çµ±
â”œâ”€â”€ Day 4: API èˆ‡ WebUI
â””â”€â”€ Day 5: ç«¯åˆ°ç«¯æ¸¬è©¦
```

**ç¸½è¨ˆ**: ğŸ“† 4-5 é€±

---

## âš ï¸ ç‰¹åˆ¥æ³¨æ„äº‹é …

### è—¥ç‰©å»ºè­°åŠŸèƒ½
```
ğŸ”´ æ³•å¾‹é¢¨éšªè©•ä¼°æ¸…å–®ï¼š

1. âœ… å…è²¬è²æ˜åœ¨æ‰€æœ‰ä»‹é¢é¡¯è‘—æ¨™ç¤º
2. âœ… ä½¿ç”¨è€…å¿…é ˆæ˜ç¢ºç¢ºèªå…è²¬è²æ˜
3. âœ… æ‰€æœ‰æŸ¥è©¢è¨˜éŒ„å¯©è¨ˆæ—¥èªŒ
4. âœ… å®šæœŸç”±é†«å­¸å°ˆå®¶å¯©æ ¸å»ºè­°å“è³ª
5. âœ… ç²å¾—é†«é™¢å€«ç†å§”å“¡æœƒæ‰¹å‡†
6. âœ… æ˜ç¢ºæ¨™ç¤ºç‚º"ç ”ç©¶åƒè€ƒ"è€Œé"è‡¨åºŠå»ºè­°"
7. âœ… ä¸æä¾›åŠ‘é‡è³‡è¨Š
8. âœ… ä¸æä¾›ç”¨è—¥æ™‚é–“è¡¨

å»ºè­°ï¼š
- è—¥ç‰©å»ºè­°åŠŸèƒ½é è¨­é—œé–‰
- éœ€ç®¡ç†å“¡æ¬Šé™å•Ÿç”¨
- æ¯æ¬¡ä½¿ç”¨éœ€é‡æ–°ç¢ºèªå…è²¬è²æ˜
```

---

## âœ… æª¢æŸ¥æ¸…å–®

### Phase 2.3 å®Œæˆç¢ºèª
- [ ] ç—‡ç‹€é—œè¯åˆ†æ API å¯ç”¨
- [ ] WebUI å¯è¦–åŒ–å®Œæ•´
- [ ] ç”Ÿç‰©å­¸æ©Ÿåˆ¶æ¨æ–·æº–ç¢º
- [ ] å–®å…ƒæ¸¬è©¦è¦†è“‹ç‡ > 70%

### Phase 2.4 å®Œæˆç¢ºèª
- [ ] ClinVar API æ•´åˆå®Œæˆ
- [ ] å¤–é¡¯ç‡ä¼°ç®—å¯¦ç¾
- [ ] æ–‡ç»æ”¯æŒè©•åˆ†æº–ç¢º
- [ ] API ç«¯é»æ¸¬è©¦é€šé

### Phase 2.5 å®Œæˆç¢ºèªï¼ˆè—¥ç‰©å»ºè­°ï¼‰
- [ ] **å…è²¬è²æ˜å¯©æ ¸é€šé**
- [ ] **é†«å­¸å°ˆå®¶å¯©æ ¸é€šé**
- [ ] **å€«ç†å§”å“¡æœƒæ‰¹å‡†æ–‡ä»¶**
- [ ] DrugBank çŸ¥è­˜åœ–è­œæ§‹å»ºå®Œæˆ
- [ ] å¯©è¨ˆæ—¥èªŒç³»çµ±é‹ä½œ
- [ ] å®‰å…¨æ€§æ¸¬è©¦é€šé

### Phase 2.6 å®Œæˆç¢ºèª
- [ ] é›¢ç·šæª¢ç´¢å®Œå…¨å¯ç”¨
- [ ] ç·šä¸Šæª¢ç´¢ï¼ˆå¯é¸ï¼‰å¯¦ç¾
- [ ] å¯ä¿¡åº¦è©•åˆ†é©—è­‰
- [ ] æ’åºçµæœèˆ‡é†«å­¸å°ˆå®¶è©•ä¼°ä¸€è‡´

---

**ç‰ˆæœ¬**: v1.0  
**å‰µå»ºæ—¥æœŸ**: 2025-11-04  
**è² è²¬äºº**: TBD  
**é†«å­¸é¡§å•**: TBDï¼ˆè—¥ç‰©å»ºè­°åŠŸèƒ½å¿…éœ€ï¼‰  
**æ³•å¾‹é¡§å•**: TBDï¼ˆè—¥ç‰©å»ºè­°åŠŸèƒ½å¿…éœ€ï¼‰
