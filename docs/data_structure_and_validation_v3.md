# SHEPHERD-Advanced è³‡æ–™çµæ§‹èˆ‡é …ç›®æ ¡é©—è¨­è¨ˆ v3.0

**ç‰ˆæœ¬**: 3.0  
**æ—¥æœŸ**: 2025-11-04  
**ç‹€æ…‹**: æ•´åˆé†«ç”Ÿåœ˜éšŠå»ºè­° + åŸæœ‰æ¶æ§‹å„ªåŒ–  
**è®Šæ›´é‡é»**: 
1. æ–°å¢ NLP/FHIR æ¨¡å¡Šæ¶æ§‹
2. å¼·åŒ– metadata ç‰ˆæœ¬è¿½æº¯
3. çµ±ä¸€ schema é©—è­‰ç³»çµ±
4. é ç•™é†«ç™‚æ¨™æº–æ¥å£

---

## ğŸ“‹ ç›®éŒ„

1. [æ ¸å¿ƒåŸå‰‡](#æ ¸å¿ƒåŸå‰‡)
2. [å®Œæ•´ç›®éŒ„çµæ§‹](#å®Œæ•´ç›®éŒ„çµæ§‹)
3. [è³‡æ–™çµæ§‹è¨­è¨ˆ](#è³‡æ–™çµæ§‹è¨­è¨ˆ)
4. [é …ç›®æ ¡é©—ç³»çµ±](#é …ç›®æ ¡é©—ç³»çµ±)
5. [é†«ç”Ÿåœ˜éšŠåŠŸèƒ½æ•´åˆ](#é†«ç”Ÿåœ˜éšŠåŠŸèƒ½æ•´åˆ)
6. [æ“´å……æ¥å£è¨­è¨ˆ](#æ“´å……æ¥å£è¨­è¨ˆ)
7. [å¯¦æ–½æª¢æŸ¥æ¸…å–®](#å¯¦æ–½æª¢æŸ¥æ¸…å–®)

---

## ğŸ¯ æ ¸å¿ƒåŸå‰‡

### 1. æ¨¡å¡ŠåŒ–èˆ‡å¯ç¶­è­·æ€§
- âœ… **å–®ä¸€è·è²¬**: æ¯å€‹æ¨¡å¡Šåªè² è²¬ä¸€ä»¶äº‹
- âœ… **æ¥å£åˆ†é›¢**: æ¸…æ™°çš„æ¨¡å¡Šé‚Šç•Œèˆ‡ä¾è³´æ–¹å‘
- âœ… **çµ±ä¸€å‘½å**: éµå¾ª PEP 8 + é†«ç™‚é ˜åŸŸæ…£ä¾‹
- âœ… **ç‰ˆæœ¬è¿½æº¯**: æ‰€æœ‰è³‡æ–™å’Œæ¨¡å‹å¯å®Œæ•´è¿½æº¯

### 2. é†«ç™‚åˆè¦æ€§
- âœ… **è³‡æ–™è¡€çµ±**: å®Œæ•´è¨˜éŒ„è³‡æ–™ä¾†æºå’Œè™•ç†éç¨‹
- âœ… **ç‰ˆæœ¬ç®¡ç†**: semantic versioning + SHA256 æ ¡é©—
- âœ… **å®‰å…¨éš”é›¢**: æ•æ„Ÿè³‡æ–™åŠ å¯†å­˜å„²
- âœ… **å¯©è¨ˆæ—¥èªŒ**: æ‰€æœ‰æ¨ç†éç¨‹å¯å¯©æŸ¥

### 3. è·¨å¹³å°ä¸€è‡´æ€§
- âœ… **é…ç½®çµ±ä¸€**: å–®ä¸€çœŸå¯¦ä¾†æº (SSOT)
- âœ… **å¹³å°æª¢æ¸¬**: è‡ªå‹•é©é… x86/ARM å·®ç•°
- âœ… **é™ç´šæ–¹æ¡ˆ**: å„ªé›…è™•ç†ç’°å¢ƒé™åˆ¶

---

## ğŸ“‚ å®Œæ•´ç›®éŒ„çµæ§‹

```
shepherd-advanced/
â”œâ”€â”€ .github/                        # CI/CD é…ç½®
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ test-x86.yml
â”‚       â”œâ”€â”€ test-arm.yml
â”‚       â””â”€â”€ deploy.yml
â”‚
â”œâ”€â”€ configs/                        # âœ¨ é…ç½®æ–‡ä»¶ (çµ±ä¸€ç®¡ç†)
â”‚   â”œâ”€â”€ schemas/                    # ğŸ†• JSON Schema é©—è­‰è¦å‰‡
â”‚   â”‚   â”œâ”€â”€ base_config.schema.json
â”‚   â”‚   â”œâ”€â”€ model_config.schema.json
â”‚   â”‚   â”œâ”€â”€ data_config.schema.json
â”‚   â”‚   â”œâ”€â”€ patient_input.schema.json      # ğŸ†• æ‚£è€…è¼¸å…¥æ ¼å¼
â”‚   â”‚   â””â”€â”€ inference_output.schema.json   # ğŸ†• æ¨ç†è¼¸å‡ºæ ¼å¼
â”‚   â”‚
â”‚   â”œâ”€â”€ base_config.yaml            # åŸºç¤é…ç½®
â”‚   â”œâ”€â”€ model_config.yaml           # æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ data_config.yaml            # è³‡æ–™é…ç½®
â”‚   â”œâ”€â”€ deployment_config.yaml      # éƒ¨ç½²é…ç½®
â”‚   â””â”€â”€ medical_standards.yaml      # ğŸ†• é†«ç™‚æ¨™æº–æ˜ å°„ (HPO/ICD/FHIR)
â”‚
â”œâ”€â”€ data/                           # âœ¨ è³‡æ–™ç›®éŒ„
â”‚   â”œâ”€â”€ raw/                        # åŸå§‹è³‡æ–™
â”‚   â”‚   â”œâ”€â”€ ontologies/
â”‚   â”‚   â”‚   â”œâ”€â”€ hpo.obo
â”‚   â”‚   â”‚   â”œâ”€â”€ mondo.owl
â”‚   â”‚   â”‚   â””â”€â”€ go.obo
â”‚   â”‚   â”œâ”€â”€ kg_sources/
â”‚   â”‚   â”‚   â”œâ”€â”€ disgenet/
â”‚   â”‚   â”‚   â”œâ”€â”€ clinvar/
â”‚   â”‚   â”‚   â””â”€â”€ omim/
â”‚   â”‚   â””â”€â”€ patient_records/        # ğŸ†• æ‚£è€…åŸå§‹è³‡æ–™
â”‚   â”‚       â”œâ”€â”€ fhir/                # ğŸ†• FHIR æ ¼å¼è³‡æ–™
â”‚   â”‚       â””â”€â”€ hiss/                # ğŸ†• HISS æ ¼å¼è³‡æ–™
â”‚   â”‚
â”‚   â”œâ”€â”€ processed/                  # è™•ç†å¾Œè³‡æ–™
â”‚   â”‚   â”œâ”€â”€ knowledge_graph/
â”‚   â”‚   â”‚   â”œâ”€â”€ metadata.json       # âœ¨ å¢å¼·ç‰ˆ metadata (ä¸‹è©³)
â”‚   â”‚   â”‚   â”œâ”€â”€ VERSION             # èªç¾©åŒ–ç‰ˆæœ¬è™Ÿ
â”‚   â”‚   â”‚   â”œâ”€â”€ hetero_graph.pt
â”‚   â”‚   â”‚   â”œâ”€â”€ hypergraph.pt
â”‚   â”‚   â”‚   â””â”€â”€ embeddings/
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ ontology_cache/         # æœ¬é«”å¿«å–
â”‚   â”‚   â”‚   â”œâ”€â”€ hpo_hierarchy.pkl
â”‚   â”‚   â”‚   â”œâ”€â”€ mondo_hierarchy.pkl
â”‚   â”‚   â”‚   â””â”€â”€ constraints.json
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ nlp_extractions/        # ğŸ†• NLP æå–çµæœå¿«å–
â”‚   â”‚       â”œâ”€â”€ symptom_cache.db    # ç—‡ç‹€æå–å¿«å–
â”‚   â”‚       â””â”€â”€ entity_mappings.json
â”‚   â”‚
â”‚   â””â”€â”€ external/                   # å¤§å‹å¤–éƒ¨è³‡æ–™ (ä¸ç´å…¥ç‰ˆæ§)
â”‚       â””â”€â”€ .gitignore
â”‚
â”œâ”€â”€ models/                         # âœ¨ æ¨¡å‹ç›®éŒ„
â”‚   â”œâ”€â”€ production/
â”‚   â”‚   â”œâ”€â”€ registry.json           # ğŸ†• æ¨¡å‹è¨»å†Šè¡¨
â”‚   â”‚   â”œâ”€â”€ checkpoint_v1.0.0/
â”‚   â”‚   â”‚   â”œâ”€â”€ model.pt
â”‚   â”‚   â”‚   â”œâ”€â”€ metadata.json       # æ¨¡å‹å…ƒè³‡æ–™
â”‚   â”‚   â”‚   â””â”€â”€ config.yaml
â”‚   â”‚   â””â”€â”€ current -> checkpoint_v1.0.0  # ç¬¦è™Ÿé€£çµ
â”‚   â”‚
â”‚   â”œâ”€â”€ checkpoints/                # è¨“ç·´æª¢æŸ¥é»
â”‚   â”œâ”€â”€ pretrained/                 # é è¨“ç·´æ¨¡å‹
â”‚   â”‚   â””â”€â”€ scibertuncased/        # ğŸ†• NLP é è¨“ç·´æ¨¡å‹
â”‚   â””â”€â”€ experiments/                # å¯¦é©—æ¨¡å‹
â”‚
â”œâ”€â”€ src/                            # âœ¨ æºä»£ç¢¼
â”‚   â”œâ”€â”€ config/                     # é…ç½®æ¨¡å¡Š
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_config.py
â”‚   â”‚   â”œâ”€â”€ config_validator.py     # ğŸ†• é…ç½®é©—è­‰å™¨
â”‚   â”‚   â””â”€â”€ schema_loader.py        # ğŸ†• Schema è¼‰å…¥å™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ ontology/                   # æœ¬é«”è™•ç†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loader.py
â”‚   â”‚   â”œâ”€â”€ hierarchy.py
â”‚   â”‚   â”œâ”€â”€ constraints.py
â”‚   â”‚   â”œâ”€â”€ similarity.py
â”‚   â”‚   â””â”€â”€ validator.py
â”‚   â”‚
â”‚   â”œâ”€â”€ kg/                         # çŸ¥è­˜åœ–è­œ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ builder.py
â”‚   â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”‚   â”œâ”€â”€ preprocessor.py
â”‚   â”‚   â”œâ”€â”€ hypergraph.py
â”‚   â”‚   â”œâ”€â”€ entity_linker.py        # ğŸ†• å¯¦é«”é€£çµ
â”‚   â”‚   â””â”€â”€ storage/
â”‚   â”‚       â”œâ”€â”€ file_storage.py
â”‚   â”‚       â””â”€â”€ graph_db.py
â”‚   â”‚
â”‚   â”œâ”€â”€ nlp/                        # ğŸ†• è‡ªç„¶èªè¨€è™•ç†æ¨¡å¡Š
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ symptom_extractor.py   # Free text â†’ HPO terms
â”‚   â”‚   â”œâ”€â”€ entity_recognizer.py   # é†«ç™‚å¯¦é«”è­˜åˆ¥
â”‚   â”‚   â”œâ”€â”€ clinical_bert.py       # ClinicalBERT åŒ…è£
â”‚   â”‚   â””â”€â”€ hpo_matcher.py         # HPO è¡“èªåŒ¹é…
â”‚   â”‚
â”‚   â”œâ”€â”€ medical_standards/          # ğŸ†• é†«ç™‚æ¨™æº–æ¥å£
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ fhir_adapter.py        # FHIR é©é…å™¨
â”‚   â”‚   â”œâ”€â”€ hiss_adapter.py        # HISS é©é…å™¨
â”‚   â”‚   â”œâ”€â”€ icd_mapper.py          # ICD-10/11 æ˜ å°„
â”‚   â”‚   â””â”€â”€ snomed_mapper.py       # SNOMED CT æ˜ å°„
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                     # æ¨¡å‹æ¶æ§‹
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ gnn/
â”‚   â”‚   â”œâ”€â”€ attention/
â”‚   â”‚   â”œâ”€â”€ encoders/
â”‚   â”‚   â”œâ”€â”€ decoders/
â”‚   â”‚   â””â”€â”€ tasks/
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/                  # æª¢ç´¢æ¨¡å¡Š
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ vector_index.py
â”‚   â”‚   â”œâ”€â”€ path_retriever.py
â”‚   â”‚   â”œâ”€â”€ path_scorer.py
â”‚   â”‚   â””â”€â”€ subgraph_sampler.py
â”‚   â”‚
â”‚   â”œâ”€â”€ reasoning/                  # æ¨ç†æ¨¡å¡Š
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ path_reasoning.py
â”‚   â”‚   â”œâ”€â”€ constraint_checker.py
â”‚   â”‚   â””â”€â”€ explanation_generator.py
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/                        # æœ¬åœ° LLM
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ interface.py           # ğŸ†• LLM æ¥å£å®šç¾©
â”‚   â”‚   â”œâ”€â”€ model_loader.py
â”‚   â”‚   â”œâ”€â”€ inference_engine.py
â”‚   â”‚   â””â”€â”€ prompt_templates.py
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/                  # æ¨ç†ç®¡é“
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”‚   â”œâ”€â”€ schemas.py              # ğŸ†• å…±äº«è³‡æ–™çµæ§‹ (SSOT)
â”‚   â”‚   â”œâ”€â”€ input_validator.py      # ğŸ†• è¼¸å…¥é©—è­‰
â”‚   â”‚   â””â”€â”€ output_formatter.py     # ğŸ†• è¼¸å‡ºæ ¼å¼åŒ–
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                   # è¨“ç·´æ¨¡å¡Š
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â”œâ”€â”€ loss_functions.py
â”‚   â”‚   â””â”€â”€ callbacks.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                      # å·¥å…·å‡½æ•¸
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ platform_detector.py
â”‚   â”‚   â”œâ”€â”€ version_checker.py      # ğŸ†• ç‰ˆæœ¬å…¼å®¹æ€§æª¢æŸ¥
â”‚   â”‚   â”œâ”€â”€ hash_generator.py       # ğŸ†• è³‡æ–™å“ˆå¸Œç”Ÿæˆ
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                        # FastAPI å¾Œç«¯
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ inference.py
â”‚   â”‚   â”‚   â”œâ”€â”€ training.py
â”‚   â”‚   â”‚   â””â”€â”€ health.py
â”‚   â”‚   â””â”€â”€ middleware/
â”‚   â”‚       â”œâ”€â”€ auth.py
â”‚   â”‚       â””â”€â”€ logging.py
â”‚   â”‚
â”‚   â””â”€â”€ webui/                      # Gradio å‰ç«¯
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ app.py
â”‚       â””â”€â”€ components/
â”‚           â”œâ”€â”€ input_form.py       # ğŸ†• æ™ºèƒ½è¡¨å–®
â”‚           â”œâ”€â”€ hpo_search.py       # ğŸ†• HPO æœå°‹
â”‚           â””â”€â”€ result_viewer.py
â”‚
â”œâ”€â”€ tests/                          # âœ¨ æ¸¬è©¦
â”‚   â”œâ”€â”€ unit/                       # å–®å…ƒæ¸¬è©¦
â”‚   â”‚   â”œâ”€â”€ test_ontology.py
â”‚   â”‚   â”œâ”€â”€ test_kg.py
â”‚   â”‚   â”œâ”€â”€ test_nlp.py             # ğŸ†•
â”‚   â”‚   â””â”€â”€ test_medical_standards.py # ğŸ†•
â”‚   â”‚
â”‚   â”œâ”€â”€ integration/                # æ•´åˆæ¸¬è©¦
â”‚   â”‚   â”œâ”€â”€ test_pipeline.py
â”‚   â”‚   â””â”€â”€ test_fhir_workflow.py   # ğŸ†•
â”‚   â”‚
â”‚   â”œâ”€â”€ benchmarks/                 # åŸºæº–æ¸¬è©¦
â”‚   â”‚   â””â”€â”€ platform_specific/
â”‚   â”‚       â”œâ”€â”€ test_attention_x86.py
â”‚   â”‚       â”œâ”€â”€ test_attention_arm.py
â”‚   â”‚       â”œâ”€â”€ test_vector_index_x86.py
â”‚   â”‚       â””â”€â”€ test_vector_index_arm.py
â”‚   â”‚
â”‚   â””â”€â”€ fixtures/                   # æ¸¬è©¦è³‡æ–™
â”‚       â”œâ”€â”€ sample_patients.json    # ğŸ†• ç¯„ä¾‹æ‚£è€…è³‡æ–™
â”‚       â””â”€â”€ sample_fhir.json        # ğŸ†• ç¯„ä¾‹ FHIR è³‡æ–™
â”‚
â”œâ”€â”€ scripts/                        # âœ¨ è…³æœ¬
â”‚   â”œâ”€â”€ download_data.py
â”‚   â”œâ”€â”€ preprocess_data.py
â”‚   â”œâ”€â”€ build_knowledge_graph.py
â”‚   â”œâ”€â”€ train_model.py
â”‚   â”œâ”€â”€ validate_installation.py    # ğŸ†• å®‰è£é©—è­‰
â”‚   â””â”€â”€ medical_standards/          # ğŸ†• é†«ç™‚æ¨™æº–å·¥å…·
â”‚       â”œâ”€â”€ convert_fhir_to_internal.py
â”‚       â””â”€â”€ export_to_fhir.py
â”‚
â”œâ”€â”€ docs/                           # âœ¨ æ–‡æª”
â”‚   â”œâ”€â”€ architecture_v3.md          # ğŸ†• æ›´æ–°æ¶æ§‹æ–‡æª”
â”‚   â”œâ”€â”€ medical_integration.md      # ğŸ†• é†«ç™‚ç³»çµ±æ•´åˆæŒ‡å—
â”‚   â”œâ”€â”€ api_reference.md
â”‚   â””â”€â”€ developer_guide.md
â”‚
â”œâ”€â”€ logs/                           # æ—¥èªŒ
â”œâ”€â”€ reports/                        # å ±å‘Šè¼¸å‡º
â”‚
â”œâ”€â”€ pyproject.toml                  # ğŸ†• Python å°ˆæ¡ˆé…ç½®
â”œâ”€â”€ .import-linter.ini              # ğŸ†• ä¾è³´è¦å‰‡ç´„æŸ
â”œâ”€â”€ .pre-commit-config.yaml         # ğŸ†• Git hooks
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## ğŸ“Š è³‡æ–™çµæ§‹è¨­è¨ˆ

### 1. å¢å¼·ç‰ˆçŸ¥è­˜åœ–è­œ Metadata

**ä½ç½®**: `data/processed/knowledge_graph/metadata.json`

```json
{
  "schema_version": "3.0",
  "data_version": "2025.11.04",
  "creation_timestamp": "2025-11-04T10:30:00Z",
  
  "generator": {
    "script": "scripts/build_knowledge_graph.py",
    "commit_sha": "a3f5b2c1234567890abcdef",
    "git_branch": "main",
    "python_version": "3.12.0",
    "torch_version": "2.8.0",
    "pyg_version": "2.6.0"
  },
  
  "data_sources": {
    "hpo": {
      "version": "2025-09-01",
      "url": "http://purl.obolibrary.org/obo/hp.obo",
      "download_date": "2025-11-01",
      "sha256": "abc123..."
    },
    "mondo": {
      "version": "2025-08-15",
      "url": "http://purl.obolibrary.org/obo/mondo.owl",
      "download_date": "2025-11-01",
      "sha256": "def456..."
    },
    "disgenet": {
      "version": "v7.0",
      "url": "https://www.disgenet.org/downloads",
      "download_date": "2025-11-01",
      "sha256": "ghi789..."
    }
  },
  
  "preprocessing": {
    "steps": [
      "ontology_alignment",
      "entity_deduplication",
      "edge_normalization",
      "hypergraph_construction"
    ],
    "parameters": {
      "min_edge_confidence": 0.5,
      "k_hop_subgraph": 3,
      "hyperedge_min_support": 10
    }
  },
  
  "statistics": {
    "num_nodes": 500000,
    "num_edges": 2000000,
    "node_types": {
      "gene": 25000,
      "disease": 15000,
      "phenotype": 130000,
      "pathway": 2500
    },
    "edge_types": {
      "gene_disease": 450000,
      "phenotype_disease": 800000,
      "gene_pathway": 120000
    },
    "hyperedges": 5000,
    "avg_degree": 8.5,
    "connected_components": 1
  },
  
  "validation": {
    "ontology_constraints_passed": true,
    "no_self_loops": true,
    "no_duplicate_edges": true,
    "all_nodes_reachable": true
  },
  
  "data_hash": {
    "graph_structure": "sha256:abcd1234...",
    "node_features": "sha256:efgh5678...",
    "edge_features": "sha256:ijkl9012..."
  },
  
  "compatibility": {
    "min_python": "3.10",
    "min_torch": "2.5.0",
    "min_pyg": "2.5.0",
    "platforms": ["x86_64", "aarch64"]
  }
}
```

### 2. æ¨¡å‹è¨»å†Šè¡¨

**ä½ç½®**: `models/production/registry.json`

```json
{
  "registry_version": "1.0",
  "models": [
    {
      "model_id": "shepherd-v1.0.0",
      "type": "ontology_aware_gnn",
      "status": "production",
      "created_at": "2025-11-04T10:00:00Z",
      
      "training_data": {
        "kg_version": "2025.11.04",
        "kg_hash": "sha256:abcd1234...",
        "patient_dataset": "synthetic_1000",
        "split": {
          "train": 800,
          "val": 100,
          "test": 100
        }
      },
      
      "hyperparameters": {
        "hidden_dim": 512,
        "num_layers": 4,
        "attention_heads": 8,
        "dropout": 0.1,
        "learning_rate": 0.0001,
        "batch_size": 32
      },
      
      "performance": {
        "val_mrr": 0.856,
        "val_hits@10": 0.923,
        "test_mrr": 0.832,
        "test_hits@10": 0.911,
        "inference_time_ms": 150
      },
      
      "compatible_data_versions": ["2025.11.04", "2025.10.*"],
      "platforms": ["x86_cuda", "arm_cuda"],
      
      "files": {
        "model_weights": "models/production/checkpoint_v1.0.0/model.pt",
        "config": "models/production/checkpoint_v1.0.0/config.yaml",
        "metadata": "models/production/checkpoint_v1.0.0/metadata.json"
      }
    }
  ],
  "current_production": "shepherd-v1.0.0"
}
```

### 3. æ‚£è€…è¼¸å…¥ Schema

**ä½ç½®**: `configs/schemas/patient_input.schema.json`

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Patient Input Schema",
  "description": "æ¨™æº–åŒ–æ‚£è€…è¼¸å…¥æ ¼å¼,æ”¯æ´å¤šç¨®è³‡æ–™é¡å‹",
  "type": "object",
  "required": ["patient_id", "phenotypes"],
  
  "properties": {
    "patient_id": {
      "type": "string",
      "pattern": "^P[0-9]{5,10}$",
      "description": "æ‚£è€…å”¯ä¸€è­˜åˆ¥ç¢¼"
    },
    
    "input_type": {
      "type": "string",
      "enum": ["structured", "free_text", "fhir", "hiss"],
      "default": "structured",
      "description": "è¼¸å…¥è³‡æ–™é¡å‹"
    },
    
    "phenotypes": {
      "type": "array",
      "minItems": 1,
      "items": {
        "oneOf": [
          {
            "type": "string",
            "pattern": "^HP:[0-9]{7}$",
            "description": "æ¨™æº– HPO term (HP:1234567)"
          },
          {
            "type": "object",
            "properties": {
              "text": {"type": "string"},
              "hpo_id": {"type": "string", "pattern": "^HP:[0-9]{7}$"},
              "confidence": {"type": "number", "minimum": 0, "maximum": 1}
            },
            "required": ["text", "hpo_id"]
          }
        ]
      }
    },
    
    "free_text_symptoms": {
      "type": "string",
      "description": "è‡ªç”±æ–‡å­—ç—‡ç‹€æè¿° (éœ€ NLP è™•ç†)"
    },
    
    "genetic_data": {
      "type": "object",
      "properties": {
        "variants": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "gene": {"type": "string"},
              "variant": {"type": "string"},
              "clinvar_id": {"type": "string"},
              "pathogenicity": {
                "type": "string",
                "enum": ["pathogenic", "likely_pathogenic", "uncertain", "likely_benign", "benign"]
              }
            }
          }
        },
        "gene_panel": {
          "type": "array",
          "items": {"type": "string"},
          "description": "å·²æª¢æ¸¬åŸºå› æ¸…å–®"
        },
        "wgs_available": {"type": "boolean"}
      }
    },
    
    "diagnoses": {
      "type": "object",
      "properties": {
        "icd10": {
          "type": "array",
          "items": {"type": "string", "pattern": "^[A-Z][0-9]{2}\\.[0-9]{1,2}$"}
        },
        "icd11": {
          "type": "array",
          "items": {"type": "string"}
        },
        "snomed": {
          "type": "array",
          "items": {"type": "string", "pattern": "^[0-9]+$"}
        },
        "mondo": {
          "type": "array",
          "items": {"type": "string", "pattern": "^MONDO:[0-9]{7}$"}
        }
      }
    },
    
    "medical_history": {
      "type": "object",
      "properties": {
        "family_history": {
          "type": "array",
          "items": {"type": "string"}
        },
        "lab_results": {
          "type": "object",
          "patternProperties": {
            ".*": {
              "type": "object",
              "properties": {
                "value": {"type": "number"},
                "unit": {"type": "string"},
                "normal_range": {
                  "type": "array",
                  "items": {"type": "number"},
                  "minItems": 2,
                  "maxItems": 2
                },
                "date": {"type": "string", "format": "date-time"}
              }
            }
          }
        },
        "medications": {
          "type": "array",
          "items": {"type": "string"}
        }
      }
    },
    
    "visit_history": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
          "date": {"type": "string", "format": "date"},
          "phenotypes": {
            "type": "array",
            "items": {"type": "string", "pattern": "^HP:[0-9]{7}$"}
          },
          "severity": {
            "type": "string",
            "enum": ["mild", "moderate", "severe"]
          }
        },
        "required": ["date", "phenotypes"]
      }
    },
    
    "fhir_bundle": {
      "type": "object",
      "description": "å®Œæ•´çš„ FHIR Bundle (è‹¥ä½¿ç”¨ FHIR è¼¸å…¥)"
    },
    
    "demographics": {
      "type": "object",
      "properties": {
        "age": {"type": "integer", "minimum": 0, "maximum": 150},
        "gender": {"type": "string", "enum": ["male", "female", "other", "unknown"]},
        "ethnicity": {"type": "string"}
      }
    }
  }
}
```

### 4. æ¨ç†è¼¸å‡º Schema

**ä½ç½®**: `configs/schemas/inference_output.schema.json`

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Inference Output Schema",
  "description": "æ¨™æº–åŒ–æ¨ç†è¼¸å‡ºæ ¼å¼",
  "type": "object",
  "required": ["patient_id", "timestamp", "top_candidates", "metadata"],
  
  "properties": {
    "patient_id": {"type": "string"},
    "timestamp": {"type": "string", "format": "date-time"},
    "inference_time_ms": {"type": "number"},
    
    "top_candidates": {
      "type": "array",
      "minItems": 1,
      "items": {
        "type": "object",
        "required": ["disease", "confidence", "reasoning_path"],
        "properties": {
          "rank": {"type": "integer", "minimum": 1},
          "disease": {
            "type": "object",
            "properties": {
              "mondo_id": {"type": "string", "pattern": "^MONDO:[0-9]{7}$"},
              "name": {"type": "string"},
              "orphanet_id": {"type": "string"},
              "omim_id": {"type": "string"}
            },
            "required": ["mondo_id", "name"]
          },
          "confidence": {
            "type": "number",
            "minimum": 0,
            "maximum": 1
          },
          "supporting_genes": {
            "type": "array",
            "items": {"type": "string"}
          },
          "reasoning_path": {
            "type": "array",
            "items": {"type": "string"},
            "description": "å¯è§£é‡‹çš„æ¨ç†è·¯å¾‘"
          },
          "evidence": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": {
                "source": {"type": "string"},
                "reference": {"type": "string"},
                "confidence": {"type": "number"}
              }
            }
          },
          "ontology_validation": {
            "type": "object",
            "properties": {
              "passed": {"type": "boolean"},
              "violations": {"type": "array", "items": {"type": "string"}}
            }
          },
          "clinical_notes": {
            "type": "string",
            "description": "è‡¨åºŠå»ºè­° (ç”± LLM ç”Ÿæˆ)"
          }
        }
      }
    },
    
    "explanation": {
      "type": "string",
      "description": "æ•´é«”æ¨ç†è§£é‡‹ (è‡ªç„¶èªè¨€)"
    },
    
    "metadata": {
      "type": "object",
      "properties": {
        "model_version": {"type": "string"},
        "kg_version": {"type": "string"},
        "platform": {"type": "string"},
        "gpu_type": {"type": "string"}
      }
    },
    
    "warnings": {
      "type": "array",
      "items": {"type": "string"},
      "description": "æ¨ç†éç¨‹ä¸­çš„è­¦å‘Šè¨Šæ¯"
    }
  }
}
```

---

## ğŸ”’ é …ç›®æ ¡é©—ç³»çµ±

### 1. å·¥ç¨‹åŒ–å·¥å…·éˆ

**ä½ç½®**: `pyproject.toml`

```toml
[project]
name = "shepherd-advanced"
version = "1.0.0"
requires-python = ">=3.10"
dependencies = [
    "torch>=2.5.0",
    "torch-geometric>=2.5.0",
    "pronto>=2.5.0",
    "pydantic>=2.0.0",
    "fastapi>=0.100.0",
    "gradio>=4.0.0",
]

[project.optional-dependencies]
nlp = [
    "transformers>=4.30.0",
    "scispacy>=0.5.0",
    "en-core-sci-sm @ https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_sm-0.5.0.tar.gz"
]
medical = [
    "fhir.resources>=7.0.0",
    "python-hl7>=0.4.0"
]
dev = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
    "mypy>=1.0.0",
    "import-linter>=1.12.0"
]

[tool.black]
line-length = 100
target-version = ['py310', 'py311', 'py312']

[tool.ruff]
line-length = 100
select = ["E", "F", "I", "N", "W"]
ignore = ["E501"]

[tool.mypy]
python_version = "3.10"
strict = true
warn_return_any = true
disallow_untyped_defs = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = "-v --cov=src --cov-report=html"
```

### 2. ä¾è³´è¦å‰‡ç´„æŸ

**ä½ç½®**: `.import-linter.ini`

```ini
[importlinter]
root_package = src

[importlinter:contract:layers]
name = Enforce layered architecture
type = layers
layers =
    src.utils
    src.config
    src.ontology
    src.kg
    src.nlp                   # æ–°å¢å±¤
    src.medical_standards     # æ–°å¢å±¤
    src.models
    src.retrieval
    src.reasoning
    src.llm
    src.training
    src.inference
    src.api
    src.webui

[importlinter:contract:independence]
name = Keep modules independent
type = independence
modules =
    src.nlp
    src.medical_standards
    src.ontology
    src.kg

ignore_imports =
    src.models.* -> src.training.*
    src.retrieval.* -> src.reasoning.*
    src.api.* -> src.training.*
    src.webui.* -> src.training.*

[importlinter:contract:forbidden]
name = Forbidden imports
type = forbidden
source_modules =
    src.models
    src.inference
forbidden_modules =
    src.api
    src.webui
    src.training
```

### 3. é…ç½®é©—è­‰å™¨

**ä½ç½®**: `src/config/config_validator.py`

```python
"""
é…ç½®é©—è­‰å™¨ - å•Ÿå‹•å‰é©—è­‰æ‰€æœ‰é…ç½®æª”æ¡ˆ
"""
import json
import yaml
from pathlib import Path
from typing import Dict, List, Any
from jsonschema import validate, ValidationError
import logging

logger = logging.getLogger(__name__)


class ConfigValidator:
    """é…ç½®æ–‡ä»¶é©—è­‰å™¨"""
    
    def __init__(self, config_dir: Path, schema_dir: Path):
        self.config_dir = config_dir
        self.schema_dir = schema_dir
        self.schemas: Dict[str, Any] = {}
        self._load_schemas()
    
    def _load_schemas(self):
        """è¼‰å…¥æ‰€æœ‰ JSON Schema"""
        for schema_file in self.schema_dir.glob("*.schema.json"):
            schema_name = schema_file.stem.replace(".schema", "")
            with open(schema_file) as f:
                self.schemas[schema_name] = json.load(f)
            logger.info(f"Loaded schema: {schema_name}")
    
    def validate_yaml_config(self, config_name: str) -> bool:
        """é©—è­‰ YAML é…ç½®æ–‡ä»¶"""
        config_file = self.config_dir / f"{config_name}.yaml"
        
        if not config_file.exists():
            logger.error(f"Config file not found: {config_file}")
            return False
        
        # è¼‰å…¥ YAML
        with open(config_file) as f:
            config_data = yaml.safe_load(f)
        
        # ç²å–å°æ‡‰ schema
        if config_name not in self.schemas:
            logger.warning(f"No schema found for {config_name}, skipping validation")
            return True
        
        # é©—è­‰
        try:
            validate(instance=config_data, schema=self.schemas[config_name])
            logger.info(f"âœ… {config_name}.yaml validation passed")
            return True
        except ValidationError as e:
            logger.error(f"âŒ {config_name}.yaml validation failed: {e.message}")
            return False
    
    def validate_all(self) -> bool:
        """é©—è­‰æ‰€æœ‰é…ç½®æª”æ¡ˆ"""
        logger.info("Starting configuration validation...")
        
        configs_to_validate = [
            "base_config",
            "model_config",
            "data_config",
            "deployment_config"
        ]
        
        results = []
        for config_name in configs_to_validate:
            result = self.validate_yaml_config(config_name)
            results.append(result)
        
        if all(results):
            logger.info("âœ… All configuration files are valid")
            return True
        else:
            logger.error("âŒ Some configuration files have errors")
            return False


def main():
    """CLI å…¥å£"""
    from src.config.base_config import Config
    
    config = Config()
    validator = ConfigValidator(
        config_dir=Path("configs"),
        schema_dir=Path("configs/schemas")
    )
    
    success = validator.validate_all()
    exit(0 if success else 1)


if __name__ == "__main__":
    main()
```

### 4. ç‰ˆæœ¬å…¼å®¹æ€§æª¢æŸ¥å™¨

**ä½ç½®**: `src/utils/version_checker.py`

```python
"""
ç‰ˆæœ¬å…¼å®¹æ€§æª¢æŸ¥å™¨ - ç¢ºä¿æ¨¡å‹èˆ‡è³‡æ–™ç‰ˆæœ¬åŒ¹é…
"""
import json
import hashlib
from pathlib import Path
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)


class IncompatibleVersionError(Exception):
    """ç‰ˆæœ¬ä¸å…¼å®¹ç•°å¸¸"""
    pass


class VersionChecker:
    """ç‰ˆæœ¬å…¼å®¹æ€§æª¢æŸ¥å™¨"""
    
    def __init__(self, models_dir: Path, data_dir: Path):
        self.models_dir = models_dir
        self.data_dir = data_dir
    
    def load_metadata(self, file_path: Path) -> Dict[str, Any]:
        """è¼‰å…¥ metadata.json"""
        with open(file_path) as f:
            return json.load(f)
    
    def check_model_data_compatibility(
        self,
        model_version: str,
        data_version: str
    ) -> bool:
        """æª¢æŸ¥æ¨¡å‹èˆ‡è³‡æ–™ç‰ˆæœ¬æ˜¯å¦å…¼å®¹"""
        
        # è¼‰å…¥æ¨¡å‹ metadata
        model_meta_path = self.models_dir / model_version / "metadata.json"
        if not model_meta_path.exists():
            raise FileNotFoundError(f"Model metadata not found: {model_meta_path}")
        
        model_meta = self.load_metadata(model_meta_path)
        
        # è¼‰å…¥è³‡æ–™ metadata
        data_meta_path = self.data_dir / "processed" / "knowledge_graph" / "metadata.json"
        if not data_meta_path.exists():
            raise FileNotFoundError(f"Data metadata not found: {data_meta_path}")
        
        data_meta = self.load_metadata(data_meta_path)
        
        # æª¢æŸ¥è³‡æ–™å“ˆå¸Œ
        expected_hash = model_meta["training_data"]["kg_hash"]
        actual_hash = data_meta["data_hash"]["graph_structure"]
        
        if expected_hash != actual_hash:
            logger.error(
                f"Data hash mismatch!\n"
                f"  Model expects: {expected_hash}\n"
                f"  Current data:  {actual_hash}"
            )
            raise IncompatibleVersionError(
                f"Model {model_version} is not compatible with current data version"
            )
        
        logger.info(f"âœ… Model {model_version} is compatible with data version {data_version}")
        return True
    
    def verify_installation(self) -> Dict[str, bool]:
        """é©—è­‰å®‰è£å®Œæ•´æ€§"""
        checks = {}
        
        # æª¢æŸ¥å¿…è¦æª”æ¡ˆ
        required_files = [
            self.data_dir / "processed" / "knowledge_graph" / "metadata.json",
            self.models_dir / "production" / "registry.json",
        ]
        
        for file_path in required_files:
            checks[str(file_path)] = file_path.exists()
        
        return checks


def compute_file_hash(file_path: Path) -> str:
    """è¨ˆç®—æª”æ¡ˆ SHA256 å“ˆå¸Œ"""
    hasher = hashlib.sha256()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hasher.update(chunk)
    return f"sha256:{hasher.hexdigest()}"
```

### 5. è¼¸å…¥é©—è­‰å™¨

**ä½ç½®**: `src/inference/input_validator.py`

```python
"""
è¼¸å…¥é©—è­‰å™¨ - é©—è­‰æ‚£è€…è¼¸å…¥æ ¼å¼
"""
import json
from pathlib import Path
from typing import Dict, Any, List
from jsonschema import validate, ValidationError
from pydantic import BaseModel, Field, validator
import logging

logger = logging.getLogger(__name__)


class PatientInput(BaseModel):
    """æ‚£è€…è¼¸å…¥è³‡æ–™æ¨¡å‹ (ä½¿ç”¨ Pydantic)"""
    
    patient_id: str = Field(..., regex=r"^P[0-9]{5,10}$")
    input_type: str = Field("structured", regex=r"^(structured|free_text|fhir|hiss)$")
    phenotypes: List[str] = Field(..., min_items=1)
    free_text_symptoms: str = Field(None)
    genetic_data: Dict[str, Any] = Field(None)
    diagnoses: Dict[str, List[str]] = Field(None)
    medical_history: Dict[str, Any] = Field(None)
    visit_history: List[Dict[str, Any]] = Field(None)
    fhir_bundle: Dict[str, Any] = Field(None)
    demographics: Dict[str, Any] = Field(None)
    
    @validator('phenotypes', each_item=True)
    def validate_hpo_term(cls, v):
        """é©—è­‰ HPO term æ ¼å¼"""
        if not v.startswith("HP:") or len(v) != 10:
            raise ValueError(f"Invalid HPO term format: {v}")
        return v
    
    class Config:
        extra = "forbid"  # ç¦æ­¢é¡å¤–æ¬„ä½


class InputValidator:
    """è¼¸å…¥é©—è­‰å™¨"""
    
    def __init__(self, schema_path: Path):
        with open(schema_path) as f:
            self.schema = json.load(f)
    
    def validate(self, patient_data: Dict[str, Any]) -> bool:
        """ä½¿ç”¨ JSON Schema é©—è­‰"""
        try:
            validate(instance=patient_data, schema=self.schema)
            logger.info("âœ… Patient input validation passed")
            return True
        except ValidationError as e:
            logger.error(f"âŒ Patient input validation failed: {e.message}")
            raise
    
    def validate_pydantic(self, patient_data: Dict[str, Any]) -> PatientInput:
        """ä½¿ç”¨ Pydantic é©—è­‰ (æ›´åš´æ ¼)"""
        try:
            validated = PatientInput(**patient_data)
            logger.info("âœ… Patient input Pydantic validation passed")
            return validated
        except Exception as e:
            logger.error(f"âŒ Patient input Pydantic validation failed: {e}")
            raise
```

### 6. å¹³å°ç‰¹å®šæ¸¬è©¦

**ä½ç½®**: `tests/benchmarks/platform_specific/test_attention_arm.py`

```python
"""
ARM å¹³å°æ³¨æ„åŠ›æ©Ÿåˆ¶å›æ­¸æ¸¬è©¦
"""
import torch
import pytest
from src.models.attention.adaptive_backend import AdaptiveAttentionBackend


@pytest.mark.skipif(
    not torch.cuda.is_available() or torch.cuda.get_device_capability()[0] < 9,
    reason="Requires Blackwell GPU"
)
@pytest.mark.arm64
def test_cudnn_sdpa_smoke_arm():
    """ARM å¹³å° cuDNN SDPA å¿«é€Ÿé©—è­‰"""
    device = torch.device("cuda")
    backend = AdaptiveAttentionBackend()
    
    # å»ºç«‹æ¸¬è©¦å¼µé‡
    batch_size, seq_len, d_model = 4, 128, 512
    q = torch.randn(batch_size, seq_len, d_model, device=device)
    k = torch.randn(batch_size, seq_len, d_model, device=device)
    v = torch.randn(batch_size, seq_len, d_model, device=device)
    
    # åŸ·è¡Œ 100 æ¬¡æ¨ç†
    for _ in range(100):
        output = backend.compute_attention(q, k, v)
    
    # é©—è­‰çµæœ
    assert torch.isfinite(output).all(), "Output contains NaN or Inf"
    assert output.shape == q.shape, "Output shape mismatch"


@pytest.mark.arm64
def test_platform_detection_arm():
    """é©—è­‰ ARM å¹³å°æª¢æ¸¬"""
    from src.utils.platform_detector import PlatformDetector
    
    detector = PlatformDetector()
    info = detector.get_platform_info()
    
    assert info["cpu_arch"] == "aarch64"
    assert "ARM" in info["cpu_model"]
```

---

## ğŸ¥ é†«ç”Ÿåœ˜éšŠåŠŸèƒ½æ•´åˆ

### 1. NLP ç—‡ç‹€æå–æ¨¡å¡Š

**ä½ç½®**: `src/nlp/symptom_extractor.py`

```python
"""
ç—‡ç‹€æå–å™¨ - å¾è‡ªç”±æ–‡å­—æå– HPO terms
ç‹€æ…‹: ğŸŸ¡ Phase 2 å¯¦ç¾ (é ç•™æ¥å£)
"""
from typing import List, Dict, Tuple
from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch
import logging

logger = logging.getLogger(__name__)


class SymptomExtractor:
    """å¾è‡ªç”±æ–‡å­—æå–ç—‡ç‹€ä¸¦æ˜ å°„åˆ° HPO"""
    
    def __init__(self, model_name: str = "allenai/scibert_scivocab_uncased"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self._initialized = False
    
    def initialize(self):
        """å»¶é²åˆå§‹åŒ– (é¿å…ä¸å¿…è¦çš„æ¨¡å‹è¼‰å…¥)"""
        if self._initialized:
            return
        
        logger.info(f"Loading NLP model: {self.model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForTokenClassification.from_pretrained(self.model_name)
        self._initialized = True
        logger.info("NLP model loaded successfully")
    
    def extract_symptoms(
        self,
        free_text: str,
        confidence_threshold: float = 0.7
    ) -> List[Dict[str, any]]:
        """
        å¾è‡ªç”±æ–‡å­—æå–ç—‡ç‹€
        
        Args:
            free_text: è‡ªç”±æ–‡å­—ç—‡ç‹€æè¿°
            confidence_threshold: æœ€ä½ä¿¡å¿ƒé–¾å€¼
        
        Returns:
            List of {'text': str, 'hpo_id': str, 'confidence': float}
        
        Example:
            >>> extractor = SymptomExtractor()
            >>> result = extractor.extract_symptoms(
            ...     "æ‚£è€…8æ­²å¥³å­©,é€æ¼¸å‡ºç¾å››è‚¢ç„¡åŠ›,é‹å‹•å¾Œå¿ƒè·³åŠ é€Ÿ"
            ... )
            >>> print(result)
            [
                {'text': 'å››è‚¢ç„¡åŠ›', 'hpo_id': 'HP:0003324', 'confidence': 0.89},
                {'text': 'å¿ƒè·³åŠ é€Ÿ', 'hpo_id': 'HP:0001649', 'confidence': 0.85}
            ]
        """
        if not self._initialized:
            self.initialize()
        
        # TODO: å¯¦éš› NER æ¨ç† (Phase 2)
        logger.warning("SymptomExtractor is not fully implemented yet (Phase 2)")
        
        # è‡¨æ™‚è¿”å›ç©ºåˆ—è¡¨
        return []
    
    def batch_extract(
        self,
        texts: List[str],
        confidence_threshold: float = 0.7
    ) -> List[List[Dict[str, any]]]:
        """æ‰¹é‡æå–ç—‡ç‹€"""
        return [self.extract_symptoms(text, confidence_threshold) for text in texts]


class HPOMatcher:
    """HPO è¡“èªåŒ¹é…å™¨ (æ¨¡ç³ŠåŒ¹é…)"""
    
    def __init__(self, hpo_index_path: str):
        self.hpo_index_path = hpo_index_path
        self.hpo_index = None
    
    def build_index(self):
        """å»ºç«‹ HPO æœå°‹ç´¢å¼• (ä½¿ç”¨ FAISS æˆ– hnswlib)"""
        # TODO: Phase 2 å¯¦ç¾
        pass
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, str, float]]:
        """
        æ¨¡ç³Šæœå°‹ HPO terms
        
        Returns:
            List of (hpo_id, hpo_name, similarity_score)
        """
        # TODO: Phase 2 å¯¦ç¾
        return []
```

### 2. FHIR é©é…å™¨

**ä½ç½®**: `src/medical_standards/fhir_adapter.py`

```python
"""
FHIR é©é…å™¨ - æ•´åˆ HL7 FHIR ç—…æ­·è³‡æ–™
ç‹€æ…‹: ğŸŸ¡ Phase 2 å¯¦ç¾ (é ç•™æ¥å£)
"""
from typing import Dict, List, Any
from fhir.resources.bundle import Bundle
from fhir.resources.patient import Patient
from fhir.resources.condition import Condition
from fhir.resources.observation import Observation
import logging

logger = logging.getLogger(__name__)


class FHIRAdapter:
    """FHIR è³‡æ–™é©é…å™¨"""
    
    def __init__(self):
        self.supported_resources = [
            "Patient",
            "Condition",
            "Observation",
            "DiagnosticReport",
            "FamilyMemberHistory"
        ]
    
    def parse_bundle(self, fhir_bundle: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æ FHIR Bundle ä¸¦è½‰æ›ç‚ºå…§éƒ¨æ ¼å¼
        
        Args:
            fhir_bundle: FHIR Bundle JSON
        
        Returns:
            Internal patient data format
        
        Example:
            >>> adapter = FHIRAdapter()
            >>> internal_data = adapter.parse_bundle(fhir_bundle)
            >>> print(internal_data['phenotypes'])
            ['HP:0003324', 'HP:0011675']
        """
        try:
            bundle = Bundle.parse_obj(fhir_bundle)
        except Exception as e:
            logger.error(f"Failed to parse FHIR Bundle: {e}")
            raise
        
        # åˆå§‹åŒ–å…§éƒ¨è³‡æ–™çµæ§‹
        internal_data = {
            "patient_id": None,
            "phenotypes": [],
            "diagnoses": {"icd10": [], "snomed": []},
            "medical_history": {
                "family_history": [],
                "lab_results": {},
                "medications": []
            },
            "demographics": {}
        }
        
        # æå– Patient è³‡æº
        for entry in bundle.entry or []:
            resource = entry.resource
            
            if resource.resource_type == "Patient":
                internal_data["patient_id"] = f"P{resource.id}"
                internal_data["demographics"] = self._extract_demographics(resource)
            
            elif resource.resource_type == "Condition":
                internal_data["diagnoses"] = self._extract_conditions(resource)
            
            elif resource.resource_type == "Observation":
                phenotypes, lab_results = self._extract_observations(resource)
                internal_data["phenotypes"].extend(phenotypes)
                internal_data["medical_history"]["lab_results"].update(lab_results)
            
            # TODO: è™•ç†å…¶ä»–è³‡æºé¡å‹
        
        return internal_data
    
    def _extract_demographics(self, patient: Patient) -> Dict[str, Any]:
        """æå–äººå£çµ±è¨ˆè³‡æ–™"""
        return {
            "age": self._calculate_age(patient.birthDate) if patient.birthDate else None,
            "gender": patient.gender,
        }
    
    def _extract_conditions(self, condition: Condition) -> Dict[str, List[str]]:
        """æå–è¨ºæ–·ç·¨ç¢¼"""
        diagnoses = {"icd10": [], "snomed": []}
        
        for coding in condition.code.coding or []:
            if coding.system == "http://hl7.org/fhir/sid/icd-10":
                diagnoses["icd10"].append(coding.code)
            elif coding.system == "http://snomed.info/sct":
                diagnoses["snomed"].append(coding.code)
        
        return diagnoses
    
    def _extract_observations(
        self,
        observation: Observation
    ) -> Tuple[List[str], Dict[str, Any]]:
        """æå–è§€å¯Ÿçµæœ (ç—‡ç‹€ + æª¢é©—æ•¸æ“š)"""
        phenotypes = []
        lab_results = {}
        
        # TODO: å¯¦éš›æ˜ å°„é‚è¼¯
        
        return phenotypes, lab_results
    
    @staticmethod
    def _calculate_age(birth_date: str) -> int:
        """è¨ˆç®—å¹´é½¡"""
        from datetime import datetime
        birth = datetime.fromisoformat(birth_date)
        today = datetime.now()
        return (today - birth).days // 365
    
    def export_to_fhir(self, internal_data: Dict[str, Any]) -> Dict[str, Any]:
        """å°‡å…§éƒ¨æ ¼å¼è½‰æ›ç‚º FHIR Bundle (ç”¨æ–¼è³‡æ–™åŒ¯å‡º)"""
        # TODO: Phase 2 å¯¦ç¾
        pass
```

### 3. æ™ºèƒ½è¼¸å…¥è¡¨å–® (Gradio Component)

**ä½ç½®**: `src/webui/components/input_form.py`

```python
"""
æ™ºèƒ½è¼¸å…¥è¡¨å–® - Gradio UI çµ„ä»¶
"""
import gradio as gr
from typing import Dict, Any, List
import logging

logger = logging.getLogger(__name__)


class SmartInputForm:
    """æ™ºèƒ½æ‚£è€…è³‡æ–™è¼¸å…¥è¡¨å–®"""
    
    def __init__(self):
        self.hpo_search_enabled = True
        self.nlp_extraction_enabled = False  # Phase 2 å•Ÿç”¨
    
    def create_interface(self) -> gr.Blocks:
        """å»ºç«‹ Gradio ç•Œé¢"""
        
        with gr.Blocks() as interface:
            gr.Markdown("# ğŸ¥ SHEPHERD è¨ºæ–·æ¨ç†ç³»çµ±")
            
            with gr.Tab("çµæ§‹åŒ–è¼¸å…¥"):
                patient_id = gr.Textbox(
                    label="æ‚£è€… ID",
                    placeholder="P12345",
                    info="æ ¼å¼: P + 5-10ä½æ•¸å­—"
                )
                
                with gr.Row():
                    age = gr.Number(label="å¹´é½¡", value=None)
                    gender = gr.Radio(
                        label="æ€§åˆ¥",
                        choices=["male", "female", "other"],
                        value="female"
                    )
                
                # HPO ç—‡ç‹€è¼¸å…¥ (å¸¶æœå°‹)
                gr.Markdown("### ç—‡ç‹€ (HPO Terms)")
                with gr.Row():
                    hpo_search = gr.Textbox(
                        label="æœå°‹ HPO è¡“èª",
                        placeholder="è¼¸å…¥ç—‡ç‹€é—œéµå­—...",
                        interactive=True
                    )
                    hpo_results = gr.Dropdown(
                        label="æœå°‹çµæœ",
                        choices=[],
                        multiselect=False,
                        interactive=True
                    )
                
                selected_phenotypes = gr.Dataframe(
                    headers=["HPO ID", "åç¨±", "ä¿¡å¿ƒåˆ†æ•¸"],
                    datatype=["str", "str", "number"],
                    label="å·²é¸æ“‡ç—‡ç‹€",
                    interactive=True
                )
                
                # åŸºå› è³‡æ–™ (å¯é¸)
                with gr.Accordion("åŸºå› è³‡æ–™ (é¸å¡«)", open=False):
                    genes = gr.Textbox(
                        label="åŸºå› æ¸…å–®",
                        placeholder="DMD, BRCA1, TP53",
                        info="é€—è™Ÿåˆ†éš”"
                    )
                    variants = gr.Textbox(
                        label="è®Šç•°ä½é»",
                        placeholder="c.123G>A",
                        lines=3
                    )
                
                # ICD è¨ºæ–·ç¢¼ (å¯é¸)
                with gr.Accordion("è¨ºæ–·ç·¨ç¢¼ (é¸å¡«)", open=False):
                    icd10_codes = gr.Textbox(
                        label="ICD-10 ç·¨ç¢¼",
                        placeholder="G71.0, I47.2",
                        info="é€—è™Ÿåˆ†éš”"
                    )
                
                submit_btn = gr.Button("é–‹å§‹æ¨ç†", variant="primary")
            
            with gr.Tab("è‡ªç”±æ–‡å­—è¼¸å…¥"):
                gr.Markdown("### ğŸ“ ç—‡ç‹€æè¿° (è‡ªç„¶èªè¨€)")
                free_text = gr.Textbox(
                    label="ç—‡ç‹€æè¿°",
                    placeholder="ä¾‹å¦‚: æ‚£è€…8æ­²å¥³å­©,é€æ¼¸å‡ºç¾å››è‚¢ç„¡åŠ›,é‹å‹•å¾Œå¿ƒè·³åŠ é€Ÿ...",
                    lines=10
                )
                
                extract_btn = gr.Button("æå–ç—‡ç‹€", variant="secondary")
                extracted_symptoms = gr.Dataframe(
                    headers=["ç—‡ç‹€æ–‡å­—", "HPO ID", "ä¿¡å¿ƒåˆ†æ•¸"],
                    label="æå–çµæœ"
                )
                
                confirm_btn = gr.Button("ç¢ºèªä¸¦æ¨ç†", variant="primary")
            
            with gr.Tab("FHIR åŒ¯å…¥"):
                fhir_upload = gr.File(
                    label="ä¸Šå‚³ FHIR Bundle (JSON)",
                    file_types=[".json"]
                )
                fhir_preview = gr.JSON(label="é è¦½")
                fhir_submit_btn = gr.Button("ä½¿ç”¨ FHIR è³‡æ–™æ¨ç†", variant="primary")
            
            # çµæœé¡¯ç¤º
            gr.Markdown("---")
            gr.Markdown("## ğŸ“Š æ¨ç†çµæœ")
            
            with gr.Row():
                with gr.Column(scale=2):
                    result_table = gr.Dataframe(
                        headers=["æ’å", "ç–¾ç—…åç¨±", "MONDO ID", "ä¿¡å¿ƒåˆ†æ•¸"],
                        label="å€™é¸ç–¾ç—…"
                    )
                
                with gr.Column(scale=1):
                    reasoning_path = gr.JSON(label="æ¨ç†è·¯å¾‘")
            
            explanation = gr.Textbox(
                label="è©³ç´°è§£é‡‹",
                lines=10,
                interactive=False
            )
            
            # äº‹ä»¶è™•ç†
            hpo_search.change(
                fn=self._search_hpo_terms,
                inputs=[hpo_search],
                outputs=[hpo_results]
            )
            
            extract_btn.click(
                fn=self._extract_symptoms_from_text,
                inputs=[free_text],
                outputs=[extracted_symptoms]
            )
            
            submit_btn.click(
                fn=self._run_inference,
                inputs=[patient_id, age, gender, selected_phenotypes, genes, icd10_codes],
                outputs=[result_table, reasoning_path, explanation]
            )
        
        return interface
    
    def _search_hpo_terms(self, query: str) -> List[str]:
        """æœå°‹ HPO è¡“èª (æ¨¡ç³ŠåŒ¹é…)"""
        if not query or len(query) < 2:
            return []
        
        # TODO: å¯¦éš› HPO æœå°‹é‚è¼¯
        # æš«æ™‚è¿”å›ç¯„ä¾‹
        return [
            "HP:0003324 - è‚Œè‚‰ç„¡åŠ› (Muscle weakness)",
            "HP:0011675 - å¿ƒå¾‹ä¸æ•´ (Arrhythmia)",
            "HP:0000365 - è½åŠ›å–ªå¤± (Hearing loss)"
        ]
    
    def _extract_symptoms_from_text(self, free_text: str) -> List[List[str]]:
        """å¾è‡ªç”±æ–‡å­—æå–ç—‡ç‹€"""
        # TODO: èª¿ç”¨ NLP æ¨¡å¡Š
        logger.warning("NLP extraction not implemented yet (Phase 2)")
        return []
    
    def _run_inference(self, *args) -> tuple:
        """åŸ·è¡Œæ¨ç†"""
        # TODO: èª¿ç”¨æ¨ç†ç®¡é“
        return [], {}, "æ¨ç†åŠŸèƒ½å°šæœªå®Œå…¨å¯¦ç¾"
```

---

## ğŸ”Œ æ“´å……æ¥å£è¨­è¨ˆ

### 1. LLM æ¥å£ (ç­–ç•¥æ¨¡å¼)

**ä½ç½®**: `src/llm/interface.py`

```python
"""
LLM æ¥å£å®šç¾© - æ”¯æ´å¤šç¨®å¾Œç«¯å¯¦ç¾
"""
from abc import ABC, abstractmethod
from typing import List, Optional, Dict, Any
from dataclasses import dataclass


@dataclass
class LLMConfig:
    """LLM é…ç½®"""
    model_name: str
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9


class LLMInterface(ABC):
    """LLM æŠ½è±¡æ¥å£ (åƒ…è² è²¬æ–‡æœ¬ç”Ÿæˆ)"""
    
    @abstractmethod
    def generate(
        self,
        prompt: str,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬ (ç´”å‡½æ•¸,ç„¡å‰¯ä½œç”¨)
        
        Args:
            prompt: è¼¸å…¥æç¤º
            max_tokens: æœ€å¤§ç”Ÿæˆé•·åº¦
            temperature: æº«åº¦åƒæ•¸
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        pass
    
    @abstractmethod
    def batch_generate(
        self,
        prompts: List[str],
        **kwargs
    ) -> List[str]:
        """æ‰¹é‡ç”Ÿæˆæ–‡æœ¬"""
        pass
    
    @abstractmethod
    def get_model_info(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹è³‡è¨Š"""
        pass


class VLLMBackend(LLMInterface):
    """vLLM å¾Œç«¯å¯¦ç¾ (é›¢ç·šæ¨ç†)"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.engine = None
    
    def initialize(self):
        """åˆå§‹åŒ– vLLM å¼•æ“"""
        from vllm import LLM
        
        self.engine = LLM(
            model=self.config.model_name,
            tensor_parallel_size=1,
            gpu_memory_utilization=0.9
        )
    
    def generate(self, prompt: str, **kwargs) -> str:
        if not self.engine:
            self.initialize()
        
        from vllm import SamplingParams
        
        sampling_params = SamplingParams(
            max_tokens=kwargs.get('max_tokens', self.config.max_tokens),
            temperature=kwargs.get('temperature', self.config.temperature),
            top_p=self.config.top_p
        )
        
        outputs = self.engine.generate([prompt], sampling_params)
        return outputs[0].outputs[0].text
    
    def batch_generate(self, prompts: List[str], **kwargs) -> List[str]:
        # TODO: å¯¦ç¾æ‰¹é‡ç”Ÿæˆ
        return [self.generate(p, **kwargs) for p in prompts]
    
    def get_model_info(self) -> Dict[str, Any]:
        return {
            "backend": "vLLM",
            "model_name": self.config.model_name,
            "max_tokens": self.config.max_tokens
        }


class LlamaCppBackend(LLMInterface):
    """llama.cpp å¾Œç«¯å¯¦ç¾ (ARM å„ªåŒ–)"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.model = None
    
    def initialize(self):
        """åˆå§‹åŒ– llama.cpp"""
        from llama_cpp import Llama
        
        self.model = Llama(
            model_path=self.config.model_name,
            n_ctx=2048,
            n_gpu_layers=-1  # å…¨éƒ¨ GPU åŠ é€Ÿ
        )
    
    def generate(self, prompt: str, **kwargs) -> str:
        if not self.model:
            self.initialize()
        
        output = self.model(
            prompt,
            max_tokens=kwargs.get('max_tokens', self.config.max_tokens),
            temperature=kwargs.get('temperature', self.config.temperature),
            top_p=self.config.top_p
        )
        
        return output['choices'][0]['text']
    
    def batch_generate(self, prompts: List[str], **kwargs) -> List[str]:
        return [self.generate(p, **kwargs) for p in prompts]
    
    def get_model_info(self) -> Dict[str, Any]:
        return {
            "backend": "llama.cpp",
            "model_name": self.config.model_name
        }
```

### 2. é†«ç™‚æ¨™æº–æ˜ å°„æ¥å£

**ä½ç½®**: `src/medical_standards/mapper_interface.py`

```python
"""
é†«ç™‚æ¨™æº–æ˜ å°„æ¥å£ - çµ±ä¸€ä¸åŒç·¨ç¢¼é«”ç³»
"""
from abc import ABC, abstractmethod
from typing import List, Dict, Optional
from dataclasses import dataclass


@dataclass
class MappingResult:
    """æ˜ å°„çµæœ"""
    source_code: str
    target_code: str
    confidence: float
    mapping_type: str  # "exact", "broad", "narrow", "related"


class MedicalCodeMapper(ABC):
    """é†«ç™‚ç·¨ç¢¼æ˜ å°„å™¨æŠ½è±¡æ¥å£"""
    
    @abstractmethod
    def map_to_hpo(self, code: str, system: str) -> List[MappingResult]:
        """
        å°‡å…¶ä»–ç·¨ç¢¼ç³»çµ±æ˜ å°„åˆ° HPO
        
        Args:
            code: æºç·¨ç¢¼
            system: ç·¨ç¢¼ç³»çµ± (icd10, icd11, snomed)
        
        Returns:
            æ˜ å°„çµæœåˆ—è¡¨
        """
        pass
    
    @abstractmethod
    def map_to_mondo(self, code: str, system: str) -> List[MappingResult]:
        """æ˜ å°„åˆ° MONDO ç–¾ç—…æœ¬é«”"""
        pass
    
    @abstractmethod
    def reverse_map(self, hpo_id: str, target_system: str) -> List[MappingResult]:
        """åå‘æ˜ å°„: HPO â†’ å…¶ä»–ç³»çµ±"""
        pass


class ICDMapper(MedicalCodeMapper):
    """ICD-10/11 æ˜ å°„å™¨"""
    
    def __init__(self, mapping_file: str):
        self.mapping_file = mapping_file
        self.mappings = {}
    
    def load_mappings(self):
        """è¼‰å…¥æ˜ å°„è¡¨"""
        # TODO: è¼‰å…¥é å»ºæ˜ å°„è¡¨
        pass
    
    def map_to_hpo(self, code: str, system: str) -> List[MappingResult]:
        # TODO: å¯¦ç¾ ICD â†’ HPO æ˜ å°„
        return []
    
    def map_to_mondo(self, code: str, system: str) -> List[MappingResult]:
        # TODO: å¯¦ç¾ ICD â†’ MONDO æ˜ å°„
        return []
    
    def reverse_map(self, hpo_id: str, target_system: str) -> List[MappingResult]:
        # TODO: å¯¦ç¾åå‘æ˜ å°„
        return []
```

---

## âœ… å¯¦æ–½æª¢æŸ¥æ¸…å–®

### Phase 1: æ ¸å¿ƒæ¶æ§‹ (Week 1-2)

#### é…ç½®èˆ‡é©—è­‰ç³»çµ±
- [ ] å‰µå»º `pyproject.toml` + å·¥å…·éˆé…ç½®
- [ ] å‰µå»º `.import-linter.ini` ä¾è³´è¦å‰‡
- [ ] å‰µå»º `.pre-commit-config.yaml` Git hooks
- [ ] å¯¦ç¾ `ConfigValidator` (é…ç½®é©—è­‰å™¨)
- [ ] å‰µå»ºæ‰€æœ‰ JSON Schema æª”æ¡ˆ
  - [ ] `patient_input.schema.json`
  - [ ] `inference_output.schema.json`
  - [ ] `base_config.schema.json`
  - [ ] `model_config.schema.json`
  - [ ] `data_config.schema.json`

#### ç‰ˆæœ¬ç®¡ç†ç³»çµ±
- [ ] å¯¦ç¾ `VersionChecker` (ç‰ˆæœ¬å…¼å®¹æ€§æª¢æŸ¥)
- [ ] å¯¦ç¾ `hash_generator.py` (è³‡æ–™å“ˆå¸Œç”Ÿæˆ)
- [ ] å‰µå»ºå¢å¼·ç‰ˆ KG metadata.json æ¨¡æ¿
- [ ] å‰µå»ºæ¨¡å‹ registry.json æ¨¡æ¿
- [ ] æ›´æ–° `builder.py` è‡ªå‹•ç”Ÿæˆ metadata

#### æ¸¬è©¦åŸºç¤è¨­æ–½
- [ ] å‰µå»ºå¹³å°ç‰¹å®šæ¸¬è©¦æ¡†æ¶
  - [ ] `test_attention_x86.py`
  - [ ] `test_attention_arm.py`
  - [ ] `test_vector_index_x86.py`
  - [ ] `test_vector_index_arm.py`
- [ ] å‰µå»ºæ¸¬è©¦è³‡æ–™ fixtures
  - [ ] `sample_patients.json`
  - [ ] `sample_fhir.json`

**é è¨ˆå·¥ä½œé‡**: 16-20 å°æ™‚

---

### Phase 2: é†«ç™‚åŠŸèƒ½æ•´åˆ (Week 3-6)

#### NLP æ¨¡å¡Š (ğŸŸ¡ ä¸­å„ªå…ˆç´š)
- [ ] å¯¦ç¾ `SymptomExtractor` åŸºç¤é¡
- [ ] å¯¦ç¾ `HPOMatcher` (æ¨¡ç³ŠåŒ¹é…)
- [ ] ä¸‹è¼‰ä¸¦æ•´åˆ SciBERT/ClinicalBERT
- [ ] å»ºç«‹ HPO è¡“èªæœå°‹ç´¢å¼•
- [ ] å¯¦ç¾æ‰¹é‡æå–æ¥å£
- [ ] ç·¨å¯« NLP å–®å…ƒæ¸¬è©¦

#### FHIR/HISS é©é…å™¨ (ğŸŸ¡ ä¸­å„ªå…ˆç´š)
- [ ] å¯¦ç¾ `FHIRAdapter` åŸºç¤é¡
- [ ] å¯¦ç¾ `HI SSAdapter` åŸºç¤é¡
- [ ] æ”¯æ´ Patient, Condition, Observation è³‡æº
- [ ] å¯¦ç¾ FHIR â†’ å…§éƒ¨æ ¼å¼è½‰æ›
- [ ] å¯¦ç¾å…§éƒ¨æ ¼å¼ â†’ FHIR åŒ¯å‡º
- [ ] ç·¨å¯« FHIR æ•´åˆæ¸¬è©¦

#### é†«ç™‚æ¨™æº–æ˜ å°„ (ğŸŸ¡ ä¸­å„ªå…ˆç´š)
- [ ] å¯¦ç¾ `ICDMapper` (ICD-10/11 â†’ HPO/MONDO)
- [ ] å¯¦ç¾ `SNOMEDMapper`
- [ ] ä¸‹è¼‰ä¸¦æ•´åˆæ˜ å°„è¡¨ (UMLS, BioPortal)
- [ ] å¯¦ç¾åå‘æ˜ å°„åŠŸèƒ½
- [ ] ç·¨å¯«æ˜ å°„å–®å…ƒæ¸¬è©¦

#### WebUI å¢å¼· (ğŸŸ¢ ä½å„ªå…ˆç´š)
- [ ] å¯¦ç¾ `SmartInputForm` (æ™ºèƒ½è¡¨å–®)
- [ ] å¯¦ç¾ HPO æœå°‹çµ„ä»¶
- [ ] å¯¦ç¾è‡ªç”±æ–‡å­—è¼¸å…¥ UI
- [ ] å¯¦ç¾ FHIR ä¸Šå‚³çµ„ä»¶
- [ ] å¯¦ç¾çµæœå¯è¦–åŒ–çµ„ä»¶

**é è¨ˆå·¥ä½œé‡**: 40-50 å°æ™‚

---

### Phase 3: æ“´å……æ¥å£ (Week 7-8)

#### LLM æ¥å£æ¨™æº–åŒ–
- [ ] å¯¦ç¾ `LLMInterface` æŠ½è±¡é¡
- [ ] å¯¦ç¾ `VLLMBackend`
- [ ] å¯¦ç¾ `LlamaCppBackend`
- [ ] æ¸¬è©¦å¤šå¾Œç«¯åˆ‡æ›
- [ ] æ€§èƒ½åŸºæº–æ¸¬è©¦

#### è¼¸å…¥/è¼¸å‡ºé©—è­‰
- [ ] å¯¦ç¾ `InputValidator` (JSON Schema + Pydantic)
- [ ] å¯¦ç¾ `OutputFormatter`
- [ ] æ•´åˆåˆ°æ¨ç†ç®¡é“
- [ ] ç·¨å¯«ç«¯åˆ°ç«¯æ¸¬è©¦

#### æ–‡æª”æ›´æ–°
- [ ] æ›´æ–° `architecture_v3.md`
- [ ] æ’°å¯« `medical_integration.md`
- [ ] æ›´æ–° API åƒè€ƒæ–‡æª”
- [ ] æ’°å¯«éƒ¨ç½²æŒ‡å—

**é è¨ˆå·¥ä½œé‡**: 20-24 å°æ™‚

---

## ğŸ“Š ç¸½æ™‚é–“ä¼°ç®—

| éšæ®µ | å·¥ä½œå…§å®¹ | é è¨ˆæ™‚é–“ | å„ªå…ˆç´š |
|------|---------|----------|--------|
| Phase 1 | æ ¸å¿ƒæ¶æ§‹ + é©—è­‰ç³»çµ± | 16-20h | ğŸ”´ P0 |
| Phase 2 | é†«ç™‚åŠŸèƒ½æ•´åˆ | 40-50h | ğŸŸ¡ P1 |
| Phase 3 | æ“´å……æ¥å£ + æ–‡æª” | 20-24h | ğŸŸ¢ P2 |
| **ç¸½è¨ˆ** | | **76-94h** | **ç´„2-3é€±** |

---

## ğŸ¯ é—œéµæˆåŠŸæŒ‡æ¨™

### æŠ€è¡“æŒ‡æ¨™
- âœ… æ‰€æœ‰é…ç½®æª”æ¡ˆé€šé JSON Schema é©—è­‰
- âœ… import-linter æª¢æŸ¥é€šé (ç„¡å¾ªç’°ä¾è³´)
- âœ… å¹³å°ç‰¹å®šæ¸¬è©¦è¦†è“‹ç‡ > 80%
- âœ… ç‰ˆæœ¬å…¼å®¹æ€§æª¢æŸ¥è‡ªå‹•åŒ–

### é†«ç™‚åŠŸèƒ½æŒ‡æ¨™
- âœ… NLP ç—‡ç‹€æå–æº–ç¢ºç‡ > 75% (Phase 2)
- âœ… FHIR è³‡æ–™è§£ææˆåŠŸç‡ > 95%
- âœ… ICD/SNOMED æ˜ å°„è¦†è“‹ç‡ > 80%

### å¯ç¶­è­·æ€§æŒ‡æ¨™
- âœ… ç¨‹å¼ç¢¼è¨»è§£è¦†è“‹ç‡ > 60%
- âœ… æ–‡æª”å®Œæ•´æ€§è©•åˆ† > 90%
- âœ… CI/CD æµç¨‹è‡ªå‹•åŒ–ç‡ 100%

---

## ğŸ”„ ç‰ˆæœ¬æ­·å²

| ç‰ˆæœ¬ | æ—¥æœŸ | è®Šæ›´å…§å®¹ |
|------|------|----------|
| v1.0 | 2025-10-07 | åˆå§‹ç‰ˆæœ¬ (åŸºç¤ç›®éŒ„çµæ§‹) |
| v2.0 | 2025-10-22 | æ•´åˆ ChatGPT å»ºè­° (å·¥å…·éˆ + metadata) |
| v3.0 | 2025-11-04 | **æ•´åˆé†«ç”Ÿåœ˜éšŠå»ºè­° (NLP + FHIR + æ™ºèƒ½UI)** |

---

## ğŸ“ è¯çµ¡è³‡è¨Š

**é …ç›®è² è²¬äºº**: [å¾…å¡«å¯«]  
**æŠ€è¡“å¯©æ ¸**: [å¾…å¡«å¯«]  
**é†«ç™‚é¡§å•**: [å¾…å¡«å¯«]  

---

**æ–‡æª”çµæŸ**
