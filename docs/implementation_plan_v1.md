# SHEPHERD-Advanced å¯¦æ–½è¨ˆåŠƒ - è³‡æ–™çµæ§‹èˆ‡é†«ç™‚æ•´åˆ

**ç‰ˆæœ¬**: 1.0  
**æ—¥æœŸ**: 2025-11-04  
**åƒè€ƒæ–‡æª”**: `data_structure_and_validation_v3.md`

---

## ğŸ¯ å¯¦æ–½ç›®æ¨™

1. **å»ºç«‹å®Œæ•´çš„è³‡æ–™çµæ§‹èˆ‡æ ¡é©—ç³»çµ±** (v3.0 æ¶æ§‹)
2. **æ•´åˆé†«ç”Ÿåœ˜éšŠå»ºè­°** (NLP + FHIR + å¤šæ¨¡æ…‹è¼¸å…¥)
3. **é ç•™æ“´å……æ¥å£** (ç¢ºä¿æœªä¾†å¯æ“´å±•æ€§)

---

## ğŸ“… å¯¦æ–½æ™‚ç¨‹

```
Week 1-2: Phase 1 - æ ¸å¿ƒæ¶æ§‹èˆ‡é©—è­‰ç³»çµ± (ğŸ”´ P0)
Week 3-6: Phase 2 - é†«ç™‚åŠŸèƒ½æ•´åˆ (ğŸŸ¡ P1)  
Week 7-8: Phase 3 - æ“´å……æ¥å£èˆ‡æ–‡æª” (ğŸŸ¢ P2)
```

**ç¸½é è¨ˆæ™‚é–“**: 8 é€±  
**æœ€å¿«å®Œæˆ**: 6 é€± (å¦‚æœä¸¦è¡Œä½œæ¥­)  
**å»ºè­°å®Œæˆ**: 8 é€± (ç©©å¥é–‹ç™¼)

---

## ğŸ“‹ Phase 1: æ ¸å¿ƒæ¶æ§‹ (Week 1-2)

### Day 1-2: å°ˆæ¡ˆé…ç½®èˆ‡å·¥å…·éˆ

#### ä»»å‹™ 1.1: å»ºç«‹ Python å°ˆæ¡ˆé…ç½®
```bash
# 1. å‰µå»º pyproject.toml
cd /path/to/shepherd-advanced
cat > pyproject.toml << 'EOF'
[project]
name = "shepherd-advanced"
version = "1.0.0"
requires-python = ">=3.10"
dependencies = [
    "torch>=2.5.0",
    "torch-geometric>=2.5.0",
    "pronto>=2.5.0",
    "pydantic>=2.0.0",
    "fastapi>=0.100.0",
    "gradio>=4.0.0",
    "jsonschema>=4.0.0",
    "pyyaml>=6.0",
]

[project.optional-dependencies]
nlp = [
    "transformers>=4.30.0",
    "scispacy>=0.5.0",
]
medical = [
    "fhir.resources>=7.0.0",
    "python-hl7>=0.4.0",
]
dev = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "black>=23.0.0",
    "ruff>=0.1.0",
    "mypy>=1.0.0",
    "import-linter>=1.12.0",
]

[tool.black]
line-length = 100

[tool.ruff]
line-length = 100
select = ["E", "F", "I", "N", "W"]

[tool.mypy]
python_version = "3.10"
strict = true

[tool.pytest.ini_options]
testpaths = ["tests"]
addopts = "-v --cov=src"
EOF

# 2. å®‰è£ä¾è³´
pip install -e ".[dev,nlp,medical]"
```

**æª¢æŸ¥é»**:
- [ ] pyproject.toml å‰µå»ºå®Œæˆ
- [ ] æ‰€æœ‰æ ¸å¿ƒä¾è³´å®‰è£æˆåŠŸ
- [ ] å¯é¸ä¾è³´ (nlp, medical) å®‰è£æˆåŠŸ
- [ ] black/ruff å¯æ­£å¸¸åŸ·è¡Œ

---

#### ä»»å‹™ 1.2: é…ç½®ä¾è³´è¦å‰‡æª¢æŸ¥
```bash
# å‰µå»º .import-linter.ini
cat > .import-linter.ini << 'EOF'
[importlinter]
root_package = src

[importlinter:contract:layers]
name = Enforce layered architecture
type = layers
layers =
    src.utils
    src.config
    src.ontology
    src.kg
    src.nlp
    src.medical_standards
    src.models
    src.retrieval
    src.reasoning
    src.llm
    src.training
    src.inference
    src.api
    src.webui

[importlinter:contract:forbidden]
name = Forbidden imports
type = forbidden
source_modules =
    src.models
    src.inference
forbidden_modules =
    src.api
    src.webui
    src.training
EOF

# æ¸¬è©¦ä¾è³´æª¢æŸ¥
lint-imports
```

**æª¢æŸ¥é»**:
- [ ] .import-linter.ini å‰µå»ºå®Œæˆ
- [ ] ä¾è³´æª¢æŸ¥é€šé (åˆæ¬¡å¯èƒ½æœ‰è­¦å‘Š)
- [ ] ç†è§£åˆ†å±¤æ¶æ§‹è¦å‰‡

---

#### ä»»å‹™ 1.3: é…ç½® Git Hooks
```bash
# å‰µå»º .pre-commit-config.yaml
cat > .pre-commit-config.yaml << 'EOF'
repos:
  - repo: https://github.com/psf/black
    rev: 23.12.0
    hooks:
      - id: black
        language_version: python3.12

  - repo: https://github.com/charliermarsh/ruff-pre-commit
    rev: v0.1.9
    hooks:
      - id: ruff
        args: [--fix]

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        additional_dependencies: [types-PyYAML, types-requests]
EOF

# å®‰è£ pre-commit hooks
pre-commit install
```

**æª¢æŸ¥é»**:
- [ ] .pre-commit-config.yaml å‰µå»ºå®Œæˆ
- [ ] pre-commit hooks å®‰è£æˆåŠŸ
- [ ] æ¸¬è©¦ commit æ™‚è‡ªå‹•æ ¼å¼åŒ–

---

### Day 3-4: JSON Schema èˆ‡é…ç½®é©—è­‰

#### ä»»å‹™ 2.1: å‰µå»º JSON Schema æª”æ¡ˆ
```bash
# å‰µå»º schemas ç›®éŒ„
mkdir -p configs/schemas

# 1. æ‚£è€…è¼¸å…¥ Schema
cat > configs/schemas/patient_input.schema.json << 'EOF'
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Patient Input Schema",
  "type": "object",
  "required": ["patient_id", "phenotypes"],
  "properties": {
    "patient_id": {
      "type": "string",
      "pattern": "^P[0-9]{5,10}$"
    },
    "phenotypes": {
      "type": "array",
      "minItems": 1,
      "items": {"type": "string", "pattern": "^HP:[0-9]{7}$"}
    },
    "demographics": {
      "type": "object",
      "properties": {
        "age": {"type": "integer", "minimum": 0, "maximum": 150},
        "gender": {"type": "string", "enum": ["male", "female", "other"]}
      }
    }
  }
}
EOF

# 2. æ¨ç†è¼¸å‡º Schema
cat > configs/schemas/inference_output.schema.json << 'EOF'
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Inference Output Schema",
  "type": "object",
  "required": ["patient_id", "timestamp", "top_candidates"],
  "properties": {
    "patient_id": {"type": "string"},
    "timestamp": {"type": "string", "format": "date-time"},
    "top_candidates": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["disease", "confidence"],
        "properties": {
          "disease": {
            "type": "object",
            "properties": {
              "mondo_id": {"type": "string", "pattern": "^MONDO:[0-9]{7}$"},
              "name": {"type": "string"}
            }
          },
          "confidence": {"type": "number", "minimum": 0, "maximum": 1}
        }
      }
    }
  }
}
EOF

# 3-5. å…¶ä»–é…ç½® Schema (base, model, data)
# ... (é¡ä¼¼æ ¼å¼)
```

**æª¢æŸ¥é»**:
- [ ] æ‰€æœ‰ 5 å€‹ schema æª”æ¡ˆå‰µå»ºå®Œæˆ
- [ ] JSON æ ¼å¼é©—è­‰é€šé (ä½¿ç”¨ jsonlint)
- [ ] Schema é‚è¼¯æ­£ç¢º (æ‰‹å‹•æ¸¬è©¦)

---

#### ä»»å‹™ 2.2: å¯¦ç¾é…ç½®é©—è­‰å™¨
```bash
# å‰µå»ºç›®éŒ„
mkdir -p src/config

# å‰µå»ºé©—è­‰å™¨
cat > src/config/config_validator.py << 'EOF'
"""é…ç½®é©—è­‰å™¨å¯¦ç¾"""
# (åƒè€ƒå®Œæ•´è¨­è¨ˆæ–‡æª”ä¸­çš„ç¨‹å¼ç¢¼)
EOF

# å‰µå»ºæ¸¬è©¦
cat > tests/unit/test_config_validator.py << 'EOF'
import pytest
from src.config.config_validator import ConfigValidator

def test_validate_patient_input():
    validator = ConfigValidator(...)
    # æ¸¬è©¦æœ‰æ•ˆè¼¸å…¥
    valid_input = {
        "patient_id": "P12345",
        "phenotypes": ["HP:0003324"]
    }
    assert validator.validate(valid_input) is True
    
    # æ¸¬è©¦ç„¡æ•ˆè¼¸å…¥
    invalid_input = {"patient_id": "INVALID"}
    with pytest.raises(ValidationError):
        validator.validate(invalid_input)
EOF

# åŸ·è¡Œæ¸¬è©¦
pytest tests/unit/test_config_validator.py -v
```

**æª¢æŸ¥é»**:
- [ ] `ConfigValidator` é¡å¯¦ç¾å®Œæˆ
- [ ] å¯è¼‰å…¥æ‰€æœ‰ JSON Schema
- [ ] å–®å…ƒæ¸¬è©¦é€šé
- [ ] CLI å‘½ä»¤å¯ç”¨: `python -m src.config.config_validator`

---

### Day 5-7: ç‰ˆæœ¬ç®¡ç†èˆ‡ Metadata

#### ä»»å‹™ 3.1: å¯¦ç¾ç‰ˆæœ¬æª¢æŸ¥å™¨
```bash
# å‰µå»ºç‰ˆæœ¬æª¢æŸ¥å™¨
cat > src/utils/version_checker.py << 'EOF'
"""ç‰ˆæœ¬å…¼å®¹æ€§æª¢æŸ¥å™¨"""
# (åƒè€ƒå®Œæ•´è¨­è¨ˆæ–‡æª”)
EOF

# å‰µå»ºå“ˆå¸Œç”Ÿæˆå™¨
cat > src/utils/hash_generator.py << 'EOF'
"""è³‡æ–™å“ˆå¸Œç”Ÿæˆå·¥å…·"""
import hashlib
from pathlib import Path

def compute_file_hash(file_path: Path) -> str:
    """è¨ˆç®— SHA256"""
    hasher = hashlib.sha256()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hasher.update(chunk)
    return f"sha256:{hasher.hexdigest()}"
EOF

# æ¸¬è©¦
python -c "
from src.utils.hash_generator import compute_file_hash
from pathlib import Path
print(compute_file_hash(Path('README.md')))
"
```

**æª¢æŸ¥é»**:
- [ ] `VersionChecker` å¯¦ç¾å®Œæˆ
- [ ] `hash_generator` å¯¦ç¾å®Œæˆ
- [ ] å¯è¨ˆç®—ä»»æ„æª”æ¡ˆå“ˆå¸Œ
- [ ] ç‰ˆæœ¬å…¼å®¹æ€§æª¢æŸ¥é‚è¼¯æ­£ç¢º

---

#### ä»»å‹™ 3.2: å‰µå»º Metadata æ¨¡æ¿
```bash
# 1. çŸ¥è­˜åœ–è­œ Metadata æ¨¡æ¿
cat > data/processed/knowledge_graph/metadata.json << 'EOF'
{
  "schema_version": "3.0",
  "data_version": "YYYY.MM.DD",
  "creation_timestamp": "ISO-8601",
  "generator": {
    "script": "scripts/build_knowledge_graph.py",
    "commit_sha": "COMMIT_SHA",
    "git_branch": "main"
  },
  "data_sources": {
    "hpo": {
      "version": "YYYY-MM-DD",
      "url": "http://purl.obolibrary.org/obo/hp.obo",
      "sha256": "TO_BE_FILLED"
    }
  },
  "statistics": {
    "num_nodes": 0,
    "num_edges": 0
  },
  "data_hash": {
    "graph_structure": "sha256:TO_BE_FILLED"
  }
}
EOF

# 2. æ¨¡å‹è¨»å†Šè¡¨æ¨¡æ¿
mkdir -p models/production
cat > models/production/registry.json << 'EOF'
{
  "registry_version": "1.0",
  "models": [],
  "current_production": null
}
EOF
```

**æª¢æŸ¥é»**:
- [ ] metadata.json æ¨¡æ¿å‰µå»º
- [ ] registry.json æ¨¡æ¿å‰µå»º
- [ ] ç†è§£ metadata çµæ§‹èˆ‡ç”¨é€”

---

#### ä»»å‹™ 3.3: æ›´æ–° KG Builder è‡ªå‹•ç”Ÿæˆ Metadata
```bash
# ç·¨è¼¯ src/kg/builder.py
# æ·»åŠ  metadata ç”Ÿæˆé‚è¼¯

cat >> src/kg/builder.py << 'EOF'

def generate_metadata(self, graph, output_path: Path):
    """ç”ŸæˆçŸ¥è­˜åœ–è­œ metadata"""
    import json
    from datetime import datetime
    from src.utils.hash_generator import compute_file_hash
    import subprocess
    
    # ç²å– Git è³‡è¨Š
    commit_sha = subprocess.check_output(
        ["git", "rev-parse", "HEAD"]
    ).decode().strip()
    
    metadata = {
        "schema_version": "3.0",
        "data_version": datetime.now().strftime("%Y.%m.%d"),
        "creation_timestamp": datetime.now().isoformat(),
        "generator": {
            "script": "scripts/build_knowledge_graph.py",
            "commit_sha": commit_sha,
            "git_branch": "main"
        },
        "statistics": {
            "num_nodes": graph.num_nodes,
            "num_edges": graph.num_edges
        },
        "data_hash": {
            "graph_structure": compute_file_hash(output_path / "hetero_graph.pt")
        }
    }
    
    with open(output_path / "metadata.json", "w") as f:
        json.dump(metadata, f, indent=2)
EOF
```

**æª¢æŸ¥é»**:
- [ ] KG builder å¯è‡ªå‹•ç”Ÿæˆ metadata
- [ ] Metadata åŒ…å«æ‰€æœ‰å¿…è¦æ¬„ä½
- [ ] è³‡æ–™å“ˆå¸Œè¨ˆç®—æ­£ç¢º

---

### Day 8-10: å¹³å°ç‰¹å®šæ¸¬è©¦

#### ä»»å‹™ 4.1: å‰µå»ºæ¸¬è©¦æ¡†æ¶
```bash
# å‰µå»ºæ¸¬è©¦ç›®éŒ„
mkdir -p tests/benchmarks/platform_specific

# x86 æ³¨æ„åŠ›æ¸¬è©¦
cat > tests/benchmarks/platform_specific/test_attention_x86.py << 'EOF'
import torch
import pytest

@pytest.mark.skipif(
    not torch.cuda.is_available(),
    reason="Requires CUDA"
)
@pytest.mark.x86_64
def test_flash_attention_x86():
    """æ¸¬è©¦ FlashAttention-2 åœ¨ x86 ä¸Šæ˜¯å¦å¯ç”¨"""
    device = torch.device("cuda")
    
    # å˜—è©¦ä½¿ç”¨ FlashAttention
    try:
        from flash_attn import flash_attn_func
        
        batch_size, seq_len, d_model = 4, 128, 512
        q = torch.randn(batch_size, seq_len, 8, 64, device=device)
        k = torch.randn(batch_size, seq_len, 8, 64, device=device)
        v = torch.randn(batch_size, seq_len, 8, 64, device=device)
        
        output = flash_attn_func(q, k, v)
        assert output.shape[0] == batch_size
        
    except ImportError:
        pytest.skip("FlashAttention not available")
EOF

# ARM æ³¨æ„åŠ›æ¸¬è©¦
cat > tests/benchmarks/platform_specific/test_attention_arm.py << 'EOF'
import torch
import pytest

@pytest.mark.arm64
def test_cudnn_sdpa_arm():
    """æ¸¬è©¦ cuDNN SDPA åœ¨ ARM ä¸Šæ˜¯å¦å¯ç”¨"""
    device = torch.device("cuda")
    
    batch_size, seq_len, d_model = 4, 128, 512
    q = torch.randn(batch_size, seq_len, d_model, device=device)
    k = torch.randn(batch_size, seq_len, d_model, device=device)
    v = torch.randn(batch_size, seq_len, d_model, device=device)
    
    # ä½¿ç”¨ PyTorch å…§å»º SDPA
    with torch.backends.cuda.sdp_kernel(
        enable_flash=False,
        enable_math=False,
        enable_mem_efficient=True
    ):
        output = torch.nn.functional.scaled_dot_product_attention(q, k, v)
    
    assert torch.isfinite(output).all()
EOF

# åŸ·è¡Œæ¸¬è©¦
pytest tests/benchmarks/platform_specific/ -v -m x86_64
```

**æª¢æŸ¥é»**:
- [ ] x86 æ¸¬è©¦æ¡†æ¶å»ºç«‹
- [ ] ARM æ¸¬è©¦æ¡†æ¶å»ºç«‹
- [ ] æ¸¬è©¦å¯æ­£ç¢ºæ¨™è¨˜å¹³å°
- [ ] CI å¯åˆ†åˆ¥åŸ·è¡Œä¸åŒå¹³å°æ¸¬è©¦

---

### Phase 1 å®Œæˆæª¢æŸ¥ âœ…

**å¿…é ˆé”æˆçš„é‡Œç¨‹ç¢‘**:
- [ ] âœ… pyproject.toml + å·¥å…·éˆæ­£å¸¸é‹ä½œ
- [ ] âœ… import-linter æª¢æŸ¥é€šé
- [ ] âœ… æ‰€æœ‰ JSON Schema å‰µå»ºä¸¦é©—è­‰é€šé
- [ ] âœ… ConfigValidator å¯¦ç¾ä¸¦æ¸¬è©¦é€šé
- [ ] âœ… VersionChecker å¯¦ç¾ä¸¦æ¸¬è©¦é€šé
- [ ] âœ… Metadata æ¨¡æ¿å‰µå»º
- [ ] âœ… å¹³å°ç‰¹å®šæ¸¬è©¦æ¡†æ¶å»ºç«‹
- [ ] âœ… æ–‡æª”æ›´æ–°: æ–°å¢å¯¦æ–½ç´€éŒ„

**é è¨ˆå·¥ä½œé‡**: 16-20 å°æ™‚  
**å¯¦éš›å·¥ä½œé‡**: _____ å°æ™‚ (å¾…å¡«å¯«)

---

## ğŸ“‹ Phase 2: é†«ç™‚åŠŸèƒ½æ•´åˆ (Week 3-6)

### Week 3: NLP æ¨¡å¡ŠåŸºç¤

#### ä»»å‹™ 5.1: å»ºç«‹ NLP æ¨¡å¡Šçµæ§‹
```bash
# å‰µå»ºç›®éŒ„
mkdir -p src/nlp
touch src/nlp/__init__.py

# å‰µå»ºä½”ä½æª”æ¡ˆ (Phase 2 å¯¦ç¾)
for file in symptom_extractor entity_recognizer clinical_bert hpo_matcher; do
    cat > src/nlp/${file}.py << 'EOF'
"""
${file} - Phase 2 å¯¦ç¾
TODO: æ•´åˆ SciBERT/ClinicalBERT
"""
from typing import List, Dict

class PlaceholderClass:
    """ä½”ä½é¡ - Phase 2 å¯¦ç¾"""
    
    def __init__(self):
        self._initialized = False
    
    def initialize(self):
        """å»¶é²åˆå§‹åŒ–"""
        raise NotImplementedError("Phase 2 implementation")
EOF
done
```

**æª¢æŸ¥é»**:
- [ ] src/nlp/ ç›®éŒ„çµæ§‹å‰µå»º
- [ ] ä½”ä½é¡å®šç¾©å®Œæˆ
- [ ] import è·¯å¾‘æ­£ç¢º

---

#### ä»»å‹™ 5.2: ä¸‹è¼‰ NLP é è¨“ç·´æ¨¡å‹
```bash
# ä¸‹è¼‰ SciBERT
mkdir -p models/pretrained/scibert
python -c "
from transformers import AutoTokenizer, AutoModel

model_name = 'allenai/scibert_scivocab_uncased'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

tokenizer.save_pretrained('models/pretrained/scibert')
model.save_pretrained('models/pretrained/scibert')
print('SciBERT downloaded successfully')
"

# å®‰è£ scispacy
pip install scispacy
pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.0/en_core_sci_sm-0.5.0.tar.gz
```

**æª¢æŸ¥é»**:
- [ ] SciBERT æ¨¡å‹ä¸‹è¼‰å®Œæˆ
- [ ] scispacy å®‰è£æˆåŠŸ
- [ ] å¯æˆåŠŸè¼‰å…¥æ¨¡å‹

---

#### ä»»å‹™ 5.3: å¯¦ç¾ HPO è¡“èªåŒ¹é…å™¨
```bash
# å¯¦ç¾ HPOMatcher (ç°¡åŒ–ç‰ˆ)
cat > src/nlp/hpo_matcher.py << 'EOF'
"""HPO è¡“èªåŒ¹é…å™¨ - æ¨¡ç³Šæœå°‹"""
from typing import List, Tuple
from pronto import Ontology
import re

class HPOMatcher:
    """HPO è¡“èªæ¨¡ç³ŠåŒ¹é…"""
    
    def __init__(self, hpo_path: str):
        self.hpo = Ontology(hpo_path)
        self._build_index()
    
    def _build_index(self):
        """å»ºç«‹æœå°‹ç´¢å¼•"""
        self.term_index = {}
        for term in self.hpo.terms():
            # ä¸»åç¨±
            self.term_index[term.name.lower()] = term.id
            # åŒç¾©è©
            for syn in term.synonyms:
                self.term_index[syn.description.lower()] = term.id
    
    def search(self, query: str, top_k: int = 5) -> List[Tuple[str, str, float]]:
        """
        æ¨¡ç³Šæœå°‹ HPO è¡“èª
        
        Returns:
            List of (hpo_id, hpo_name, similarity_score)
        """
        query = query.lower()
        results = []
        
        for term_name, hpo_id in self.term_index.items():
            # ç°¡å–®çš„å­—ä¸²ç›¸ä¼¼åº¦
            if query in term_name or term_name in query:
                score = len(query) / max(len(term_name), len(query))
                results.append((hpo_id, term_name, score))
        
        # æ’åºä¸¦è¿”å› top_k
        results.sort(key=lambda x: x[2], reverse=True)
        return results[:top_k]
EOF

# æ¸¬è©¦
python -c "
from src.nlp.hpo_matcher import HPOMatcher
matcher = HPOMatcher('data/raw/ontologies/hpo.obo')
results = matcher.search('muscle weakness')
print(results)
"
```

**æª¢æŸ¥é»**:
- [ ] HPOMatcher å¯¦ç¾å®Œæˆ
- [ ] å¯æœå°‹ HPO è¡“èª
- [ ] æœå°‹çµæœåˆç†

---

### Week 4: FHIR/HISS é©é…å™¨

#### ä»»å‹™ 6.1: å»ºç«‹é†«ç™‚æ¨™æº–æ¨¡å¡Š
```bash
# å‰µå»ºç›®éŒ„
mkdir -p src/medical_standards
touch src/medical_standards/__init__.py

# å®‰è£ FHIR åº«
pip install fhir.resources python-hl7
```

---

#### ä»»å‹™ 6.2: å¯¦ç¾ FHIR é©é…å™¨ (åŸºç¤ç‰ˆ)
```bash
cat > src/medical_standards/fhir_adapter.py << 'EOF'
"""FHIR é©é…å™¨ - åŸºç¤å¯¦ç¾"""
from typing import Dict, Any
from fhir.resources.bundle import Bundle
import logging

logger = logging.getLogger(__name__)

class FHIRAdapter:
    """FHIR è³‡æ–™é©é…å™¨"""
    
    def parse_bundle(self, fhir_json: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æ FHIR Bundle"""
        try:
            bundle = Bundle.parse_obj(fhir_json)
        except Exception as e:
            logger.error(f"Failed to parse FHIR Bundle: {e}")
            raise
        
        # æå–æ‚£è€…è³‡æ–™
        patient_data = {
            "patient_id": None,
            "phenotypes": [],
            "diagnoses": {"icd10": []},
            "demographics": {}
        }
        
        for entry in bundle.entry or []:
            resource = entry.resource
            
            if resource.resource_type == "Patient":
                patient_data["patient_id"] = f"P{resource.id}"
                if resource.birthDate:
                    from datetime import datetime
                    age = (datetime.now() - datetime.fromisoformat(str(resource.birthDate))).days // 365
                    patient_data["demographics"]["age"] = age
                patient_data["demographics"]["gender"] = resource.gender
            
            elif resource.resource_type == "Condition":
                for coding in resource.code.coding or []:
                    if "icd" in coding.system.lower():
                        patient_data["diagnoses"]["icd10"].append(coding.code)
        
        return patient_data
EOF

# æ¸¬è©¦
python -c "
from src.medical_standards.fhir_adapter import FHIRAdapter

# æ¸¬è©¦è³‡æ–™
fhir_json = {
    'resourceType': 'Bundle',
    'entry': [
        {
            'resource': {
                'resourceType': 'Patient',
                'id': '12345',
                'birthDate': '2010-01-01',
                'gender': 'female'
            }
        }
    ]
}

adapter = FHIRAdapter()
result = adapter.parse_bundle(fhir_json)
print(result)
"
```

**æª¢æŸ¥é»**:
- [ ] FHIRAdapter åŸºç¤å¯¦ç¾å®Œæˆ
- [ ] å¯è§£æ FHIR Bundle
- [ ] å–®å…ƒæ¸¬è©¦é€šé

---

### Week 5-6: WebUI å¢å¼·

#### ä»»å‹™ 7.1: å¯¦ç¾æ™ºèƒ½è¼¸å…¥è¡¨å–®
```bash
# å‰µå»º WebUI çµ„ä»¶
mkdir -p src/webui/components
touch src/webui/components/__init__.py

# å¯¦ç¾ HPO æœå°‹çµ„ä»¶
cat > src/webui/components/hpo_search.py << 'EOF'
"""HPO æœå°‹çµ„ä»¶"""
import gradio as gr
from src.nlp.hpo_matcher import HPOMatcher

class HPOSearchComponent:
    """HPO è¡“èªæœå°‹ UI çµ„ä»¶"""
    
    def __init__(self, hpo_path: str):
        self.matcher = HPOMatcher(hpo_path)
    
    def search(self, query: str) -> list:
        """æœå°‹ HPO è¡“èª"""
        if not query or len(query) < 2:
            return []
        
        results = self.matcher.search(query, top_k=10)
        return [f"{hpo_id} - {name}" for hpo_id, name, score in results]
    
    def create_ui(self):
        """å‰µå»º UI"""
        with gr.Row():
            search_box = gr.Textbox(
                label="æœå°‹ HPO è¡“èª",
                placeholder="è¼¸å…¥ç—‡ç‹€é—œéµå­—..."
            )
            results = gr.Dropdown(
                label="æœå°‹çµæœ",
                choices=[],
                multiselect=False
            )
        
        search_box.change(
            fn=self.search,
            inputs=[search_box],
            outputs=[results]
        )
        
        return search_box, results
EOF
```

---

#### ä»»å‹™ 7.2: æ•´åˆåˆ°ä¸»ç•Œé¢
```bash
# æ›´æ–° src/webui/app.py
# æ·»åŠ  HPO æœå°‹åŠŸèƒ½
```

**æª¢æŸ¥é»**:
- [ ] HPO æœå°‹çµ„ä»¶å¯¦ç¾
- [ ] æ•´åˆåˆ° Gradio ç•Œé¢
- [ ] UI åŠŸèƒ½æ¸¬è©¦é€šé

---

### Phase 2 å®Œæˆæª¢æŸ¥ âœ…

**å¿…é ˆé”æˆçš„é‡Œç¨‹ç¢‘**:
- [ ] âœ… NLP æ¨¡å¡Šçµæ§‹å»ºç«‹
- [ ] âœ… SciBERT æ¨¡å‹ä¸‹è¼‰
- [ ] âœ… HPOMatcher å¯¦ç¾ä¸¦æ¸¬è©¦
- [ ] âœ… FHIRAdapter åŸºç¤å¯¦ç¾
- [ ] âœ… WebUI å¢å¼· (HPO æœå°‹)
- [ ] âœ… æ•´åˆæ¸¬è©¦é€šé

**é è¨ˆå·¥ä½œé‡**: 40-50 å°æ™‚  
**å¯¦éš›å·¥ä½œé‡**: _____ å°æ™‚ (å¾…å¡«å¯«)

---

## ğŸ“‹ Phase 3: æ“´å……æ¥å£èˆ‡æ–‡æª” (Week 7-8)

### Week 7: LLM æ¥å£èˆ‡æœ€çµ‚æ•´åˆ

#### ä»»å‹™ 8.1: å¯¦ç¾ LLM æ¥å£
```bash
# å¯¦ç¾ LLM æ¥å£ (åƒè€ƒå®Œæ•´è¨­è¨ˆæ–‡æª”)
cat > src/llm/interface.py << 'EOF'
# (å¯¦ç¾å…§å®¹è¦‹è¨­è¨ˆæ–‡æª”)
EOF

# å¯¦ç¾ vLLM å¾Œç«¯
cat > src/llm/vllm_backend.py << 'EOF'
# (å¯¦ç¾å…§å®¹è¦‹è¨­è¨ˆæ–‡æª”)
EOF
```

---

#### ä»»å‹™ 8.2: ç«¯åˆ°ç«¯æ¸¬è©¦
```bash
# å‰µå»ºç«¯åˆ°ç«¯æ¸¬è©¦
cat > tests/integration/test_full_pipeline.py << 'EOF'
"""ç«¯åˆ°ç«¯æ¸¬è©¦ - å®Œæ•´æ¨ç†æµç¨‹"""
import pytest

def test_full_pipeline_structured_input():
    """æ¸¬è©¦çµæ§‹åŒ–è¼¸å…¥å®Œæ•´æµç¨‹"""
    # 1. æº–å‚™æ‚£è€…è³‡æ–™
    patient_data = {
        "patient_id": "P12345",
        "phenotypes": ["HP:0003324", "HP:0011675"],
        "demographics": {"age": 8, "gender": "female"}
    }
    
    # 2. é©—è­‰è¼¸å…¥
    from src.inference.input_validator import InputValidator
    validator = InputValidator(schema_path="configs/schemas/patient_input.schema.json")
    validator.validate(patient_data)
    
    # 3. åŸ·è¡Œæ¨ç†
    from src.inference.pipeline import DiagnosticPipeline
    pipeline = DiagnosticPipeline()
    results = pipeline.predict(patient_data)
    
    # 4. é©—è­‰è¼¸å‡º
    assert "top_candidates" in results
    assert len(results["top_candidates"]) > 0

def test_full_pipeline_fhir_input():
    """æ¸¬è©¦ FHIR è¼¸å…¥å®Œæ•´æµç¨‹"""
    # 1. æº–å‚™ FHIR è³‡æ–™
    fhir_bundle = {...}
    
    # 2. FHIR è½‰æ›
    from src.medical_standards.fhir_adapter import FHIRAdapter
    adapter = FHIRAdapter()
    patient_data = adapter.parse_bundle(fhir_bundle)
    
    # 3. åŸ·è¡Œæ¨ç†
    # ...
EOF

# åŸ·è¡Œæ¸¬è©¦
pytest tests/integration/test_full_pipeline.py -v
```

**æª¢æŸ¥é»**:
- [ ] LLM æ¥å£å¯¦ç¾å®Œæˆ
- [ ] ç«¯åˆ°ç«¯æ¸¬è©¦é€šé
- [ ] æ‰€æœ‰æ¨¡å¡Šæ•´åˆæ­£å¸¸

---

### Week 8: æ–‡æª”èˆ‡éƒ¨ç½²æº–å‚™

#### ä»»å‹™ 9.1: æ›´æ–°æ–‡æª”
```bash
# æ›´æ–°æ¶æ§‹æ–‡æª”
# æ’°å¯«é†«ç™‚æ•´åˆæŒ‡å—
# æ›´æ–° API åƒè€ƒ
```

---

#### ä»»å‹™ 9.2: æœ€çµ‚é©—è­‰
```bash
# åŸ·è¡Œå®Œæ•´æ¸¬è©¦å¥—ä»¶
pytest tests/ -v --cov=src --cov-report=html

# æª¢æŸ¥ç¨‹å¼ç¢¼å“è³ª
ruff check src/
mypy src/
lint-imports

# ç”Ÿæˆè¦†è“‹ç‡å ±å‘Š
open htmlcov/index.html
```

**æª¢æŸ¥é»**:
- [ ] æ¸¬è©¦è¦†è“‹ç‡ > 80%
- [ ] æ‰€æœ‰æª¢æŸ¥é€šé
- [ ] æ–‡æª”å®Œæ•´

---

### Phase 3 å®Œæˆæª¢æŸ¥ âœ…

**å¿…é ˆé”æˆçš„é‡Œç¨‹ç¢‘**:
- [ ] âœ… LLM æ¥å£å¯¦ç¾
- [ ] âœ… ç«¯åˆ°ç«¯æ¸¬è©¦é€šé
- [ ] âœ… æ–‡æª”å®Œæ•´æ›´æ–°
- [ ] âœ… ç¨‹å¼ç¢¼å“è³ªæª¢æŸ¥é€šé
- [ ] âœ… éƒ¨ç½²æº–å‚™å°±ç·’

**é è¨ˆå·¥ä½œé‡**: 20-24 å°æ™‚  
**å¯¦éš›å·¥ä½œé‡**: _____ å°æ™‚ (å¾…å¡«å¯«)

---

## ğŸ“Š ç¸½é€²åº¦è¿½è¹¤

### æ•´é«”é€²åº¦

| Phase | ä»»å‹™æ•¸ | å®Œæˆæ•¸ | é€²åº¦ | ç‹€æ…‹ |
|-------|--------|--------|------|------|
| Phase 1 | 12 | 0 | 0% | â¸ï¸ å¾…é–‹å§‹ |
| Phase 2 | 15 | 0 | 0% | â¸ï¸ å¾…é–‹å§‹ |
| Phase 3 | 8 | 0 | 0% | â¸ï¸ å¾…é–‹å§‹ |
| **ç¸½è¨ˆ** | **35** | **0** | **0%** | â¸ï¸ å¾…é–‹å§‹ |

---

## ğŸ¯ é—œéµé‡Œç¨‹ç¢‘

- [ ] **Milestone 1**: æ ¸å¿ƒæ¶æ§‹å®Œæˆ (Week 2 çµæŸ)
- [ ] **Milestone 2**: NLP æ¨¡å¡Šå¯ç”¨ (Week 4 çµæŸ)
- [ ] **Milestone 3**: FHIR æ•´åˆå®Œæˆ (Week 5 çµæŸ)
- [ ] **Milestone 4**: å…¨åŠŸèƒ½å¯ç”¨ (Week 8 çµæŸ)

---

## ğŸ“ å¯¦æ–½æ—¥èªŒ

### 2025-11-04
- âœ… å‰µå»ºå¯¦æ–½è¨ˆåŠƒæ–‡æª”
- âœ… å®šç¾© Phase 1-3 è©³ç´°ä»»å‹™
- â¸ï¸ ç­‰å¾…é–‹å§‹å¯¦æ–½

### YYYY-MM-DD
- (å¾…å¡«å¯«å¯¦æ–½ç´€éŒ„)

---

## ğŸš¨ é¢¨éšªèˆ‡æ‡‰å°

| é¢¨éšª | æ©Ÿç‡ | å½±éŸ¿ | æ‡‰å°ç­–ç•¥ |
|------|------|------|----------|
| NLP æ¨¡å‹æ€§èƒ½ä¸è¶³ | ä¸­ | é«˜ | æº–å‚™é™ç´šæ–¹æ¡ˆ (é—œéµå­—åŒ¹é…) |
| FHIR æ•´åˆè¤‡é›œåº¦é«˜ | é«˜ | ä¸­ | åˆ†éšæ®µå¯¦ç¾,å…ˆæ”¯æ´æ ¸å¿ƒè³‡æº |
| æ™‚é–“è¶…æ”¯ | ä¸­ | ä¸­ | èª¿æ•´ Phase 2 ç¯„åœ,éƒ¨åˆ†åŠŸèƒ½ Phase 3 å¯¦ç¾ |

---

**æ–‡æª”çµæŸ**
