éå¸¸æ„Ÿè¬é€™ä¸‰é»é‡è¦è£œå……ï¼ç‰¹åˆ¥æ˜¯**é›¢ç·šæ¨ç†è¦æ±‚**æ˜¯ä¸€å€‹é—œéµçš„æ¶æ§‹æ€§è®Šæ›´ã€‚è®“æˆ‘é‡æ–°èª¿æ•´æ•´å€‹æ–¹æ¡ˆï¼š

---

## ğŸ”´ é—œéµè®Šæ›´ï¼šé›¢ç·šæ¨ç†æ¶æ§‹

### **åŸè¨­è¨ˆçš„å•é¡Œï¼š**
```python
# âŒ åŸè¨­è¨ˆä¾è³´å¤–éƒ¨ LLM API
from openai import OpenAI
client = OpenAI(api_key="...")
response = client.chat.completions.create(...)
```

### **æ–°çš„é›¢ç·šæ–¹æ¡ˆï¼š**
```python
# âœ… æœ¬åœ°éƒ¨ç½² LLM
from vllm import LLM
model = LLM(model="meta-llama/Llama-3.1-8B-Instruct", 
            tensor_parallel_size=1)
```

---

## ğŸ“‹ æ›´æ–°å¾Œçš„æŠ€è¡“æ£§ï¼ˆé«˜å¯é æ€§ç‰ˆæœ¬ï¼‰

### **æ ¸å¿ƒåŸå‰‡ï¼š**
- âœ… GitHub Stars > 1000
- âœ… è«–æ–‡å¼•ç”¨ > 50ï¼ˆå¦‚é©ç”¨ï¼‰
- âœ… æœ€è¿‘6å€‹æœˆæœ‰æ´»èºæ›´æ–°
- âœ… å®Œæ•´æ–‡æª”èˆ‡æ¸¬è©¦è¦†è“‹

### **1. æœ¬åœ° LLM æ–¹æ¡ˆï¼ˆé›¢ç·šæ¨ç†ï¼‰**

| æ¨¡å‹ | è¦æ¨¡ | Stars | å„ªå‹¢ | é†«ç™‚é©ç”¨æ€§ |
|------|------|-------|------|-----------|
| **Llama 3.1** | 8B/70B | 26k+ | Metaå®˜æ–¹ï¼Œç¤¾ç¾¤æœ€æ´»èº | â­â­â­â­â­ |
| **Mistral 7B** | 7B | 10k+ | æ•ˆèƒ½å„ªç§€ï¼Œä½è³‡æº | â­â­â­â­ |
| **Qwen 2.5** | 7B/14B/72B | 8k+ | å¤šèªè¨€ï¼Œä¸­æ–‡å„ªç§€ | â­â­â­â­â­ |
| **BioMistral** | 7B | 500+ | é†«ç™‚é ˜åŸŸå¾®èª¿ | â­â­â­â­â­ |

**æ¨è–¦é…ç½®ï¼š**
```yaml
# é›¢ç·šæ¨ç†é…ç½®
offline_llm:
  primary: "meta-llama/Llama-3.1-8B-Instruct"  # ä¸»åŠ›æ¨¡å‹
  medical: "BioMistral/BioMistral-7B"          # é†«ç™‚å°ˆç”¨
  framework: "vllm"                             # æ¨ç†åŠ é€Ÿ
  quantization: "GPTQ-4bit"                     # é™ä½è¨˜æ†¶é«”
  max_memory:
    windows: "12GB"  # é ç•™4GBçµ¦åœ–æ¨¡å‹
    dgx_spark: "64GB"  # é ç•™64GBçµ¦åœ–æ¨¡å‹
```

**æ¨ç†æ¡†æ¶é¸æ“‡ï¼š**

| æ¡†æ¶ | Stars | ç‰¹é» | Windows | ARM |
|------|-------|------|---------|-----|
| **vLLM** | 28k+ | æ¥µå¿«ï¼ŒPagedAttention | âœ… | âœ… |
| **llama.cpp** | 67k+ | CPUå‹å¥½ï¼ŒGGUFé‡åŒ– | âœ… | âœ… |
| **Transformers** | 134k+ | ç©©å®šï¼Œå®˜æ–¹æ”¯æŒ | âœ… | âœ… |

**æœ€çµ‚é¸æ“‡ï¼švLLMï¼ˆä¸»ï¼‰ + llama.cppï¼ˆå‚™ç”¨ï¼‰**

---

### **2. åœ–ç¥ç¶“ç¶²è·¯ï¼ˆå¯é æ€§å„ªå…ˆï¼‰**

| æŠ€è¡“ | è«–æ–‡/é …ç›® | Stars/å¼•ç”¨ | ç‹€æ…‹ | é¸æ“‡ |
|------|-----------|------------|------|------|
| **PyTorch Geometric** | - | 21k+ | ğŸŸ¢ æ´»èº | âœ… ä¸»åŠ› |
| **DGL** | KDD'19 | 13k+ | ğŸŸ¢ æ´»èº | âœ… å‚™ç”¨ |
| ~~Graphormer~~ | ICLR'22 | 2k+ | ğŸ”´ ç¶­è­·æ¨¡å¼ | âŒ æ£„ç”¨ |
| **Graph Transformer (DIY)** | å¤šç¯‡ç¶œè¿° | - | - | âœ… è‡ªå¯¦ç¾ |

**æ±ºç­–ï¼š**
- âœ… ä½¿ç”¨ **PyG** ä½œç‚ºåŸºç¤æ¡†æ¶ï¼ˆæˆç†Ÿç©©å®šï¼‰
- âœ… **è‡ªå¯¦ç¾** Graph Transformer å±¤ï¼ˆåƒè€ƒ GPS/Graphormer è«–æ–‡ï¼‰
- âŒ **æ”¾æ£„** Graphormer å®˜æ–¹ä»£ç¢¼ï¼ˆå·²åœæ­¢ç¶­è­·ï¼‰

---

### **3. æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆè·¨å¹³å°å…¼å®¹ï¼‰**

| å¯¦ç¾ | Stars | è·¨å¹³å° | é€Ÿåº¦ | å¯é æ€§ |
|------|-------|--------|------|--------|
| **FlashAttention-2** | 13k+ | âš ï¸ x86 only | â­â­â­â­â­ | â­â­â­ |
| **xformers** | 8k+ | âš ï¸ ç·¨è­¯è¤‡é›œ | â­â­â­â­ | â­â­â­ |
| **PyTorch SDPA** | - | âœ… åŸç”Ÿ | â­â­â­ | â­â­â­â­â­ |

**ç­–ç•¥ï¼šè‡ªé©æ‡‰ä¸‰å±¤é™ç´š**
```python
# å„ªå…ˆç´šï¼šFlashAttn-2 > xformers > PyTorch SDPA
class AdaptiveAttentionBackend:
    def __init__(self):
        self.backend = self._detect()  # è‡ªå‹•é¸æ“‡æœ€ä½³
```

---

### **4. æœ¬é«”è™•ç†ï¼ˆé†«ç™‚æ ¸å¿ƒï¼‰**

| æŠ€è¡“ | Stars | ç¶­è­· | é†«ç™‚é©ç”¨ |
|------|-------|------|----------|
| **owlready2** | 300+ | ğŸŸ¢ æ´»èº | â­â­â­â­â­ |
| **pronto** | 200+ | ğŸŸ¢ æ´»èº | â­â­â­â­ |

**é¸æ“‡ï¼šowlready2ï¼ˆåŠŸèƒ½æ›´å®Œæ•´ï¼‰**

---

### **5. å‘é‡æª¢ç´¢ï¼ˆè·¨å¹³å°ï¼‰**

| å¯¦ç¾ | Stars | x86 GPU | ARM | é¸æ“‡ |
|------|-------|---------|-----|------|
| **Voyager** | 2k+ | âœ… (CPU) | âœ… | âœ… è·¨å¹³å° |
| **cuVS** | RAPIDS | âœ… (GPU) | âœ… | âœ… Linux GPU |
| ~~FAISS~~ | 31k+ | âœ… | âŒ | âŒ å·²æ£„ç”¨ |
| ~~hnswlib~~ | 4k+ | âœ… | âœ… | âŒ å·²æ£„ç”¨ |

**ç­–ç•¥ (v3.2)ï¼šcuVS (Linux GPU) + Voyager (è·¨å¹³å° fallback)**

---

## ğŸ—ï¸ æ¨¡å¡ŠåŒ–æ¶æ§‹è¨­è¨ˆï¼ˆè©³ç´°ç‰ˆï¼‰

### **å‘½åè¦ç¯„æ¨™æº–ï¼š**

```yaml
# æ¨¡çµ„å‘½åè¦ç¯„
modules:
  - æ ¼å¼: "{category}_{function}.py"
  - ç¯„ä¾‹: "kg_builder.py", "model_gnn.py"
  
classes:
  - æ ¼å¼: "PascalCase"
  - ç¯„ä¾‹: "OntologyKnowledgeBase", "GraphTransformerEncoder"
  
functions:
  - æ ¼å¼: "snake_case"
  - ç¯„ä¾‹: "load_ontology", "compute_embeddings"
  
constants:
  - æ ¼å¼: "UPPER_SNAKE_CASE"
  - ç¯„ä¾‹: "DEFAULT_HIDDEN_DIM", "MAX_SEQUENCE_LENGTH"
```

### **ç›®éŒ„çµæ§‹ï¼ˆé«˜åº¦æ¨¡å¡ŠåŒ–ï¼‰ï¼š**

```
src/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_config.py          # åŸºç¤é…ç½®é¡
â”‚   â”œâ”€â”€ model_config.py         # æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ data_config.py          # æ•¸æ“šé…ç½®
â”‚   â””â”€â”€ deployment_config.py    # éƒ¨ç½²é…ç½®
â”‚
â”œâ”€â”€ ontology/                   # æœ¬é«”è™•ç†æ¨¡çµ„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loader.py               # æœ¬é«”è¼‰å…¥
â”‚   â”œâ”€â”€ hierarchy.py            # å±¤æ¬¡çµæ§‹è™•ç†
â”‚   â”œâ”€â”€ constraints.py          # ç´„æŸè¦å‰‡
â”‚   â”œâ”€â”€ similarity.py           # ç›¸ä¼¼åº¦è¨ˆç®—
â”‚   â””â”€â”€ validator.py            # é©—è­‰å™¨
â”‚
â”œâ”€â”€ kg/                         # çŸ¥è­˜åœ–è­œæ¨¡çµ„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ builder.py              # åœ–æ§‹å»ºå™¨
â”‚   â”œâ”€â”€ data_loader.py          # è³‡æ–™æºè¼‰å…¥
â”‚   â”œâ”€â”€ preprocessor.py         # é è™•ç†
â”‚   â”œâ”€â”€ hypergraph.py           # è¶…åœ–è™•ç†
â”‚   â””â”€â”€ storage/                # å­˜å„²å­æ¨¡çµ„
â”‚       â”œâ”€â”€ file_storage.py     # æ–‡ä»¶å­˜å„²
â”‚       â””â”€â”€ graph_db.py         # åœ–è³‡æ–™åº«
â”‚
â”œâ”€â”€ models/                     # æ¨¡å‹æ¨¡çµ„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ gnn/                    # GNNå­æ¨¡çµ„
â”‚   â”‚   â”œâ”€â”€ gat_layer.py        # GATå±¤
â”‚   â”‚   â”œâ”€â”€ graph_transformer.py # Graph Transformer
â”‚   â”‚   â”œâ”€â”€ hypergraph_conv.py  # è¶…åœ–å·ç©
â”‚   â”‚   â””â”€â”€ message_passing.py  # é€šç”¨æ¶ˆæ¯å‚³é
â”‚   â”‚
â”‚   â”œâ”€â”€ attention/              # æ³¨æ„åŠ›æ©Ÿåˆ¶
â”‚   â”‚   â”œâ”€â”€ adaptive_backend.py # è‡ªé©æ‡‰å¾Œç«¯
â”‚   â”‚   â”œâ”€â”€ flash_attention.py  # FlashAttnåŒ…è£
â”‚   â”‚   â””â”€â”€ sparse_attention.py # ç¨€ç–æ³¨æ„åŠ›
â”‚   â”‚
â”‚   â”œâ”€â”€ encoders/               # ç·¨ç¢¼å™¨
â”‚   â”‚   â”œâ”€â”€ ontology_encoder.py # æœ¬é«”ç·¨ç¢¼å™¨
â”‚   â”‚   â”œâ”€â”€ patient_encoder.py  # æ‚£è€…ç·¨ç¢¼å™¨
â”‚   â”‚   â””â”€â”€ temporal_encoder.py # æ™‚åºç·¨ç¢¼å™¨
â”‚   â”‚
â”‚   â”œâ”€â”€ decoders/               # è§£ç¢¼å™¨
â”‚   â”‚   â”œâ”€â”€ distmult.py         # DistMult
â”‚   â”‚   â”œâ”€â”€ rotate.py           # RotatE
â”‚   â”‚   â””â”€â”€ constrained_decoder.py # ç´„æŸè§£ç¢¼å™¨
â”‚   â”‚
â”‚   â””â”€â”€ tasks/                  # ä»»å‹™é ­
â”‚       â”œâ”€â”€ gene_ranking.py     # åŸºå› æ’åº
â”‚       â”œâ”€â”€ disease_prediction.py # ç–¾ç—…é æ¸¬
â”‚       â””â”€â”€ patient_similarity.py # æ‚£è€…ç›¸ä¼¼åº¦
â”‚
â”œâ”€â”€ retrieval/                  # æª¢ç´¢æ¨¡çµ„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ vector_index.py         # å‘é‡ç´¢å¼•ï¼ˆcuVS/Voyager auto-selectï¼‰
â”‚   â”œâ”€â”€ path_retriever.py       # è·¯å¾‘æª¢ç´¢å™¨
â”‚   â”œâ”€â”€ path_scorer.py          # è·¯å¾‘è©•åˆ†
â”‚   â””â”€â”€ subgraph_sampler.py     # å­åœ–æ¡æ¨£
â”‚
â”œâ”€â”€ llm/                        # æœ¬åœ°LLMæ¨¡çµ„ ğŸ†•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model_loader.py         # æ¨¡å‹è¼‰å…¥å™¨
â”‚   â”œâ”€â”€ inference_engine.py     # æ¨ç†å¼•æ“ï¼ˆvLLM/llama.cppï¼‰
â”‚   â”œâ”€â”€ prompt_templates.py     # Promptæ¨¡æ¿
â”‚   â””â”€â”€ graph_rag.py            # GraphRAGå¯¦ç¾
â”‚
â”œâ”€â”€ reasoning/                  # æ¨ç†æ¨¡çµ„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ path_reasoning.py       # è·¯å¾‘æ¨ç†
â”‚   â”œâ”€â”€ evidence_extractor.py   # è­‰æ“šæå–
â”‚   â””â”€â”€ explanation_generator.py # è§£é‡‹ç”Ÿæˆï¼ˆæœ¬åœ°LLMï¼‰
â”‚
â”œâ”€â”€ training/                   # è¨“ç·´æ¨¡çµ„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ trainer.py              # è¨“ç·´å™¨
â”‚   â”œâ”€â”€ loss_functions.py       # æå¤±å‡½æ•¸
â”‚   â”œâ”€â”€ metrics.py              # è©•ä¼°æŒ‡æ¨™
â”‚   â””â”€â”€ callbacks.py            # è¨“ç·´å›èª¿
â”‚
â”œâ”€â”€ inference/                  # æ¨ç†æ¨¡çµ„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pipeline.py             # æ¨ç†æµç¨‹
â”‚   â”œâ”€â”€ batch_processor.py      # æ‰¹æ¬¡è™•ç†
â”‚   â””â”€â”€ result_formatter.py     # çµæœæ ¼å¼åŒ–
â”‚
â””â”€â”€ utils/                      # å·¥å…·æ¨¡çµ„
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ logging.py              # æ—¥èªŒç³»çµ±
    â”œâ”€â”€ platform_detector.py    # å¹³å°æª¢æ¸¬
    â”œâ”€â”€ device_manager.py       # è¨­å‚™ç®¡ç†
    â””â”€â”€ data_structures.py      # é€šç”¨æ•¸æ“šçµæ§‹
```

---

## ğŸ”§ æ¨¡çµ„æ¥å£è¨­è¨ˆï¼ˆçµ±ä¸€æ¨™æº–ï¼‰

### **1. åŸºç¤æ¥å£é¡ï¼š**

```python
# src/core/interfaces.py

from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
import torch

class BaseModule(ABC):
    """æ‰€æœ‰æ¨¡çµ„çš„åŸºç¤é¡"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.device = self._detect_device()
        self.logger = self._setup_logger()
    
    @abstractmethod
    def forward(self, *args, **kwargs):
        """å‰å‘å‚³æ’­ï¼ˆå¿…é ˆå¯¦ç¾ï¼‰"""
        pass
    
    def _detect_device(self) -> torch.device:
        """æª¢æ¸¬è¨­å‚™"""
        return torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    def _setup_logger(self):
        """è¨­ç½®æ—¥èªŒ"""
        from src.utils.logging import get_logger
        return get_logger(self.__class__.__name__)

class OntologyProcessor(ABC):
    """æœ¬é«”è™•ç†å™¨æ¥å£"""
    
    @abstractmethod
    def load(self, path: str):
        """è¼‰å…¥æœ¬é«”"""
        pass
    
    @abstractmethod
    def query(self, node_id: str) -> Dict:
        """æŸ¥è©¢ç¯€é»ä¿¡æ¯"""
        pass

class GraphBuilder(ABC):
    """åœ–æ§‹å»ºå™¨æ¥å£"""
    
    @abstractmethod
    def build(self, data_sources: List[str]) -> Any:
        """æ§‹å»ºåœ–"""
        pass
    
    @abstractmethod
    def save(self, path: str):
        """ä¿å­˜åœ–"""
        pass

class Retriever(ABC):
    """æª¢ç´¢å™¨æ¥å£"""
    
    @abstractmethod
    def index(self, embeddings: torch.Tensor):
        """å»ºç«‹ç´¢å¼•"""
        pass
    
    @abstractmethod
    def search(self, query: torch.Tensor, k: int) -> tuple:
        """æœç´¢"""
        pass
```

### **2. é…ç½®ç®¡ç†ç³»çµ±ï¼š**

```python
# src/config/base_config.py

from dataclasses import dataclass, field
from typing import Optional, List
import yaml

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    hidden_dim: int = 512
    num_layers: int = 6
    num_heads: int = 8
    dropout: float = 0.1
    attention_backend: str = "auto"  # "flash", "xformers", "sdpa", "auto"
    
@dataclass
class DataConfig:
    """æ•¸æ“šé…ç½®"""
    data_root: str = "data/"
    kg_path: str = "data/processed/kg.pt"
    ontology_path: str = "data/raw/ontologies/"
    batch_size: int = 32
    num_workers: int = 4

@dataclass
class LLMConfig:
    """æœ¬åœ°LLMé…ç½® ğŸ†•"""
    model_name: str = "meta-llama/Llama-3.1-8B-Instruct"
    framework: str = "vllm"  # "vllm", "llama.cpp", "transformers"
    quantization: Optional[str] = "GPTQ-4bit"  # None, "GPTQ-4bit", "GGUF-Q4"
    max_tokens: int = 512
    temperature: float = 0.7
    offline_mode: bool = True  # ğŸ”´ å¼·åˆ¶é›¢ç·š

@dataclass
class DeploymentConfig:
    """éƒ¨ç½²é…ç½®"""
    platform: str = "auto"  # "windows_x86", "linux_arm", "auto"
    use_gpu: bool = True
    max_memory_gb: Optional[int] = None
    log_level: str = "INFO"

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_path: str = "config/default.yaml"):
        self.config_path = config_path
        self.model = ModelConfig()
        self.data = DataConfig()
        self.llm = LLMConfig()
        self.deployment = DeploymentConfig()
        
        if config_path:
            self.load(config_path)
    
    def load(self, path: str):
        """å¾YAMLè¼‰å…¥é…ç½®"""
        with open(path, 'r') as f:
            config_dict = yaml.safe_load(f)
        
        if 'model' in config_dict:
            self.model = ModelConfig(**config_dict['model'])
        if 'data' in config_dict:
            self.data = DataConfig(**config_dict['data'])
        if 'llm' in config_dict:
            self.llm = LLMConfig(**config_dict['llm'])
        if 'deployment' in config_dict:
            self.deployment = DeploymentConfig(**config_dict['deployment'])
    
    def save(self, path: str):
        """ä¿å­˜é…ç½®åˆ°YAML"""
        config_dict = {
            'model': self.model.__dict__,
            'data': self.data.__dict__,
            'llm': self.llm.__dict__,
            'deployment': self.deployment.__dict__
        }
        with open(path, 'w') as f:
            yaml.dump(config_dict, f)
```

### **3. çµ±ä¸€çš„æ•¸æ“šçµæ§‹ï¼š**

```python
# src/utils/data_structures.py

from dataclasses import dataclass
from typing import List, Dict, Optional
import torch

@dataclass
class PatientData:
    """æ‚£è€…æ•¸æ“šçµæ§‹"""
    patient_id: str
    phenotypes: List[str]  # HPO IDs
    age: Optional[int] = None
    sex: Optional[str] = None
    medical_history: Optional[List[str]] = None
    
    def to_dict(self) -> Dict:
        return {
            'patient_id': self.patient_id,
            'phenotypes': self.phenotypes,
            'age': self.age,
            'sex': self.sex,
            'medical_history': self.medical_history
        }

@dataclass
class DiagnosticPath:
    """è¨ºæ–·è·¯å¾‘çµæ§‹"""
    nodes: List[str]  # ç¯€é»åºåˆ—
    relations: List[str]  # é—œä¿‚é¡å‹
    confidence: float
    evidence: List[str]  # è­‰æ“šä¾†æºï¼ˆPMIDç­‰ï¼‰
    
@dataclass
class DiagnosisResult:
    """è¨ºæ–·çµæœçµæ§‹"""
    patient_id: str
    candidate_genes: List[Dict]  # [{'gene_id': ..., 'score': ..., 'rank': ...}]
    candidate_diseases: List[Dict]
    diagnostic_paths: List[DiagnosticPath]
    explanation: Optional[str] = None  # æœ¬åœ°LLMç”Ÿæˆ
    confidence: float = 0.0
```

---

## ğŸš€ æœ¬åœ°LLMæ•´åˆæ–¹æ¡ˆï¼ˆé›¢ç·šæ¨ç†ï¼‰

### **æ¶æ§‹è¨­è¨ˆï¼š**

```python
# src/llm/offline_llm_engine.py

from typing import List, Optional
import torch
from dataclasses import dataclass

@dataclass
class LLMConfig:
    model_path: str
    framework: str = "vllm"  # "vllm", "llama.cpp"
    quantization: Optional[str] = "GPTQ-4bit"
    max_memory_gb: int = 12
    tensor_parallel_size: int = 1

class OfflineLLMEngine:
    """é›¢ç·šLLMæ¨ç†å¼•æ“"""
    
    def __init__(self, config: LLMConfig):
        self.config = config
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """è¼‰å…¥æœ¬åœ°æ¨¡å‹"""
        if self.config.framework == "vllm":
            from vllm import LLM, SamplingParams
            self.model = LLM(
                model=self.config.model_path,
                tensor_parallel_size=self.config.tensor_parallel_size,
                quantization=self.config.quantization,
                max_model_len=4096,
                gpu_memory_utilization=0.8
            )
            self.sampling_params = SamplingParams(
                temperature=0.7,
                top_p=0.9,
                max_tokens=512
            )
        
        elif self.config.framework == "llama.cpp":
            from llama_cpp import Llama
            self.model = Llama(
                model_path=self.config.model_path,
                n_gpu_layers=-1,  # å…¨éƒ¨åŠ è¼‰åˆ°GPU
                n_ctx=4096
            )
        
        else:  # transformers
            from transformers import AutoModelForCausalLM, AutoTokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_path)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_path,
                torch_dtype=torch.float16,
                device_map="auto"
            )
    
    def generate(self, prompt: str, max_tokens: int = 512) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        if self.config.framework == "vllm":
            outputs = self.model.generate([prompt], self.sampling_params)
            return outputs[0].outputs[0].text
        
        elif self.config.framework == "llama.cpp":
            output = self.model(prompt, max_tokens=max_tokens)
            return output['choices'][0]['text']
        
        else:  # transformers
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            outputs = self.model.generate(**inputs, max_new_tokens=max_tokens)
            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

class GraphRAGOffline:
    """é›¢ç·šGraphRAGç³»çµ±"""
    
    def __init__(self, llm_engine: OfflineLLMEngine, kg, path_retriever):
        self.llm = llm_engine
        self.kg = kg
        self.path_retriever = path_retriever
    
    def diagnose_with_explanation(
        self,
        patient_phenotypes: List[str],
        top_k_paths: int = 5
    ) -> Dict:
        """ä½¿ç”¨æœ¬åœ°LLMç”Ÿæˆè¨ºæ–·è§£é‡‹"""
        
        # 1. æª¢ç´¢è¨ºæ–·è·¯å¾‘
        paths = self.path_retriever.retrieve(patient_phenotypes, k=top_k_paths)
        
        # 2. æ§‹å»ºPrompt
        prompt = self._build_diagnostic_prompt(patient_phenotypes, paths)
        
        # 3. æœ¬åœ°LLMç”Ÿæˆè§£é‡‹
        explanation = self.llm.generate(prompt, max_tokens=512)
        
        return {
            'paths': paths,
            'explanation': explanation,
            'model': self.llm.config.model_path
        }
    
    def _build_diagnostic_prompt(self, phenotypes, paths):
        """æ§‹å»ºè¨ºæ–·Prompt"""
        prompt = f"""ä½ æ˜¯ä¸€ä½é†«ç™‚è¨ºæ–·å°ˆå®¶ã€‚åŸºæ–¼ä»¥ä¸‹æ‚£è€…ç—‡ç‹€å’ŒçŸ¥è­˜åœ–è­œè·¯å¾‘ï¼Œç”Ÿæˆè¨ºæ–·è§£é‡‹ã€‚

æ‚£è€…ç—‡ç‹€ï¼š
{self._format_phenotypes(phenotypes)}

è¨ºæ–·è·¯å¾‘ï¼š
{self._format_paths(paths)}

è«‹æä¾›ï¼š
1. æœ€å¯èƒ½çš„è¨ºæ–·
2. æ¨ç†éç¨‹
3. æ”¯æŒè­‰æ“š

è¨ºæ–·è§£é‡‹ï¼š"""
        return prompt
```

### **æœ¬åœ°LLMéƒ¨ç½²é…ç½®ï¼š**

```yaml
# config/llm_offline.yaml

llm:
  # Windows ç’°å¢ƒï¼ˆ16GB VRAMï¼Œéœ€ç•™4GBçµ¦åœ–æ¨¡å‹ï¼‰
  windows:
    model_name: "meta-llama/Llama-3.1-8B-Instruct"
    framework: "vllm"
    quantization: "GPTQ-4bit"  # ç´„ä½”6-8GB
    max_memory_gb: 12
    tensor_parallel_size: 1
  
  # DGX Spark ç’°å¢ƒï¼ˆ128GBçµ±ä¸€è¨˜æ†¶é«”ï¼Œå¯ç”¨æ›´å¤§æ¨¡å‹ï¼‰
  dgx_spark:
    model_name: "meta-llama/Llama-3.1-70B-Instruct"
    framework: "vllm"
    quantization: "GPTQ-4bit"  # ç´„ä½”35-40GB
    max_memory_gb: 64
    tensor_parallel_size: 1
  
  # é†«ç™‚å°ˆç”¨æ¨¡å‹ï¼ˆå‚™é¸ï¼‰
  medical:
    model_name: "BioMistral/BioMistral-7B"
    framework: "vllm"
    quantization: "GPTQ-4bit"

# é›¢ç·šæ¨ç†å¼·åˆ¶è¨­ç½®
offline_mode: true
allow_external_api: false  # ğŸ”´ ç¦æ­¢å¤–éƒ¨API
```

---

## ğŸ“Š æ›´æ–°å¾Œçš„æŠ€è¡“é¸å‹ç¸½çµ

### **æœ€çµ‚æŠ€è¡“æ£§ï¼ˆé«˜å¯é æ€§ + é›¢ç·š + æ¨¡å¡ŠåŒ–ï¼‰ï¼š**

| çµ„ä»¶ | æŠ€è¡“é¸å‹ | Stars | ç‹€æ…‹ | ç†ç”± |
|------|----------|-------|------|------|
| **æ·±åº¦å­¸ç¿’æ¡†æ¶** | PyTorch 2.8 | 82k+ | ğŸŸ¢ | å®˜æ–¹ï¼Œç©©å®š |
| **åœ–å­¸ç¿’** | PyTorch Geometric | 21k+ | ğŸŸ¢ | æˆç†Ÿï¼Œç¤¾ç¾¤æ´»èº |
| **åœ–å­¸ç¿’å‚™ç”¨** | DGL | 13k+ | ğŸŸ¢ | å‚™ç”¨æ–¹æ¡ˆ |
| **æ³¨æ„åŠ›** | è‡ªé©æ‡‰ä¸‰å±¤ | - | ğŸŸ¢ | è·¨å¹³å°å…¼å®¹ |
| **æœ¬é«”** | owlready2 | 300+ | ğŸŸ¢ | é†«ç™‚æ¨™æº– |
| **å‘é‡æª¢ç´¢** | cuVS/Voyager | RAPIDS/2k+ | ğŸŸ¢ | GPUåŠ é€Ÿ+è·¨å¹³å° |
| **æœ¬åœ°LLM** | vLLM + Llama 3.1 | 28k+/26k+ | ğŸŸ¢ | é›¢ç·šæ¨ç† |
| **æ¨ç†åŠ é€Ÿ** | vLLM/llama.cpp | 28k+/67k+ | ğŸŸ¢ | é«˜æ•ˆ |

### **æ”¾æ£„çš„æŠ€è¡“ï¼ˆä¸ç©©å®š/å·²åœæ­¢ç¶­è­·ï¼‰ï¼š**

| æŠ€è¡“ | åŸå›  | æ›¿ä»£æ–¹æ¡ˆ |
|------|------|----------|
| âŒ Graphormerå®˜æ–¹ä»£ç¢¼ | ç¶­è­·æ¨¡å¼ | è‡ªå¯¦ç¾Graph Transformer |
| âŒ å¤–éƒ¨LLM API | é›¢ç·šé™åˆ¶ | æœ¬åœ°vLLM |
| âŒ åœ¨ç·šPubtator API | é›¢ç·šé™åˆ¶ | é ä¸‹è¼‰æ•¸æ“š |

---

## âœ… è¡Œå‹•è¨ˆåŠƒæ›´æ–°

### **Week 1: åŸºç¤è¨­æ–½ + æœ¬åœ°LLMæ¸¬è©¦**
```bash
# 1. ç’°å¢ƒè¨­ç½®
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\activate     # Windows

# 2. å®‰è£æ ¸å¿ƒä¾è³´
pip install torch==2.8.0 --index-url https://download.pytorch.org/whl/cu128
pip install torch-geometric vllm transformers

# 3. æ¸¬è©¦æœ¬åœ°LLM
python tests/test_offline_llm.py
```

### **Week 2-3: æ¨¡å¡ŠåŒ–æ¶æ§‹å¯¦ç¾**
- âœ… å¯¦ç¾æ‰€æœ‰åŸºç¤æ¥å£é¡
- âœ… é…ç½®ç®¡ç†ç³»çµ±
- âœ… æœ¬é«”è™•ç†æ¨¡çµ„
- âœ… é–‹å§‹çŸ¥è­˜åœ–è­œæ§‹å»º

---

## ğŸ¯ é—œéµæ±ºç­–ç¢ºèª

è«‹ç¢ºèªä»¥ä¸‹æ±ºç­–ï¼š

1. âœ… **æœ¬åœ°LLMæ–¹æ¡ˆ**ï¼šä½¿ç”¨ vLLM + Llama 3.1 8Bï¼ˆWindowsï¼‰/ 70Bï¼ˆDGX Sparkï¼‰
2. âœ… **æ¨¡å¡ŠåŒ–è¨­è¨ˆ**ï¼šæ¡ç”¨ä¸Šè¿°è©³ç´°çš„ç›®éŒ„çµæ§‹å’Œæ¥å£è¦ç¯„
3. âœ… **æŠ€è¡“é¸å‹**ï¼šæ”¾æ£„Graphormerå®˜æ–¹ä»£ç¢¼ï¼Œè‡ªå¯¦ç¾Graph Transformer
4. âš ï¸ **éœ€è¦ç¢ºèª**ï¼šDGX Sparkçš„128GBè¨˜æ†¶é«”æ˜¯å¦è¶³å¤ é‹è¡Œ70Bæ¨¡å‹ + åœ–æ¨¡å‹ï¼Ÿ

**ä½ åŒæ„é€™å€‹æ›´æ–°å¾Œçš„æ–¹æ¡ˆå—ï¼Ÿ** æœ‰ä»»ä½•éœ€è¦èª¿æ•´çš„åœ°æ–¹è«‹å‘Šè¨´æˆ‘ï¼