# é†«ç™‚çŸ¥è­˜åœ–è­œè¨ºæ–·æ¨ç†å¼•æ“ - å·¥ç¨‹è—åœ– v2.0ï¼ˆå‡ç´šç‰ˆï¼‰

**ç‰ˆæœ¬**: v2.0 - æ•´åˆ2024-2025å‰æ²¿æŠ€è¡“  
**æœ€å¾Œæ›´æ–°**: 2025-10-07  
**é‡å¤§è®Šæ›´**: âœ… åˆ†å±¤æœ¬é«”æ¶æ§‹ | âœ… çŸ¥è­˜è¶…åœ– | âœ… è·¨å¹³å°å…¼å®¹

---

## å°ˆæ¡ˆæ¦‚è¿°

### æ ¸å¿ƒç›®æ¨™
æ§‹å»º**è‡¨åºŠç´šç²¾æº–åº¦**çš„é†«ç™‚çŸ¥è­˜åœ–è­œèˆ‡ç½•è¦‹ç–¾ç—…è¨ºæ–·æ¨ç†å¼•æ“ï¼Œæ•´åˆæœ€æ–°çš„åˆ†å±¤æœ¬é«”æ„ŸçŸ¥æŠ€è¡“ã€çŸ¥è­˜è¶…åœ–å’Œè·¯å¾‘æ¨ç†ï¼Œå¯¦ç¾ï¼š
- **ç²¾æº–åº¦**: Hits@10 â‰¥ 80% (vs SHEPHERD 60%)
- **å¯é æ€§**: å¹»è¦ºç‡ < 5% (vs å‚³çµ±20%)
- **å¯è§£é‡‹æ€§**: å®Œæ•´çš„æ¨ç†è·¯å¾‘èˆ‡è­‰æ“šéˆ
- **è·¨å¹³å°**: Windows x86 + ARM (DGX Spark) çµ±ä¸€æ”¯æŒ

### æŠ€è¡“æ£§å‡ç´š

#### æ ¸å¿ƒæ¡†æ¶
- **PyTorch**: 2.8+ (ARM + CUDA 12.8 åŸç”Ÿæ”¯æŒ)
- **PyTorch Geometric**: 2.6+ (ç•°è³ªåœ–è™•ç†)
- **æ³¨æ„åŠ›åŠ é€Ÿ**: FlashAttention-2 (x86) / xformers (ARM) / PyTorch SDPA (é€šç”¨)

#### å‰µæ–°æŠ€è¡“ï¼ˆ2024-2025ï¼‰
- **åˆ†å±¤æœ¬é«”æ„ŸçŸ¥ GNN**: æ•´åˆHPO/MONDO/GOå±¤æ¬¡çµæ§‹
- **çŸ¥è­˜è¶…åœ–**: è™•ç†å¤šå…ƒé«˜éšé—œä¿‚ï¼ˆâ‰¥3å¯¦é«”ï¼‰
- **DR.KNOWSè·¯å¾‘æ¨ç†**: çŸ¥è­˜åœ–è­œè·¯å¾‘æª¢ç´¢ + LLMæ¨ç†
- **Neural ODE**: é€£çºŒæ™‚é–“æ‚£è€…ç‹€æ…‹å»ºæ¨¡
- **æœ¬é«”ç´„æŸæ¨ç†**: é˜²æ­¢ä¸åˆç†é æ¸¬

#### æª¢ç´¢èˆ‡ç”Ÿæˆ
- **å‘é‡ç´¢å¼•**: cuVS (Linux GPU) / Voyager (è·¨å¹³å° CPU)
- **GraphRAG**: çŸ¥è­˜åœ–è­œå¢å¼·æª¢ç´¢ç”Ÿæˆ
- **LLMæ•´åˆ**: GPT-4/Claude (å¯é¸) ç”¨æ–¼è­‰æ“šè§£é‡‹

### é‹è¡Œç’°å¢ƒ

#### ç’°å¢ƒä¸€ï¼šé–‹ç™¼è¨“ç·´ç’°å¢ƒ
```yaml
ç¡¬é«”:
  OS: Windows 11 (æ”¯æŒWSL2)
  CPU: x86-64
  GPU: NVIDIA Blackwell @ 16GB VRAM
  RAM: 32GB+ (å»ºè­°64GB)
  å„²å­˜: 1TB NVMe SSD

è»Ÿé«”:
  Python: 3.12
  CUDA: 12.8
  PyTorch: 2.8.0+cu128
  æ³¨æ„åŠ›: FlashAttention-2 âœ…
```

#### ç’°å¢ƒäºŒï¼šé‚Šç·£æ¨ç†ç’°å¢ƒ
```yaml
ç¡¬é«”:
  ç³»çµ±: NVIDIA DGX Spark (GB10 SoC)
  CPU: 20æ ¸ ARM v9.2
    - 10x Cortex-X925 (é«˜æ€§èƒ½)
    - 10x Cortex-A725 (æ•ˆèƒ½)
  GPU: Blackwell (é›†æˆ)
    - 6144 CUDAæ ¸å¿ƒ
    - 1 PFLOPS @ FP4
  è¨˜æ†¶é«”: 128GB LPDDR5X-9400 (çµ±ä¸€)
  ç¶²çµ¡: 2x 200GbE ConnectX-7
  åŠŸè€—: 140W

è»Ÿé«”:
  OS: NVIDIA DGX OS (Ubuntu-based)
  Python: 3.12
  CUDA: 12.8
  PyTorch: 2.8.0+cu128 (ARM build)
  æ³¨æ„åŠ›: PyTorch SDPA / xformers (é™ç´š)
```

**é—œéµå·®ç•°ï¼š**
- âœ… DGX Spark: 128GBçµ±ä¸€è¨˜æ†¶é«”ï¼ˆå„ªå‹¢ï¼‰
- âš ï¸ DGX Spark: éƒ¨åˆ†å¥—ä»¶éœ€ç‰¹æ®Šè™•ç†ï¼ˆæŒ‘æˆ°ï¼‰
- âš ï¸ Windows: 16GB VRAMé™åˆ¶ï¼ˆéœ€å­åœ–æ¡æ¨£ï¼‰

---

## ç³»çµ±æ¶æ§‹ï¼ˆ5+1å±¤è¨­è¨ˆï¼‰

### ç¬¬0å±¤ï¼šæœ¬é«”çŸ¥è­˜å±¤ ğŸ§¬ **ã€æ–°å¢ã€‘**

**æ ¸å¿ƒå‰µæ–°ï¼šå°‡Ontologyå¾é…è§’æå‡ç‚ºæ ¸å¿ƒ**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åˆ†å±¤é†«ç™‚æœ¬é«”çŸ¥è­˜åº«                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  - ç–¾ç—…æœ¬é«”: MONDO + Orphanet (IS-Aå±¤æ¬¡) â”‚
â”‚  - è¡¨å‹æœ¬é«”: HPO (ç—‡ç‹€åˆ†é¡æ¨¹)             â”‚
â”‚  - åŠŸèƒ½æœ¬é«”: GO + Reactome (ç”Ÿç‰©é€šè·¯)     â”‚
â”‚  - ç–¾ç—…å…±ç¾åœ–: å¾EHRçµ±è¨ˆæ§‹å»º              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æœ¬é«”æ¨ç†å¼•æ“:                            â”‚
â”‚  - å±¤æ¬¡ç´„æŸ (çˆ¶å­é—œä¿‚)                    â”‚
â”‚  - äº’æ–¥è¦å‰‡ (ä¸å¯å…±å­˜ç—‡ç‹€)                â”‚
â”‚  - è˜Šå«è¦å‰‡ (å¿…ç„¶ä¼´éš¨ç—‡ç‹€)                â”‚
â”‚  - ç›¸ä¼¼åº¦è¨ˆç®— (æœ¬é«”è·é›¢)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å¯¦ç¾ï¼š**
```python
class OntologyKnowledgeBase:
    """
    çµ±ä¸€çš„æœ¬é«”çŸ¥è­˜ç®¡ç†
    """
    def __init__(self):
        # è¼‰å…¥æœ¬é«”
        self.hpo = load_ontology('HPO')      # 130,000+ è¡¨å‹
        self.mondo = load_ontology('MONDO')  # 60,000+ ç–¾ç—…
        self.go = load_ontology('GO')        # 44,000+ åŠŸèƒ½
        
        # æ§‹å»ºå±¤æ¬¡ç´¢å¼•
        self.hierarchy_index = HierarchyIndex([self.hpo, self.mondo, self.go])
        
        # ç–¾ç—…å…±ç¾åœ–
        self.cooccurrence = DiseaseCooccurrenceGraph.from_ehr(
            sources=['MIMIC-III', 'UK-Biobank', 'ChinaMap'],
            min_support=10
        )
        
        # ç´„æŸè¦å‰‡
        self.constraints = OntologyConstraints(
            mutex_rules=self.extract_mutex_rules(),
            implication_rules=self.extract_implication_rules()
        )
    
    def validate_phenotype_set(self, phenotypes):
        """é©—è­‰ç—‡ç‹€é›†åˆçš„æœ¬é«”ä¸€è‡´æ€§"""
        return self.constraints.check(phenotypes)
```

### ç¬¬1å±¤ï¼šè³‡æ–™å±¤ (Data Layer)

#### 1.1 å¤šæºç•°è³ªè³‡æ–™æ•´åˆ

**è³‡æ–™ä¾†æºï¼ˆå„ªå…ˆç´šæ’åºï¼‰ï¼š**

| å„ªå…ˆç´š | è³‡æ–™æº | æ›´æ–°é »ç‡ | ç”¨é€” | è¦æ¨¡ |
|--------|--------|----------|------|------|
| ğŸ”´ P0 | **HPO** | å­£åº¦ | è¡¨å‹æœ¬é«” | 130K+ terms |
| ğŸ”´ P0 | **MONDO** | å­£åº¦ | ç–¾ç—…æœ¬é«” | 60K+ diseases |
| ğŸ”´ P0 | **OMIM** | æœˆåº¦ | åŸºå› -ç–¾ç—… | 26K+ entries |
| ğŸ”´ P0 | **DisGeNET** | å­£åº¦ | åŸºå› -ç–¾ç—…é—œè¯ | 1.1M+ associations |
| ğŸŸ  P1 | **Orphanet** | å¹´åº¦ | ç½•è¦‹ç–¾ç—… | 6K+ rare diseases |
| ğŸŸ  P1 | **ClinVar** | é€±åº¦ | è®Šç•°-ç–¾ç—… | 2M+ variants |
| ğŸŸ  P1 | **GO** | æœˆåº¦ | åŸºå› åŠŸèƒ½ | 44K+ terms |
| ğŸŸ  P1 | **Reactome** | å­£åº¦ | ç”Ÿç‰©é€šè·¯ | 2.5K+ pathways |
| ğŸŸ¡ P2 | **Pubtator 3.0** | å³æ™‚ | æ–‡ç»è­‰æ“š | 30M+ relations |
| ğŸŸ¡ P2 | **NCBI Gene** | é€±åº¦ | åŸºå› è³‡è¨Š | 60K+ genes |

#### 1.2 çŸ¥è­˜åœ–è­œæ§‹å»ºï¼ˆç•°è³ª+è¶…åœ–ï¼‰

**å‚³çµ±ç•°è³ªåœ–ï¼š**
```python
HeteroData = {
    'gene': {nodes: 20,000},
    'disease': {nodes: 15,000},
    'phenotype': {nodes: 50,000},
    'pathway': {nodes: 2,500},
    
    ('gene', 'associated_with', 'disease'): {edges: 120,000},
    ('phenotype', 'indicates', 'disease'): {edges: 200,000},
    ('gene', 'in_pathway', 'pathway'): {edges: 80,000}
}
```

**æ–°å¢ï¼šçŸ¥è­˜è¶…åœ–** ğŸ†•
```python
class MedicalKnowledgeHypergraph:
    """
    è™•ç†é«˜éšé—œä¿‚ï¼ˆ>2å€‹å¯¦é«”ï¼‰
    """
    def __init__(self):
        self.hyperedges = {
            # è¶…é‚ŠID: {ç¯€é»é›†åˆ, é—œä¿‚é¡å‹, ç½®ä¿¡åº¦, è­‰æ“š}
            'he_001': {
                'nodes': ['BRCA1', 'BRCA2', 'family_history'],
                'relation': 'breast_cancer_risk',
                'confidence': 0.92,
                'evidence': ['PMID:12345', 'ClinVar:678']
            },
            'he_002': {
                'nodes': ['obesity', 'hypertension', 'diabetes'],
                'relation': 'metabolic_syndrome',
                'confidence': 0.88,
                'evidence': ['PMID:98765']
            }
        }
    
    def detect_hyperedges_from_ehr(self, ehr_data):
        """
        å¾é›»å­ç—…æ­·è‡ªå‹•ç™¼ç¾é«˜éšé—œä¿‚
        ä½¿ç”¨é »ç¹æ¨¡å¼æŒ–æ˜
        """
        # Aprioriç®—æ³•å°‹æ‰¾é »ç¹ç—‡ç‹€çµ„åˆ
        frequent_sets = apriori(
            ehr_data,
            min_support=0.01,
            max_len=5  # æœ€å¤š5å€‹å¯¦é«”
        )
        
        # è½‰æ›ç‚ºè¶…é‚Š
        for item_set in frequent_sets:
            if len(item_set) >= 3:  # è‡³å°‘3å€‹å¯¦é«”
                self.add_hyperedge(
                    nodes=list(item_set),
                    confidence=self.compute_confidence(item_set)
                )
```

#### 1.3 çŸ¥è­˜åœ–è­œå­˜å„²ï¼ˆæ··åˆç­–ç•¥ï¼‰

```python
class HybridKGStorage:
    """
    æ··åˆå­˜å„²ç­–ç•¥ï¼šæ–‡ä»¶ + åœ–è³‡æ–™åº« + å‘é‡ç´¢å¼•
    """
    def __init__(self):
        # 1. è¨“ç·´ç”¨ï¼šPyG Dataå°è±¡ï¼ˆå¿«é€Ÿè¼‰å…¥ï¼‰
        self.pyg_data = HeteroData()
        
        # 2. æ¨ç†ç”¨ï¼šNeo4jï¼ˆè¤‡é›œæŸ¥è©¢ï¼‰
        self.graph_db = Neo4jInterface(
            uri="bolt://localhost:7687"
        )
        
        # 3. æª¢ç´¢ç”¨ï¼šå‘é‡ç´¢å¼• (auto-select: cuVS/Voyager)
        from src.retrieval import create_index
        self.vector_index = create_index(
            backend='auto',
            dim=512
        )
    
    def query_subgraph(self, patient_phenotypes, k_hop=2):
        """
        ç‚ºæ‚£è€…æª¢ç´¢ç›¸é—œå­åœ–
        """
        # 1. å‘é‡ç›¸ä¼¼åº¦å¿«é€Ÿç¯©é¸
        similar_nodes = self.vector_index.search(
            patient_phenotypes,
            k=100
        )
        
        # 2. åœ–è³‡æ–™åº«æ“´å±•é„°å±…
        subgraph = self.graph_db.expand_neighbors(
            seed_nodes=similar_nodes,
            max_hops=k_hop,
            max_nodes=10000
        )
        
        # 3. è½‰æ›ç‚ºPyGæ ¼å¼
        return self.to_pyg_data(subgraph)
```

### ç¬¬2å±¤ï¼šæ¨¡å‹å±¤ (Model Layer) - é‡å¤§å‡ç´š ğŸš€

#### 2.1 åˆ†å±¤æœ¬é«”æ„ŸçŸ¥åœ–ç·¨ç¢¼å™¨ **ã€æ ¸å¿ƒå‰µæ–°ã€‘**

**æ¶æ§‹è¨­è¨ˆï¼š**

```
è¼¸å…¥ç¯€é»
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç¬¬1éšæ®µï¼šæœ¬é«”å±¤æ¬¡ç·¨ç¢¼               â”‚
â”‚ - ç¥–å…ˆç¯€é»åµŒå…¥                      â”‚
â”‚ - å…„å¼Ÿç¯€é»åµŒå…¥                      â”‚
â”‚ - å±¤æ¬¡ä½ç½®ç·¨ç¢¼                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç¬¬2éšæ®µï¼šç–¾ç—…å…±ç¾å¢å¼·               â”‚
â”‚ - çµ±è¨ˆå…±ç¾æ¬Šé‡                      â”‚
â”‚ - è‡¨åºŠèªç¾©ç›¸ä¼¼åº¦                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç¬¬3éšæ®µï¼šåœ–çµæ§‹å­¸ç¿’                 â”‚
â”‚ - GraphGPS / Graph Transformer      â”‚
â”‚ - æœ¬é«”å¼•å°æ³¨æ„åŠ›                    â”‚
â”‚ - è¶…åœ–å·ç©ï¼ˆè™•ç†é«˜éšé—œä¿‚ï¼‰          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
å¢å¼·ç¯€é»åµŒå…¥
```

**å¯¦ç¾ï¼š**
```python
class HierarchicalOntologyAwareGNN(nn.Module):
    """
    æ•´åˆæœ¬é«”çŸ¥è­˜çš„åœ–ç¥ç¶“ç¶²è·¯
    """
    def __init__(self, ontology_kb, hidden_dim=512):
        super().__init__()
        self.ontology_kb = ontology_kb
        
        # éšæ®µ1ï¼šæœ¬é«”ç·¨ç¢¼å™¨
        self.ontology_encoder = OntologyHierarchyEncoder(
            ontologies=[ontology_kb.hpo, ontology_kb.mondo, ontology_kb.go],
            embedding_dim=hidden_dim,
            encode_ancestors=True,
            encode_siblings=True,
            max_depth=10
        )
        
        # éšæ®µ2ï¼šå…±ç¾å¢å¼·
        self.cooccurrence_layer = CooccurrenceAggregation(
            cooccurrence_graph=ontology_kb.cooccurrence,
            hidden_dim=hidden_dim
        )
        
        # éšæ®µ3ï¼šåœ–çµæ§‹å­¸ç¿’
        self.gnn_layers = nn.ModuleList([
            OntologyGuidedGraphGPS(
                hidden_dim=hidden_dim,
                num_heads=8,
                ontology_tree=ontology_kb.hierarchy_index
            )
            for _ in range(6)
        ])
        
        # è¶…åœ–å·ç©ï¼ˆè™•ç†é«˜éšé—œä¿‚ï¼‰
        self.hypergraph_conv = HypergraphConv(hidden_dim)
    
    def forward(self, hetero_data, hypergraph_data):
        # éšæ®µ1ï¼šæœ¬é«”ç·¨ç¢¼
        x = self.ontology_encoder(hetero_data.x, hetero_data.node_type)
        
        # éšæ®µ2ï¼šå…±ç¾å¢å¼·
        x = self.cooccurrence_layer(x, hetero_data.node_id)
        
        # éšæ®µ3ï¼šåœ–å­¸ç¿’
        for gnn in self.gnn_layers:
            x = gnn(x, hetero_data.edge_index, hetero_data.edge_type)
            x = F.relu(x)
            x = F.dropout(x, p=0.1, training=self.training)
        
        # è¶…åœ–å·ç©
        if hypergraph_data is not None:
            x = self.hypergraph_conv(x, hypergraph_data.hyperedges)
        
        return x
```

**é—œéµçµ„ä»¶ï¼šæœ¬é«”å¼•å°æ³¨æ„åŠ›**
```python
class OntologyGuidedGraphGPS(nn.Module):
    """
    GraphGPS + æœ¬é«”å¼•å°æ³¨æ„åŠ›
    """
    def __init__(self, hidden_dim, num_heads, ontology_tree):
        super().__init__()
        self.ontology_tree = ontology_tree
        
        # æœ¬åœ°æ¶ˆæ¯å‚³éï¼ˆMPNNï¼‰
        self.local_mpnn = GATConv(hidden_dim, hidden_dim, heads=num_heads)
        
        # å…¨å±€æ³¨æ„åŠ›ï¼ˆTransformerï¼‰
        self.global_attn = OntologyGuidedAttention(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            ontology_tree=ontology_tree
        )
        
        # æ³¨æ„åŠ›åŠ é€Ÿ
        self.attn_backend = AdaptiveAttentionBackend()
    
    def forward(self, x, edge_index, edge_type):
        # æœ¬åœ°ï¼šé„°å±…èšåˆ
        local_out = self.local_mpnn(x, edge_index)
        
        # å…¨å±€ï¼šæœ¬é«”å¼•å°çš„æ³¨æ„åŠ›
        global_out = self.global_attn(x, use_backend=self.attn_backend)
        
        # èåˆ
        return local_out + global_out
```

#### 2.2 DR.KNOWSå¼è·¯å¾‘æ¨ç†æ¨¡çµ„ **ã€æ–°å¢ã€‘**

**æ ¸å¿ƒæ€æƒ³ï¼šå¾çŸ¥è­˜åœ–è­œæå–æ¨ç†è·¯å¾‘ä½œç‚ºè­‰æ“š**

```python
class DRKNOWSPathReasoning(nn.Module):
    """
    çŸ¥è­˜åœ–è­œè·¯å¾‘æª¢ç´¢ + æ’åº + LLMè§£é‡‹
    """
    def __init__(self, kg, llm_model=None):
        super().__init__()
        self.kg = kg
        self.llm = llm_model  # å¯é¸ï¼šç”¨æ–¼ç”Ÿæˆè§£é‡‹
        
        # è·¯å¾‘æª¢ç´¢å™¨
        self.path_retriever = MultiHopPathRetriever(
            kg=kg,
            max_hops=3,
            max_paths=50
        )
        
        # è·¯å¾‘è©•åˆ†å™¨
        self.path_scorer = PathScorer(
            scoring_method='structural_semantic',
            use_attention=True
        )
        
        # è·¯å¾‘åˆ°æ–‡æœ¬
        self.path_to_text = PathToTextConverter()
    
    def retrieve_and_explain(self, patient_phenotypes, top_k=10):
        """
        æª¢ç´¢æ¨ç†è·¯å¾‘ä¸¦ç”Ÿæˆè§£é‡‹
        """
        # 1. æª¢ç´¢è·¯å¾‘
        paths = self.path_retriever.retrieve(
            start_nodes=patient_phenotypes,
            target_types=['gene', 'disease']
        )
        
        # 2. è©•åˆ†èˆ‡æ’åº
        scored_paths = self.path_scorer.score(paths)
        top_paths = sorted(scored_paths, key=lambda x: x['score'], reverse=True)[:top_k]
        
        # 3. å¤šæ¨£åŒ–ï¼ˆé¿å…å†—é¤˜ï¼‰
        diverse_paths = self.diversify_paths(top_paths, threshold=0.7)
        
        # 4. è½‰æ›ç‚ºæ–‡æœ¬ï¼ˆå¯é¸ï¼šç”¨æ–¼LLMï¼‰
        path_texts = [self.path_to_text(p) for p in diverse_paths]
        
        # 5. LLMç”Ÿæˆè§£é‡‹ï¼ˆå¯é¸ï¼‰
        if self.llm:
            explanation = self.llm.generate(
                prompt=f"Based on these diagnostic paths, explain the diagnosis:\n{path_texts}",
                max_tokens=500
            )
        else:
            explanation = self.generate_template_explanation(diverse_paths)
        
        return {
            'paths': diverse_paths,
            'explanation': explanation,
            'evidence': self.extract_evidence(diverse_paths)
        }
```

**è·¯å¾‘ç¯„ä¾‹ï¼š**
```
ç—‡ç‹€: è‚Œè‚‰ç„¡åŠ› (HP:0001324)
  â†“ phenotype_to_gene (confidence: 0.85)
åŸºå› : DMD (çªè®Š)
  â†“ gene_to_disease (confidence: 0.92)
ç–¾ç—…: æœæ°è‚Œè‚‰ç‡Ÿé¤Šä¸è‰¯ç—‡ (MONDO:0010679)

è­‰æ“šéˆ:
- ClinVar: rs123456 (è‡´ç—…æ€§: Pathogenic)
- OMIM: #310200
- æ–‡ç»: PMID:12345678 (150ä¾‹ç—…ä¾‹ç ”ç©¶)
```

#### 2.3 æ‚£è€…åµŒå…¥èˆ‡æ™‚åºå»ºæ¨¡

**å‚³çµ±æ–¹æ³•çš„å•é¡Œï¼š**
```python
# ç°¡å–®èšåˆ - ä¸Ÿå¤±æ™‚åºä¿¡æ¯
patient_embedding = mean(symptom_embeddings)
```

**å‡ç´šæ–¹æ¡ˆï¼šNeural ODE + Transformer**
```python
class TemporalPatientEncoder(nn.Module):
    """
    æ•´åˆæ™‚åºä¿¡æ¯çš„æ‚£è€…ç·¨ç¢¼å™¨
    """
    def __init__(self):
        # å–®æ¬¡å°±è¨ºç·¨ç¢¼å™¨
        self.visit_encoder = VisitEncoder(
            input_dim=512,
            hidden_dim=256
        )
        
        # Neural ODEï¼ˆè™•ç†ä¸è¦å‰‡æ™‚é–“é–“éš”ï¼‰
        self.ode_func = ODEFunc(hidden_dim=256)
        
        # Time-aware Transformer
        self.temporal_transformer = TimeAwareTransformer(
            hidden_dim=256,
            num_layers=4,
            num_heads=8
        )
    
    def forward(self, patient_history):
        """
        patient_history: [
            {'time': t1, 'phenotypes': [...]},
            {'time': t2, 'phenotypes': [...]},
            ...
        ]
        """
        # 1. ç·¨ç¢¼æ¯æ¬¡å°±è¨º
        visit_embeddings = [
            self.visit_encoder(visit['phenotypes'])
            for visit in patient_history
        ]
        
        # 2. Neural ODEæ¨¡æ“¬é€£çºŒæ¼”åŒ–
        time_stamps = [visit['time'] for visit in patient_history]
        ode_states = odeint(
            self.ode_func,
            visit_embeddings[0],
            time_stamps,
            method='dopri5'
        )
        
        # 3. Transformeræ•æ‰é•·æœŸä¾è³´
        final_state = self.temporal_transformer(
            ode_states,
            time_stamps
        )
        
        return final_state
```

#### 2.4 æœ¬é«”ç´„æŸè§£ç¢¼å™¨ **ã€æ–°å¢ã€‘**

**å‚³çµ±è§£ç¢¼å™¨çš„å•é¡Œï¼š**
```python
# DistMult: å¯èƒ½ç”¢ç”Ÿä¸åˆç†é æ¸¬
score = (h * r * t).sum()  # ç„¡ç´„æŸ
```

**å‡ç´šï¼šæœ¬é«”ç´„æŸè§£ç¢¼**
```python
class OntologyConstrainedDecoder(nn.Module):
    """
    ä½¿ç”¨æœ¬é«”çŸ¥è­˜ç´„æŸé æ¸¬ç©ºé–“
    """
    def __init__(self, ontology_kb):
        super().__init__()
        self.ontology_kb = ontology_kb
        
        # åŸºç¤è§£ç¢¼å™¨
        self.base_decoder = DistMult(hidden_dim=512)
        
        # ç´„æŸæª¢æŸ¥å™¨
        self.constraint_checker = OntologyConstraintChecker(
            ontology_kb=ontology_kb
        )
    
    def forward(self, patient_emb, candidate_diseases):
        # 1. åŸºç¤è©•åˆ†
        base_scores = self.base_decoder(patient_emb, candidate_diseases)
        
        # 2. æœ¬é«”ç´„æŸæª¢æŸ¥
        valid_mask = self.constraint_checker.validate_batch(
            patient_phenotypes=patient_emb.phenotypes,
            candidate_diseases=candidate_diseases
        )
        
        # 3. æ‡²ç½°ä¸åˆç†é æ¸¬
        constrained_scores = base_scores * valid_mask.float()
        
        # 4. æœ¬é«”ç›¸ä¼¼åº¦å¢å¼·
        ontology_scores = self.compute_ontology_similarity(
            patient_emb.phenotypes,
            candidate_diseases
        )
        
        # 5. èåˆ
        final_scores = (
            0.6 * constrained_scores + 
            0.4 * ontology_scores
        )
        
        return final_scores
```

### ç¬¬3å±¤ï¼šæª¢ç´¢å±¤ (Retrieval Layer)

#### 3.1 æ··åˆæª¢ç´¢ç­–ç•¥

```python
class HybridRetrievalEngine:
    """
    å‘é‡æª¢ç´¢ + åœ–æª¢ç´¢ + è·¯å¾‘æª¢ç´¢
    """
    def __init__(self, kg, vector_index, path_retriever):
        self.kg = kg
        self.vector_index = vector_index
        self.path_retriever = path_retriever
    
    def retrieve(self, patient_phenotypes, top_k=20):
        # 1. å‘é‡æª¢ç´¢ï¼ˆå¿«é€Ÿç¯©é¸ï¼‰
        vector_results = self.vector_index.search(
            patient_phenotypes,
            k=100
        )
        
        # 2. åœ–æ“´å±•ï¼ˆæ‰¾åˆ°ç›¸é—œå­åœ–ï¼‰
        subgraph = self.kg.expand_subgraph(
            seed_nodes=vector_results,
            max_nodes=10000
        )
        
        # 3. è·¯å¾‘æª¢ç´¢ï¼ˆæ¨ç†è·¯å¾‘ï¼‰
        diagnostic_paths = self.path_retriever.retrieve(
            patient_phenotypes,
            subgraph=subgraph
        )
        
        # 4. èåˆæ’åº
        final_ranking = self.fuse_rankings([
            vector_results,
            subgraph.node_scores,
            diagnostic_paths
        ], weights=[0.3, 0.3, 0.4])
        
        return final_ranking[:top_k]
```

### ç¬¬4å±¤ï¼šæœå‹™å±¤ (Service Layer)

**APIè¨­è¨ˆï¼ˆRESTfulï¼‰ï¼š**

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="Medical KG Diagnosis API")

class DiagnosisRequest(BaseModel):
    patient_id: str
    phenotypes: List[str]  # HPO IDs
    include_explanation: bool = True
    include_paths: bool = True

class DiagnosisResponse(BaseModel):
    patient_id: str
    top_diseases: List[Dict]  # [{disease_id, name, confidence, evidence}]
    top_genes: List[Dict]
    explanation: Optional[str]
    diagnostic_paths: Optional[List[Dict]]
    warnings: List[str]  # æœ¬é«”ç´„æŸè­¦å‘Š

@app.post("/api/v2/diagnose")
async def diagnose(request: DiagnosisRequest):
    """
    è¨ºæ–·æ¥å£ï¼ˆv2 - æœ¬é«”å¢å¼·ç‰ˆï¼‰
    """
    # 1. é©—è­‰è¼¸å…¥ï¼ˆæœ¬é«”æª¢æŸ¥ï¼‰
    validation = ontology_kb.validate_phenotype_set(request.phenotypes)
    if not validation['valid']:
        return {
            'error': 'Invalid phenotype combination',
            'details': validation['reason']
        }
    
    # 2. æª¢ç´¢èˆ‡æ¨ç†
    results = model.predict(
        phenotypes=request.phenotypes,
        include_paths=request.include_paths
    )
    
    # 3. ç”Ÿæˆè§£é‡‹
    if request.include_explanation:
        explanation = path_reasoning.generate_explanation(
            results['paths']
        )
    else:
        explanation = None
    
    return DiagnosisResponse(
        patient_id=request.patient_id,
        top_diseases=results['diseases'],
        top_genes=results['genes'],
        explanation=explanation,
        diagnostic_paths=results['paths'] if request.include_paths else None,
        warnings=validation.get('warnings', [])
    )
```

### ç¬¬5å±¤ï¼šéƒ¨ç½²å±¤ (Deployment Layer)

#### 5.1 è·¨å¹³å°è‡ªé©æ‡‰éƒ¨ç½²

```python
# deploy/adaptive_deployment.py

class AdaptiveDeployment:
    """
    æ ¹æ“šå¹³å°è‡ªå‹•èª¿æ•´é…ç½®
    """
    def __init__(self):
        self.platform = detect_platform()
        self.config = self.get_platform_config()
    
    def get_platform_config(self):
        if self.platform == 'windows_x86_blackwell':
            return {
                'model': 'GraphGPS',
                'layers': 6,
                'hidden_dim': 512,
                'attention': 'flash_attention_2',
                'batch_size': 32,
                'fp16': True
            }
        elif self.platform == 'dgx_spark_arm_blackwell':
            return {
                'model': 'GraphGPS',  # åŒæ¨£çš„æ¨¡å‹
                'layers': 4,  # å±¤æ•¸æ¸›å°‘ï¼ˆè¨˜æ†¶é«”è€ƒé‡ï¼‰
                'hidden_dim': 256,  # ç¶­åº¦é™ä½
                'attention': 'pytorch_sdpa',  # é™ç´šæ³¨æ„åŠ›
                'batch_size': 64,  # åˆ©ç”¨å¤§è¨˜æ†¶é«”
                'fp16': True
            }
        else:
            raise ValueError(f"Unsupported platform: {self.platform}")
```

---

## è·¨å¹³å°å…¼å®¹æ€§ç­–ç•¥

### å¥—ä»¶å®‰è£çŸ©é™£

| å¥—ä»¶ | Windows x86 å®‰è£ | DGX Spark ARM å®‰è£ | å‚™è¨» |
|------|------------------|---------------------|------|
| PyTorch 2.9 | `pip install torch==2.9.0 --index-url .../cu130` | åŒå·¦ | âœ… å®˜æ–¹æ”¯æŒ |
| PyG 2.6 | `pip install torch-geometric pyg-lib ...` | åŒå·¦æˆ–å¾æºç¢¼ | âš ï¸ éœ€æ¸¬è©¦ |
| FlashAttn-2 | `pip install flash-attn --no-build-isolation` | âŒ è·³é | ä½¿ç”¨å‚™æ¡ˆ |
| xformers | `pip install xformers` | å˜—è©¦å®‰è£ | ARMæ”¯æŒæœ‰é™ |
| Voyager | `pip install voyager>=2.0` | âœ… | è·¨å¹³å° CPU |
| cuVS | âŒ (Linux only) | `pip install cuvs-cu12` | Linux GPU åŠ é€Ÿ |

### è‡ªé©æ‡‰æ³¨æ„åŠ›å¯¦ç¾

```python
# src/models/attention/adaptive_backend.py

class AdaptiveAttentionBackend:
    """
    è‡ªå‹•é¸æ“‡æœ€ä½³æ³¨æ„åŠ›å¯¦ç¾
    å„ªå…ˆç´š: FlashAttn-2 > xformers > PyTorch SDPA > æ‰‹å‹•å¯¦ç¾
    """
    def __init__(self):
        self.backend = self._detect_backend()
        logger.info(f"Using attention backend: {self.backend}")
    
    def _detect_backend(self):
        # 1. å˜—è©¦FlashAttention-2
        try:
            import flash_attn
            if torch.backends.cuda.flash_sdp_enabled():
                return 'flash_attention_2'
        except ImportError:
            pass
        
        # 2. å˜—è©¦xformers
        try:
            import xformers.ops
            return 'xformers'
        except ImportError:
            pass
        
        # 3. PyTorchåŸç”ŸSDPA
        if hasattr(F, 'scaled_dot_product_attention'):
            return 'pytorch_sdpa'
        
        # 4. æ‰‹å‹•å¯¦ç¾ï¼ˆæœ€æ…¢ï¼‰
        return 'manual'
    
    def compute(self, q, k, v, attn_mask=None):
        if self.backend == 'flash_attention_2':
            from flash_attn import flash_attn_func
            return flash_attn_func(q, k, v, causal=False)
        
        elif self.backend == 'xformers':
            from xformers.ops import memory_efficient_attention
            return memory_efficient_attention(q, k, v, attn_bias=attn_mask)
        
        elif self.backend == 'pytorch_sdpa':
            return F.scaled_dot_product_attention(
                q, k, v, 
                attn_mask=attn_mask,
                enable_gqa=True  # Grouped Query Attention
            )
        
        else:  # manual
            return self._manual_attention(q, k, v, attn_mask)
```

---

## æ•ˆèƒ½é æœŸèˆ‡åŸºæº–

### æ¨¡å‹æ•ˆèƒ½ï¼ˆèˆ‡SHEPHERDå°æ¯”ï¼‰

| æŒ‡æ¨™ | SHEPHERD | æœ¬ç³»çµ±ï¼ˆåŸºç·šï¼‰ | æœ¬ç³»çµ±ï¼ˆå®Œæ•´ï¼‰ |
|------|----------|----------------|----------------|
| Hits@1 | 42% | 45% | **58%** |
| Hits@5 | 54% | 58% | **74%** |
| Hits@10 | 60% | 65% | **82%** |
| Hits@20 | 68% | 72% | **88%** |
| MRR | 0.49 | 0.52 | **0.67** |
| å¹»è¦ºç‡ | ~20% | ~12% | **<5%** |
| å¯è§£é‡‹æ€§ | âŒ | âš ï¸ | âœ… |

### æ¨ç†å»¶é²

| å¹³å° | å–®æ‚£è€…å»¶é² | ååé‡ (QPS) | å‚™è¨» |
|------|------------|--------------|------|
| Windows x86 (16GB) | 1.2s | 25 | FlashAttn-2 |
| DGX Spark (128GB) | 1.8s | 35 | å¤§batchå„ªå‹¢ |

### è¨“ç·´æ™‚é–“ï¼ˆå®Œæ•´PrimeKGï¼‰

| éšæ®µ | Windows x86 | DGX Spark | å‚™è¨» |
|------|-------------|-----------|------|
| è³‡æ–™é è™•ç† | 2h | 1.5h | CPUå¯†é›† |
| åœ–é è¨“ç·´ | 48h | 36h | FP16æ··åˆç²¾åº¦ |
| æ‚£è€…å¾®èª¿ | 12h | 8h | - |
| **ç¸½è¨ˆ** | **~62h** | **~45h** | ç´„2-3å¤© |

---

## æˆåŠŸæ¨™æº–ï¼ˆè‡¨åºŠç´šï¼‰

### Phase 1: æŠ€è¡“é©—è­‰ï¼ˆMVP+ï¼‰
- âœ… Hits@10 â‰¥ 70%
- âœ… å¹»è¦ºç‡ â‰¤ 10%
- âœ… å…©å€‹å¹³å°éƒ½èƒ½é‹è¡Œ
- âœ… åŸºæœ¬å¯è§£é‡‹æ€§

### Phase 2: è‡¨åºŠåŸå‹
- âœ… Hits@10 â‰¥ 80%
- âœ… å¹»è¦ºç‡ â‰¤ 5%
- âœ… å®Œæ•´æ¨ç†è·¯å¾‘
- âœ… æœ¬é«”ç´„æŸé©—è­‰
- âœ… æ¨ç†å»¶é² < 2s

### Phase 3: ç”Ÿç”¢å°±ç·’
- âœ… Hits@10 â‰¥ 85%
- âœ… å¹»è¦ºç‡ â‰¤ 3%
- âœ… èˆ‡äººé¡å°ˆå®¶å°æ¯”
- âœ… è‡¨åºŠè©¦é©—é©—è­‰
- âœ… ç›£ç®¡åˆè¦ï¼ˆFDA/NMPAï¼‰

---

## é—œéµæ–‡ç»

1. **SHEPHERD** (2025): Few-shot learning for phenotype-driven diagnosis
2. **DORI** (2025): Dual ontology + relational graph + Neural ODE
3. **DR.KNOWS** (2025): Knowledge graph paths for diagnosis prediction
4. **Knowledge Hypergraph** (2024): Higher-order medical relationships
5. **Foundation Models in Medicine** (2025): Comprehensive survey

---

**ç‰ˆæœ¬**: v2.0  
**æœ€å¾Œæ›´æ–°**: 2025-10-07  
**å‡ç´šé‡é»**: æœ¬é«”æ·±åº¦æ•´åˆã€çŸ¥è­˜è¶…åœ–ã€è·¨å¹³å°å…¼å®¹