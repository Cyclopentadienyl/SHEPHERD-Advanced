# =============================================================================
# SHEPHERD-Advanced â€” Windows x86 CUDA Packages (Reference Only)
# =============================================================================
#
# IMPORTANT: This file is kept as a REFERENCE for CUDA-dependent packages.
# deploy.cmd now installs these directly (Stage 2) and uses pyproject.toml
# for all other dependencies (Stage 3: pip install .).
#
# Do NOT use: pip install -r requirements_windows.txt
# Instead run: deploy.cmd
#
# Manual equivalent:
#   pip install --index-url https://download.pytorch.org/whl/cu130 ^
#       torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0
#   pip install torch_geometric
#   pip install pyg-lib torch-sparse torch-scatter torch-cluster ^
#       -f https://data.pyg.org/whl/torch-2.9.0+cu130.html
#   pip install .
# =============================================================================

# PyTorch 2.9.0 + cu130 (unified across all platforms)
# NOTE: Use exact torch==2.9.0 to avoid 2.9.1 which breaks pyg-lib compatibility
--index-url https://download.pytorch.org/whl/cu130
torch==2.9.0
torchvision==0.24.0
torchaudio==2.9.0

# PyTorch Geometric (required for heterogeneous GNN message passing)
# Native extensions need matching torch+CUDA wheels:
#   pip install pyg-lib torch-sparse torch-scatter torch-cluster ^
#       -f https://data.pyg.org/whl/torch-2.9.0+cu130.html
torch_geometric

# All other dependencies (pronto, networkx, pandas, numpy, scipy, pydantic,
# fastapi, uvicorn, gradio, voyager, tqdm, requests, python-dotenv, etc.)
# are declared in pyproject.toml and installed via: pip install .

# Optional accelerators (flash-attn, xformers, sageattention) are installed
# at launch time via: launch_shepherd.cmd --flash-attn / --xformers
